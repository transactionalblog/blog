= Raft is Okay
:revdate: 2024-06-14
:page-hidden: true
:stem: latexmath
:page-features: stem
:toc: right
:bibtex-file: raft-is-okay.bib

[.display-none]
== Section

:uri-rystsov-simple-consensus: https://web.archive.org/web/20240121170711/http://rystsov.info/2017/02/15/simple-consensus.html

Raft blends together failure masking and failure detection into one replication algorithm, and thus exists as a middle-ground between resource efficiency and high availability.  It delivers a better read bandwidth efficiency than leaderless consensus, and also delivers a lower chance of unavailability upon random replica failure.

This blending leaves Raft with the full complexity of both sides as well. By partly being a failure detection-based replication algorithm, a reliable Raft implementation has to pick up all the complexity of detecting gray failures in the same way as a reconfiguration-based algorithm, but without the corresponding resource efficiency advantages to justify the complexity.  However, by also being a failure masking algorithm, it also must deal with nodes being transiently unavailable, and the corresponding error handling complexity and state space explosion that occurs in tracking that.  Existing in the middle of this design space trade off is accepting the highest amount of implementation complexity.

The main benefit of being a failure masking, quorum-based algorithm is that failures can be handled transparently without the client noticing it in the form of a transient spike in latency.  As soon as one replica (Raft's leader) is using failure detection instead, the client needs to be okay with the fact that the replica group will have occasional seconds of unavailability whenever the leader unexpectedly dies.  The upper bound of expected latency rises drastically with the first addition of a replica requiring failure detection.  It doesn't increase as more replicas are converted to requiring failure detection.  The frequency of transient unavailability events increases, but the expected duration of those events is constant.

Furthermore, when Raft is deployed to replicate partitions of a database, leadership responsibility is distributed evenly across the cluster to evenly balance the increased workload of being a leader.  Thus, the failure of any one node is going to lead to _some_ partition being transiently unavailable.  It's not even an argument of _will_ there being unavailability, it's just a question of how many partitions will be affected. 

The main benefit of being a failure detection, reconfigurable primary-backup based replication algorithm is the ability to use only 3 replicas to achieve stem:[f=2] as opposed to 5 for a quorum-based system. Choosing to use Raft over Hermes means paying for 66% more hardware for replicas, in exchange for which one gains a partial decrease in p99.9 latency from better transient unavailability handling.  I'm doubtful that this is as commonly the correct trade off as Raft's pervasive usage would make it seem.  Denis Rystsov {uri-rystsov-simple-consensus}[compared Raft versus Paxos]{nospace}sidenote:ref[] in the form of CASPaxos{nospace}sidenote:ref[] and showed the leaderless version of consensus to be less complex and more available.  Individual machines in production are reasonably reliable, we just emphasize their unreliability because at scale, the aggregate chance of a failure is quite high.  FoundationDB{nospace}sidenote:ref[] runs its entire (greatly more than 5 node) distributed transaction subsystem as one virtual synchrony-based entity, and runs through an extensive recovery process on failure. The paper comments "In August 2020, there was only one transaction system recovery, which took 8.61 seconds. This corresponds to five 9s availability."  So even for a system that's more failure risky and with a worse recovery time than Hermes, the tail latency and unavailability on failure isn't bad enough to warrant SLA concern.  The set of use cases where Raft would be the optimal choice seems to be quite narrow.
[.aside]#sidenote:def[] This has been locally mirrored (with permission from Denis) from its original home {uri-rystsov-simple-consensus-original}[on rystsov.info], which is currently unavailable, and {uri-rystsov-simple-consensus-archive}[the archive.org mirror] suffers the occasional DDoS.#
[.aside]#sidenote:def[] bibentry:[CASPaxos]#
[.aside]#sidenote:def[] bibentry:[FoundationDB]#

This isn't to say that Raft is a poor choice of replication algorithm.  Raft is always a _safe_ choice.  Choosing Raft over leaderless Paxos means that one doesn't have to worry about livelock on contended items.  Choosing Raft over Hermes means one can likely move the leader away from a machine causing persistent gray failure issues, and then mask the failures instead.  There is a story one can build that Raft removes the largest sources of potential outages on both sides, at the cost of paying some extra money for stem:[2f+1] replicas.  Most use cases are read-heavy, and Raft lets you deliver 100% read bandwidth efficiency via follower reads, so it's not money being entirely wasted either.  Raft's safety as a choice is only further increased by there being a single text giving sufficient description on how to implement Raft, complemented by a rich number of blog posts detailing subtle issues which caused outages elsewhere.  No one is doing to get in trouble for choosing Raft, as it's not _too_ bad at anything.  It's just resoundingly mediocre at everything.

And I do understand that there's reasons why we don't see a large number of reconfigurable-primary backup databases.  Requiring an external consensus service to manage replication groups requires first having a consensus solution implemented and production ready.  This could either be using an existing solution like Zookeeper or Etcd, but then those might have existing issues, you're then required to become an expert in running them, and using a separate project as the core of your service adds a significant hassle to testing.  The alternative is to implement your own, and then you're doing all the work of just deploying leaderless Paxos or leaderful Raft, only to turn and do even more work before being able to deploy to production.  And there still isn't a single great reference for how to safely identify and reconfigure around gray failures.

My only ask is that before choosing Raft by default to solve your next replication need, please do spend a moment thinking about if it strikes the right set of tradeoffs for your use case.