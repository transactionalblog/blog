= Raft is Okay
:revdate: 2024-06-14
:page-hidden: true
:stem: latexmath
:page-features: stem
:toc: preamble
:bibtex-file: raft-is-okay.bib

[.display-hidden]
== Section

:uri-rystsov-simple-consensus: https://web.archive.org/web/20240121170711/http://rystsov.info/2017/02/15/simple-consensus.html

Raft blends together failure masking and failure detection into one replication algorithm, and thus exists as a middle-ground between resource efficiency and high availability.  It delivers a better read bandwidth efficiency than leaderless consensus, and also delivers a lower chance of unavailability upon random replica failure.

This blending leaves Raft with the full complexity of both sides as well. By partly being a failure detection-based replication algorithm, a reliable Raft implementation has to pick up all the complexity of detecting gray failures in the same way as a reconfiguration-based algorithm, but without the corresponding resource efficiency advantages to justify the complexity.  However, by also being a failure masking algorithm, it also must deal with nodes being transiently unavailable, and the corresponding error handling complexity and state space explosion that occurs in tracking that.  Existing in the middle of this design space trade off is accepting the highest amount of implementation complexity.

The main benefit of being a failure masking, quorum-based algorithm is that failures can be handled transparently without the client noticing it in the form of a transient spike in latency.  As soon as one replica (Raft's leader) is using failure detection instead, the client needs to be okay with the fact that the replica group will have occasional seconds of unavailability whenever the leader unexpectedly dies.  The upper bound of expected latency rises drastically with the first addition of a replica requiring failure detection.  It doesn't increase as more replicas are converted to requiring failure detection.  The frequency of transient unavailability events increases, but the expected duration of those events is constant.

Furthermore, when Raft is deployed to replicate partitions of a database, leadership responsibility is distributed evenly across the cluster to evenly balance the increased workload of being a leader.  Thus, the failure of any one node is going to lead to _some_ partition being transiently unavailable.  It's not even an argument of _will_ there being unavailability, it's just a question of how many partitions will be affected. However you feel about reconfigurable primary-backup and failure detection-based distributed system design{nospace}sidenote:ref[] is _exactly_ how you should feel about the leader in Raft/Multi-Paxos.
[.aside]#sidenote:def[] Some folk really don't like systems which rely on failure detectors and have a reconfiguration step during which the partition is unavailable, and that's okay.  But any failure pattern you might have thought of and felt concerned about while reading the failure detection section applies precisely the same to the leader in Raft. If you think it's unacceptable that chain replication has unavailability during reconfiguration when any replica fails, the exact same unavailability during reconfiguration happening to Raft when the leader fails should also feel unacceptable.#

The main benefit of being a failure detection, reconfigurable primary-backup based replication algorithm is the ability to use only 3 replicas to achieve stem:[f=2] as opposed to 5 for a quorum-based system. Choosing to use Raft over Hermes means paying for 66% more hardware for replicas, in exchange for which one gains a partial decrease in p99.9 latency from better transient unavailability handling.  I'm doubtful that this is as commonly the correct trade off as Raft's pervasive usage would make it seem.  Denis Rystsov {uri-rystsov-simple-consensus}[compared Raft versus Paxos]{nospace}sidenote:ref[] in the form of CASPaxos{nospace}sidenote:ref[] and showed the leaderless version of consensus to be less complex and more available.  Individual machines in production are reasonably reliable, we just emphasize their unreliability because at scale, the aggregate chance of a failure is quite high.  FoundationDB{nospace}sidenote:ref[] runs its entire (greatly more than 5 node) distributed transaction subsystem as one virtual synchrony-based entity, and runs through an extensive recovery process on failure. The paper comments "In August 2020, there was only one transaction system recovery, which took 8.61 seconds. This corresponds to five 9s availability."  So even for a system that's more failure risky and with a worse recovery time than Hermes, the tail latency and unavailability on failure isn't bad enough to warrant SLA concern.  The set of use cases where Raft would be the optimal choice seems to be quite narrow.
[.aside]#sidenote:def[] This has been locally mirrored (with permission from Denis) from its original home {uri-rystsov-simple-consensus-original}[on rystsov.info], which is currently unavailable, and {uri-rystsov-simple-consensus-archive}[the archive.org mirror] suffers the occasional DDoS.#
[.aside]#sidenote:def[] bibentry:[CASPaxos]#
[.aside]#sidenote:def[] bibentry:[FoundationDB]#

This isn't to say that Raft is a poor choice of replication algorithm.  Raft is always a _safe_ choice.  Choosing Raft over leaderless Paxos means that one doesn't have to worry about livelock on contended items.  Choosing Raft over Hermes means one can likely move the leader away from a machine causing persistent gray failure issues, and then mask the failures instead.  There is a story one can build that Raft removes the largest sources of potential outages on both sides, at the cost of paying some extra money for stem:[2f+1] replicas.  Most use cases are read-heavy, and Raft lets you deliver 100% read bandwidth efficiency via follower reads, so it's not money being entirely wasted either.  Raft's safety as a choice is only further increased by there being a single text giving sufficient description on how to implement Raft, complemented by a rich number of blog posts detailing subtle issues which caused outages elsewhere.  No one is doing to get in trouble for choosing Raft, as it's not _too_ bad at anything.  It's just resoundingly mediocre at everything.

My only ask is that before choosing Raft by default to solve your next replication need, please do spend a moment thinking about if it strikes the right set of tradeoffs for your use case.