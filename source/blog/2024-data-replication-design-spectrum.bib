@article{Paxos,
    author = {Lamport, Leslie},
    title = {Paxos Made Simple},
    year = {2001},
    month = {December},
    abstract = {At the PODC 2001 conference, I got tired of everyone saying how difficult it was to understand the Paxos algorithm, published in [122]. Although people got so hung up in the pseudo-Greek names that they found the paper hard to understand, the algorithm itself is very simple. So, I cornered a couple of people at the conference and explained the algorithm to them orally, with no paper. When I got home, I wrote down the explanation as a short note, which I later revised based on comments from Fred Schneider and Butler Lampson. The current version is 13 pages long, and contains no formula more complicated than n1 > n2.},
    _url = {https://www.microsoft.com/en-us/research/publication/paxos-made-simple/},
    pages = {51-58},
    journal = {ACM SIGACT News (Distributed Computing Column) 32, 4 (Whole Number 121, December 2001)},
    edition = {ACM SIGACT News (Distributed Computing Column) 32, 4 (Whole Number 121, December 2001)},
    scholarcluster = {12753876235558083852},
}
@INPROCEEDINGS{ReadRepair,
    author = {Malkhi, D. and Reiter, M.K.},
    booktitle = {Proceedings Seventeenth IEEE Symposium on Reliable Distributed Systems (Cat. No.98CB36281)}, 
    title = {Secure and scalable replication in Phalanx}, 
    year = {1998},
    volume = {},
    number = {},
    pages = {51-58},
    keywords = {Public key;Voting;Identity-based encryption;Publishing;Reactive power;Application software;Software systems;Buildings;Large-scale systems;Online services},
    _doi = {10.1109/RELDIS.1998.740474},
    refname = {Read Repair},
    scholarcluster = {9927566946845895796},
}
@inproceedings{PaxosVsRaft,
    author = {Howard, Heidi and Mortier, Richard},
    title = {Paxos vs Raft: have we reached consensus on distributed consensus?},
    year = {2020},
    isbn = {9781450375245},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    _url = {https://doi.org/10.1145/3380787.3393681},
    _doi = {10.1145/3380787.3393681},
    abstract = {Distributed consensus is a fundamental primitive for constructing fault-tolerant, strongly-consistent distributed systems. Though many distributed consensus algorithms have been proposed, just two dominate production systems: Paxos, the traditional, famously subtle, algorithm; and Raft, a more recent algorithm positioned as a more understandable alternative to Paxos.In this paper, we consider the question of which algorithm, Paxos or Raft, is the better solution to distributed consensus? We analyse both to determine exactly how they differ by describing a simplified Paxos algorithm using Raft's terminology and pragmatic abstractions.We find that both Paxos and Raft take a very similar approach to distributed consensus, differing only in their approach to leader election. Most notably, Raft only allows servers with up-to-date logs to become leaders, whereas Paxos allows any server to be leader provided it then updates its log to ensure it is up-to-date. Raft's approach is surprisingly efficient given its simplicity as, unlike Paxos, it does not require log entries to be exchanged during leader election. We surmise that much of the understandability of Raft comes from the paper's clear presentation rather than being fundamental to the underlying algorithm being presented.},
    booktitle = {Proceedings of the 7th Workshop on Principles and Practice of Consistency for Distributed Data},
    articleno = {8},
    numpages = {9},
    location = {Heraklion, Greece},
    series = {PaPoC '20},
    refname = {Paxos vs Raft},
    scholarcluster = {398538607954569089},
    arxiv = {2004.05074},
}
@inproceedings{Raft,
    author = {Ongaro, Diego and Ousterhout, John},
    title = {In search of an understandable consensus algorithm},
    year = {2014},
    isbn = {9781931971102},
    publisher = {USENIX Association},
    address = {USA},
    abstract = {Raft is a consensus algorithm for managing a replicated log. It produces a result equivalent to (multi-)Paxos, and it is as efficient as Paxos, but its structure is different from Paxos; this makes Raft more understandable than Paxos and also provides a better foundation for building practical systems. In order to enhance understandability, Raft separates the key elements of consensus, such as leader election, log replication, and safety, and it enforces a stronger degree of coherency to reduce the number of states that must be considered. Results from a user study demonstrate that Raft is easier for students to learn than Paxos. Raft also includes a new mechanism for changing the cluster membership, which uses overlapping majorities to guarantee safety.},
    booktitle = {Proceedings of the 2014 USENIX Conference on USENIX Annual Technical Conference},
    pages = {305-320},
    numpages = {16},
    location = {Philadelphia, PA},
    series = {USENIX ATC'14},
    scholarcluster = {12646889551697084617},
}
@phdthesis{ConsistentFollowerReads,
    author = {Ongaro, Diego},
    advisor = {K., Ousterhout, John and David, Mazi\`{e}res, and Mendel, Rosenblum,},
    title = {Consensus: Bridging Theory and Practice},
    year = {2014},
    isbn = {9798662514218},
    publisher = {Stanford University},
    address = {Stanford, CA, USA},
    abstract = {Distributed consensus is fundamental to building fault-tolerant systems. It allows a collection of machines to work as a coherent group that can survive the failures of some of its members. Unfortunately, the most common consensus algorithm, , is widely regarded as difficult to understand and implement correctly. This dissertation presents a new consensus algorithm called Raft, which was designed for understandability. Raft first elects a server as leader, then concentrates all decision-making onto the leader. These two basic steps are relatively independent and form a better structure than Paxos, whose components are hard to separate. Raft elects a leader using voting and randomized timeouts. The election guarantees that the leader already stores all the information it needs, so data only flows outwards from the leader to other servers. Compared to other leader-based algorithms, this reduces mechanism and simplifies the behavior. Once a leader is elected, it manages a replicated log. Raft leverages a simple invariant on how logs grow to reduce the algorithm's state space and accomplish this task with minimal mechanism. Raft is also more suitable than previous algorithms for real-world implementations. It performs well enough for practical deployments, and it addresses all aspects of building a complete system, including how to manage client interactions, how to change the cluster membership, and how to compact the log when it grows too large. To change the cluster membership, Raft allows adding or removing one server at a time (complex changes can be composed from these basic steps), and the cluster continues servicing requests throughout the change. We believe that Raft is superior to Paxos and other consensus algorithms, both for educational purposes and as a foundation for implementation. Results from a user study demonstrate that Raft is easier for students to learn than Paxos. The algorithm has been formally specified and proven, its leader election algorithm works well in a variety of environments, and its performance is equivalent to Multi-Paxos. Many implementations of Raft are now available, and several companies are deploying Raft.},
    note = {Section 6.4},
    refname = {Consistent Follower Reads},
    scholarcluster = {12811710649127524514},
}
@article{PaxosMadeModeratelyComplex,
    author = {Van Renesse, Robbert and Altinbuken, Deniz},
    title = {Paxos Made Moderately Complex},
    year = {2015},
    issue_date = {April 2015},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    volume = {47},
    number = {3},
    issn = {0360-0300},
    _url = {https://doi.org/10.1145/2673577},
    _doi = {10.1145/2673577},
    abstract = {This article explains the full reconfigurable multidecree Paxos (or multi-Paxos) protocol. Paxos is by no means a simple protocol, even though it is based on relatively simple invariants. We provide pseudocode and explain it guided by invariants. We initially avoid optimizations that complicate comprehension. Next we discuss liveness, list various optimizations that make the protocol practical, and present variants of the protocol.},
    journal = {ACM Comput. Surv.},
    month = feb,
    articleno = {42},
    numpages = {36},
    keywords = {consensus, voting, Replicated state machines},
    refname = {Paxos Made Moderately Complex},
    scholarcluster = {12811710649127524514},
}
@inproceedings{ZAB,
    author = {Junqueira, Flavio P. and Reed, Benjamin C. and Serafini, Marco},
    title = {Zab: High-performance broadcast for primary-backup systems},
    year = {2011},
    isbn = {9781424492329},
    publisher = {IEEE Computer Society},
    address = {USA},
    _url = {https://doi.org/10.1109/DSN.2011.5958223},
    _doi = {10.1109/DSN.2011.5958223},
    abstract = {Zab is a crash-recovery atomic broadcast algorithm we designed for the ZooKeeper coordination service. ZooKeeper implements a primary-backup scheme in which a primary process executes clients operations and uses Zab to propagate the corresponding incremental state changes to backup processes1. Due the dependence of an incremental state change on the sequence of changes previously generated, Zab must guarantee that if it delivers a given state change, then all other changes it depends upon must be delivered first. Since primaries may crash, Zab must satisfy this requirement despite crashes of primaries.},
    booktitle = {Proceedings of the 2011 IEEE/IFIP 41st International Conference on Dependable Systems&Networks},
    pages = {245-256},
    numpages = {12},
    series = {DSN '11},
    citekeys = {Zab,ZAB},
    scholarcluster = {13624279146503836178},
}
@inproceedings{CockroachDB,
    author = {Taft, Rebecca and Sharif, Irfan and Matei, Andrei and VanBenschoten, Nathan and Lewis, Jordan and Grieger, Tobias and Niemi, Kai and Woods, Andy and Birzin, Anne and Poss, Raphael and Bardea, Paul and Ranade, Amruta and Darnell, Ben and Gruneir, Bram and Jaffray, Justin and Zhang, Lucy and Mattis, Peter},
    title = {CockroachDB: The Resilient Geo-Distributed SQL Database},
    year = {2020},
    isbn = {9781450367356},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    _url = {https://doi.org/10.1145/3318464.3386134},
    _doi = {10.1145/3318464.3386134},
    abstract = {We live in an increasingly interconnected world, with many organizations operating across countries or even continents. To serve their global user base, organizations are replacing their legacy DBMSs with cloud-based systems capable of scaling OLTP workloads to millions of users. CockroachDB is a scalable SQL DBMS that was built from the ground up to support these global OLTP workloads while maintaining high availability and strong consistency. Just like its namesake, CockroachDB is resilient to disasters through replication and automatic recovery mechanisms. This paper presents the design of CockroachDB and its novel transaction model that supports consistent geo-distributed transactions on commodity hardware. We describe how CockroachDB replicates and distributes data to achieve fault tolerance and high performance, as well as how its distributed SQL layer automatically scales with the size of the database cluster while providing the standard SQL interface that users expect. Finally, we present a comprehensive performance evaluation and share a couple of case studies of CockroachDB users. We conclude by describing lessons learned while building CockroachDB over the last five years.},
    booktitle = {Proceedings of the 2020 ACM SIGMOD International Conference on Management of Data},
    pages = {1493-1509},
    numpages = {17},
    location = {Portland, OR, USA},
    series = {SIGMOD '20},
    citekeys = {CockroachDB},
    scholarcluster = {13649983341597312439},
}
@article{TiDB,
    author = {Huang, Dongxu and Liu, Qi and Cui, Qiu and Fang, Zhuhe and Ma, Xiaoyu and Xu, Fei and Shen, Li and Tang, Liu and Zhou, Yuxing and Huang, Menglong and Wei, Wan and Liu, Cong and Zhang, Jian and Li, Jianjun and Wu, Xuelian and Song, Lingyu and Sun, Ruoxi and Yu, Shuaipeng and Zhao, Lei and Cameron, Nicholas and Pei, Liquan and Tang, Xin},
    title = {TiDB: a Raft-based HTAP database},
    year = {2020},
    issue_date = {August 2020},
    publisher = {VLDB Endowment},
    volume = {13},
    number = {12},
    issn = {2150-8097},
    _url = {https://doi.org/10.14778/3415478.3415535},
    _doi = {10.14778/3415478.3415535},
    abstract = {Hybrid Transactional and Analytical Processing (HTAP) databases require processing transactional and analytical queries in isolation to remove the interference between them. To achieve this, it is necessary to maintain different replicas of data specified for the two types of queries. However, it is challenging to provide a consistent view for distributed replicas within a storage system, where analytical requests can efficiently read consistent and fresh data from transactional workloads at scale and with high availability.To meet this challenge, we propose extending replicated state machine-based consensus algorithms to provide consistent replicas for HTAP workloads. Based on this novel idea, we present a Raft-based HTAP database: TiDB. In the database, we design a multi-Raft storage system which consists of a row store and a column store. The row store is built based on the Raft algorithm. It is scalable to materialize updates from transactional requests with high availability. In particular, it asynchronously replicates Raft logs to learners which transform row format to column format for tuples, forming a real-time updatable column store. This column store allows analytical queries to efficiently read fresh and consistent data with strong isolation from transactions on the row store. Based on this storage system, we build an SQL engine to process large-scale distributed transactions and expensive analytical queries. The SQL engine optimally accesses row-format and column-format replicas of data. We also include a powerful analysis engine, TiSpark, to help TiDB connect to the Hadoop ecosystem. Comprehensive experiments show that TiDB achieves isolated high performance under CH-benCHmark, a benchmark focusing on HTAP workloads.},
    journal = {Proc. VLDB Endow.},
    month = {aug},
    pages = {3072-3084},
    numpages = {13},
    scholarcluster = {4024782010863299783},
}
@inproceedings {Zookeeper,
    author = {Patrick Hunt and Mahadev Konar and Flavio P. Junqueira and Benjamin Reed},
    title = {{ZooKeeper}: Wait-free Coordination for Internet-scale Systems},
    booktitle = {2010 USENIX Annual Technical Conference (USENIX ATC 10)},
    year = {2010},
    _url = {https://www.usenix.org/conference/usenix-atc-10/zookeeper-wait-free-coordination-internet-scale-systems},
    publisher = {USENIX Association},
    month = jun,
    scholarcluster = {16979330189653726967},
}
@inproceedings{PaxosMadeLive,
    title = {Paxos Made Live - An Engineering Perspective (2006 Invited Talk)},
    author = {Tushar Deepak Chandra and Robert Griesemer and Joshua Redstone},year	= {2007},
    _url = {http://dx.doi.org/10.1145/1281100.1281103},
    booktitle = {Proceedings of the 26th Annual ACM Symposium on Principles of Distributed Computing},
    refname = {Paxos Made Live},
    scholarcluster = {17465339664204453932},
}
@inproceedings{Spanner,
    title = {Spanner: Google's Globally-Distributed Database},author	= {James C. Corbett and Jeffrey Dean and Michael Epstein and Andrew Fikes and Christopher Frost and JJ Furman and Sanjay Ghemawat and Andrey Gubarev and Christopher Heiser and Peter Hochschild and Wilson Hsieh and Sebastian Kanthak and Eugene Kogan and Hongyi Li and Alexander Lloyd and Sergey Melnik and David Mwaura and David Nagle and Sean Quinlan and Rajesh Rao and Lindsay Rolig and Dale Woodford and Yasushi Saito and Christopher Taylor and Michal Szymaniak and Ruth Wang},
    year = {2012},
    booktitle = {OSDI},
    scholarcluster = {3523173873845838643},
}
@InProceedings{WitnessReplicas,
    author = {Paris, Jehan-Francois},
    editor = {Abiteboul, Serge and Kanellakis, Paris C.},
    title = {Efficient voting protocols with witnesses},
    booktitle = {ICDT '90},
    year = {1990},
    publisher = {Springer Berlin Heidelberg},
    address = {Berlin, Heidelberg},
    pages = {305--317},
    abstract = {Witnesses are small entities recording the state of a replicated object. They are especially useful in voting protocols as these protocols require 2n + 1 replicas to guarantee continuous access to the data in the presence of n site failures but allow some of these replicas to be replaced by a witness.},
    isbn = {978-3-540-46682-6},
    refname = {Witness Replicas},
    scholarcluster = {1415144878608869709},
}
@article{ABD,
    author = {Attiya, Hagit and Bar-Noy, Amotz and Dolev, Danny},
    title = {Sharing memory robustly in message-passing systems},
    year = {1995},
    issue_date = {Jan. 1995},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    volume = {42},
    number = {1},
    issn = {0004-5411},
    _url = {https://doi.org/10.1145/200836.200869},
    _doi = {10.1145/200836.200869},
    abstract = {Emulators that translate algorithms from the shared-memory model to two different message-passing models are presented. Both are achieved by implementing a wait-free, atomic, single-writer multi-reader register in unreliable, asynchronous networks. The two message-passing models considered are a complete network with processor failures and an arbitrary network with dynamic link failures.These results make it possible to view the shared-memory model as a higher-level language for designing algorithms in asynchronous distributed systems. Any wait-free algorithm based on atomic, single-writer multi-reader registers can be automatically emulated in message-passing systems, provided that at least a majority of the processors are not faulty and remain connected. The overhead introduced by these emulations is polynomial in the number of processors in the system.Immediate new results are obtained by applying the emulators to known shared-memory algorithms. These include, among others, protocols to solve the following problems in the message-passing model in the presence of processor or link failures: multi-writer multi-reader registers, concurrent time-stamp systems, l-exclusion, atomic snapshots, randomized consensus, and implementation of data structures.},
    journal = {J. ACM},
    month = {jan},
    pages = {124-142},
    numpages = {19},
    keywords = {atomic registers, emulation, fault-tolerance, message passing, processor and link failures, shared memory, wait-freedom},
    scholarcluster = {8138971298707379383},
}
@inproceedings{Megastore,
    title = {Megastore: Providing Scalable, Highly Available Storage for Interactive Services},
    author = {Jason Baker and Chris Bond and James C. Corbett and JJ Furman and Andrey Khorlin and James Larson and Jean-Michel Leon and Yawei Li and Alexander Lloyd and Vadim Yushprakh},
    year = {2011},
   _url = {http://www.cidrdb.org/cidr2011/Papers/CIDR11_Paper32.pdf},
    booktitle = {Proceedings of the Conference on Innovative Data system Research (CIDR)},
    pages = {223--234},
    scholarcluster = {75122057060478473},
}
@article{PaxosStore,
    author = {Zheng, Jianjun and Lin, Qian and Xu, Jiatao and Wei, Cheng and Zeng, Chuwei and Yang, Pingan and Zhang, Yunfan},
    title = {PaxosStore: high-availability storage made practical in WeChat},
    year = {2017},
    issue_date = {August 2017},
    publisher = {VLDB Endowment},
    volume = {10},
    number = {12},
    issn = {2150-8097},
    _url = {https://doi.org/10.14778/3137765.3137778},
    _doi = {10.14778/3137765.3137778},
    abstract = {In this paper, we present PaxosStore, a high-availability storage system developed to support the comprehensive business of WeChat. It employs a combinational design in the storage layer to engage multiple storage engines constructed for different storage models. PaxosStore is characteristic of extracting the Paxos-based distributed consensus protocol as a middleware that is universally accessible to the underlying multi-model storage engines. This facilitates tuning, maintaining, scaling and extending the storage engines. According to our experience in engineering practice, to achieve a practical consistent read/write protocol is far more complex than its theory. To tackle such engineering complexity, we propose a layered design of the Paxos-based storage protocol stack, where PaxosLog, the key data structure used in the protocol, is devised to bridge the programming-oriented consistent read/write to the storage-oriented Paxos procedure. Additionally, we present optimizations based on Paxos that made fault-tolerance more efficient. Discussion throughout the paper primarily focuses on pragmatic solutions that could be insightful for building practical distributed storage systems.},
    journal = {Proc. VLDB Endow.},
    month = {aug},
    pages = {1730-1741},
    numpages = {12},
    scholarcluster = {12164791380407440973},
}
@article{Cassandra,
    author = {Lakshman, Avinash and Malik, Prashant},
    title = {Cassandra: a decentralized structured storage system},
    year = {2010},
    issue_date = {April 2010},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    volume = {44},
    number = {2},
    issn = {0163-5980},
    _url = {https://doi.org/10.1145/1773912.1773922},
    _doi = {10.1145/1773912.1773922},
    abstract = {Cassandra is a distributed storage system for managing very large amounts of structured data spread out across many commodity servers, while providing highly available service with no single point of failure. Cassandra aims to run on top of an infrastructure of hundreds of nodes (possibly spread across different data centers). At this scale, small and large components fail continuously. The way Cassandra manages the persistent state in the face of these failures drives the reliability and scalability of the software systems relying on this service. While in many ways Cassandra resembles a database and shares many design and implementation strategies therewith, Cassandra does not support a full relational data model; instead, it provides clients with a simple data model that supports dynamic control over data layout and format. Cassandra system was designed to run on cheap commodity hardware and handle high write throughput while not sacrificing read efficiency.},
    journal = {SIGOPS Oper. Syst. Rev.},
    month = {apr},
    pages = {35-40},
    numpages = {6},
    scholarcluster = {9829178954647343079},
}
@inproceedings{Tempo,
    author = {Enes, Vitor and Baquero, Carlos and Gotsman, Alexey and Sutra, Pierre},
    title = {Efficient replication via timestamp stability},
    year = {2021},
    isbn = {9781450383349},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    _url = {https://doi.org/10.1145/3447786.3456236},
    _doi = {10.1145/3447786.3456236},
    abstract = {Modern web applications replicate their data across the globe and require strong consistency guarantees for their most critical data. These guarantees are usually provided via state-machine replication (SMR). Recent advances in SMR have focused on leaderless protocols, which improve the availability and performance of traditional Paxos-based solutions. We propose Tempo - a leaderless SMR protocol that, in comparison to prior solutions, achieves superior throughput and offers predictable performance even in contended workloads. To achieve these benefits, Tempo timestamps each application command and executes it only after the timestamp becomes stable, i.e., all commands with a lower timestamp are known. Both the timestamping and stability detection mechanisms are fully decentralized, thus obviating the need for a leader replica. Our protocol furthermore generalizes to partial replication settings, enabling scalability in highly parallel workloads. We evaluate the protocol in both real and simulated geo-distributed environments and demonstrate that it outperforms state-of-the-art alternatives.},
    booktitle = {Proceedings of the Sixteenth European Conference on Computer Systems},
    pages = {178-193},
    numpages = {16},
    keywords = {consensus, fault tolerance, geo-replication},
    location = {Online Event, United Kingdom},
    series = {EuroSys '21},
    scholarcluster = {4288377313301185536},
    arxiv = {2104.01142},
}
@misc{Accord,
    title = {CEP-15: Fast General Purpose Transactions}, 
    author = {Benedict, Elliott Smith and Zhang, Tony and Eggleston, Blake and Andreas, Scott},
    year = {2021},
    url = {https://cwiki.apache.org/confluence/download/attachments/188744725/Accord.pdf}, 
}
@inproceedings{VirtualSynchrony,
    author = {Birman, K. and Joseph, T.},
    title = {Exploiting virtual synchrony in distributed systems},
    year = {1987},
    isbn = {089791242X},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    _url = {https://doi.org/10.1145/41457.37515},
    _doi = {10.1145/41457.37515},
    abstract = {We describe applications of a virtually synchronous environment for distributed programming, which underlies a collection of distributed programming tools in the ISIS2 system. A virtually synchronous environment allows processes to be structured into process groups, and makes events like broadcasts to the group as an entity, group membership changes, and even migration of an activity from one place to another appear to occur instantaneously — in other words, synchronously. A major advantage to this approach is that many aspects of a distributed application can be treated independently without compromising correctness. Moreover, user code that is designed as if the system were synchronous can often be executed concurrently. We argue that this approach to building distributed and fault-tolerant software is more straightforward, more flexible, and more likely to yield correct solutions than alternative approaches.},
    booktitle = {Proceedings of the Eleventh ACM Symposium on Operating Systems Principles},
    pages = {123-138},
    numpages = {16},
    location = {Austin, Texas, USA},
    series = {SOSP '87},
    refname = {Virtual Synchrony},
    scholarcluster = {2271986924920893419},
}
@techreport{ViewstampedReplication,
    author = {Barbara Liskov and James Cowling},
    title = {Viewstamped Replication Revisited},
    institution = {MIT},
    number = {MIT-CSAIL-TR-2012-021},
    month = jul,
    year = {2012},
    refname = {Viewstamped Replication},
    scholarcluster = {13000400770252658813},
}
@inproceedings {ErasureCodedRaft,
    author = {Zizhong Wang and Tongliang Li and Haixia Wang and Airan Shao and Yunren Bai and Shangming Cai and Zihan Xu and Dongsheng Wang},
    title = {{CRaft}: An Erasure-coding-supported Version of Raft for Reducing Storage Cost and Network Cost},
    booktitle = {18th USENIX Conference on File and Storage Technologies (FAST 20)},
    year = {2020},
    isbn = {978-1-939133-12-0},
    address = {Santa Clara, CA},
    pages = {297--308},
    _url = {https://www.usenix.org/conference/fast20/presentation/wang-zizhong},
    publisher = {USENIX Association},
    month = feb,
    refname = {Erasure Coded Raft},
    scholarcluster = {10123939731603884260},
}
@INPROCEEDINGS{ErasureCodedHRaft,
    author = {Jia, Yulei and Xu, Guangping and Sung, Chi Wan and Mostafa, Salwa and Wu, Yulei},
    booktitle = {2022 IEEE International Parallel and Distributed Processing Symposium (IPDPS)}, 
    title = {HRaft: Adaptive Erasure Coded Data Maintenance for Consensus in Distributed Networks}, 
    year = {2022},
    volume = {},
    number = {},
    pages = {1316-1326},
    keywords = {Fault tolerance;Costs;System performance;Fault tolerant systems;Distributed databases;Switches;Throughput;Erasure coding;Consensus protocol;Raft;Paxos;Fault tolerance;Network storage},
    _doi = {10.1109/IPDPS53621.2022.00130},
    refname = {Erasure Coded HRaft},
    scholarcluster = {15724086733201598850},
}
@inproceedings{FlexiblePaxos,
    author = {Howard, Heidi and Charapko, Aleksey and Mortier, Richard},
    title = {Fast Flexible Paxos: Relaxing Quorum Intersection for Fast Paxos},
    year = {2021},
    isbn = {9781450389334},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    _url = {https://doi.org/10.1145/3427796.3427815},
    _doi = {10.1145/3427796.3427815},
    abstract = {Paxos, the de facto standard approach to solving distributed consensus, operates in two phases, each of which requires an intersecting quorum of nodes. Multi-Paxos reduces this to one phase by electing a leader but this leader is also a performance bottleneck. Fast Paxos bypasses the leader but has stronger quorum intersection requirements. In this paper we observe that Fast Paxos’ intersection requirements can be safely relaxed, reducing to just one additional intersection requirement between phase-1 quorums and any pair of fast round phase-2 quorums. We thus find that the quorums used with Fast Paxos are larger than necessary, allowing alternative quorum systems to obtain new tradeoffs between performance and fault-tolerance.},
    booktitle = {Proceedings of the 22nd International Conference on Distributed Computing and Networking},
    pages = {186-190},
    numpages = {5},
    keywords = {Distributed computing, Distributed consensus, Paxos},
    location = {Nara, Japan},
    series = {ICDCN '21},
    refname = {Flexible Paxos},
    scholarcluster = {6509870440808150538},
}
@inproceedings {LinearizableQuorumReads,
    author = {Aleksey Charapko and Ailidani Ailijiang and Murat Demirbas},
    title = {Linearizable Quorum Reads in Paxos},
    booktitle = {11th USENIX Workshop on Hot Topics in Storage and File Systems (HotStorage 19)},
    year = {2019},
    address = {Renton, WA},
    _url = {https://www.usenix.org/conference/hotstorage19/presentation/charapko},
    publisher = {USENIX Association},
    month = jul,
    refname = {Linearizable Quorum Reads},
    scholarcluster = {10098760952745259234},
}
@inproceedings{PaxosQuorumLeases,
    author = {Moraru, Iulian and Andersen, David G. and Kaminsky, Michael},
    title = {Paxos Quorum Leases: Fast Reads Without Sacrificing Writes},
    year = {2014},
    isbn = {9781450332521},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    _url = {https://doi.org/10.1145/2670979.2671001},
    _doi = {10.1145/2670979.2671001},
    abstract = {This paper describes quorum leases, a new technique that allows Paxos-based systems to perform reads with high throughput and low latency. Quorum leases do not sacrifice consistency and have only a small impact on system availability and write latency. Quorum leases allow a majority of replicas to perform strongly consistent local reads, which substantially reduces read latency at those replicas (e.g., by two orders of magnitude in wide-area scenarios). Previous techniques for performing local reads in Paxos systems either (a) sacrifice consistency; (b) allow only one replica to read locally; or (c) decrease the availability of the system and increase the latency of all updates by requiring all replicas to be notified synchronously. We describe the design of quorum leases and evaluate their benefits compared to previous approaches through an implementation running in five geo-distributed Amazon EC2 datacenters.},
    booktitle = {Proceedings of the ACM Symposium on Cloud Computing},
    pages = {1-13},
    numpages = {13},
    location = {Seattle, WA, USA},
    series = {SOCC '14},
    refname = {Paxos Quorum Leases},
    scholarcluster = {2618624974148224118},
}
@inproceedings {PullBasedConsensus,
    author = {Siyuan Zhou and Shuai Mu},
    title = {{Fault-Tolerant} Replication with {Pull-Based} Consensus in {MongoDB}},
    booktitle = {18th USENIX Symposium on Networked Systems Design and Implementation (NSDI 21)},
    year = {2021},
    isbn = {978-1-939133-21-2},
    pages = {687--703},
    _url = {https://www.usenix.org/conference/nsdi21/presentation/zhou},
    publisher = {USENIX Association},
    month = apr,
    refname = {Pull-Based Consensus},
    scholarcluster = {3477252701158690968},
}
@inproceedings{ScalingReplication,
    author = {Charapko, Aleksey and Ailijiang, Ailidani and Demirbas, Murat},
    title = {PigPaxos: Devouring the Communication Bottlenecks in Distributed Consensus},
    year = {2021},
    isbn = {9781450383431},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    _url = {https://doi.org/10.1145/3448016.3452834},
    _doi = {10.1145/3448016.3452834},
    abstract = {Strongly consistent replication helps keep application logic simple and provides significant benefits for correctness and manageability. Unfortunately, the adoption of strongly-consistent replication protocols has been curbed due to their limited scalability and performance. To alleviate the leader bottleneck in strongly-consistent replication protocols, we introduce Pig, an in-protocol communication aggregation and piggybacking technique. Pig employs randomly selected nodes from follower subgroups to relay the leader's message to the rest of the followers in the subgroup, and to perform in-network aggregation of acknowledgments back from these followers. By randomly alternating the relay nodes across replication operations, Pig shields the relay nodes as well as the leader from becoming hotspots and improves throughput scalability. We showcase Pig in the context of classical Paxos protocols employed for strongly consistent replication by many cloud computing services and databases. We implement and evaluate PigPaxos, in comparison to Paxos and EPaxos protocols under various workloads over clusters of size 5 to 25 nodes. We show that the aggregation at the relay has little latency overhead, and PigPaxos can provide more than 3 folds improved throughput over Paxos and EPaxos with little latency deterioration. We support our experimental observations with the analytical modeling of the bottlenecks and show that the communication bottlenecks are minimized when employing only one randomly rotating relay node.},
    booktitle = {Proceedings of the 2021 International Conference on Management of Data},
    pages = {235-247},
    numpages = {13},
    keywords = {distributed consensus, linearizability, paxos, replication},
    location = {Virtual Event, China},
    series = {SIGMOD '21},
    refname = {Scaling Replication},
    scholarcluster = {1909096821088376701},
}
@inproceedings {CommutativeRaft,
    author = {Seo Jin Park and John Ousterhout},
    title = {Exploiting Commutativity For Practical Fast Replication},
    booktitle = {16th USENIX Symposium on Networked Systems Design and Implementation (NSDI 19)},
    year = {2019},
    isbn = {978-1-931971-49-2},
    address = {Boston, MA},
    pages = {47--64},
    _url = {https://www.usenix.org/conference/nsdi19/presentation/park},
    publisher = {USENIX Association},
    month = feb,
    refname = {Commutative Raft},
    scholarcluster = {3451458773692631815},
}
@phdthesis{PrimaryBackup,
    author = {Budhiraja, Navin},
    title = {The primary-backup approach: lower and upper bounds},
    year = {1993},
    publisher = {Cornell University},
    address = {USA},
    abstract = {The most widely used approach to building replicated, fault-tolerant services is the primary-backup approach. In this approach, the state of the service is replicated across multiple servers, with one server designated as the primary, and the rest as backups. Clients send requests only to the primary. However, in case the primary fails, one of the backups takes over as the new primary. Ever since it was introduced in 1976 by Alsberg and Day, the primary-backup approach has become the basis for building many practical fault-tolerant services. However, despite the widespread use, the approach has not been studied systematically, and little is known of the fundamental costs and tradeoffs of using the approach under various kinds of failures. Thus there is a gap between theory and practice. In order to close this gap, this thesis analyzes the primary-backup approach, both from the theoretical perspective of specification, lower bounds and upper bounds, as well as from the practical viewpoint of performance tradeoffs in protocols. We identify three key cost metrics of primary-backup protocols--degree of replication, blocking time and failover time--and then show lower and upper bounds on these metrics for a hierarchy of failure models. We then implement an important subclass of our primary-backup protocols, called 0-blocking protocols, and give performance figures. In addition to leading to the development of new, more efficient protocols, we believe that the work in this thesis has resulted in a better understanding of the properties of existing primary-backup protocols.},
    note = {UMI Order No. GAX94-06131},
    refname = {Primary-Backup},
    scholarcluster = {15584274088478098258},
}
@techreport{VerticalPaxosII,
    author = {Lamport, Leslie and Malkhi, Dahlia and Zhou, Lidong},
    title = {Vertical Paxos and Primary-Backup Replication},
    year = {2009},
    month = {May},
    abstract = {We introduce a class of Paxos algorithms called Vertical Paxos, in which reconfiguration can occur in the middle of reaching agreement on an individual state-machine command. Vertical Paxos algorithms use an auxiliary configuration master that facilitates agreement on reconfiguration. A special case of these algorithms leads to traditional primary-backup protocols. We show how primary-backup systems in current use can be viewed, and shown to be correct, as instances of Vertical Paxos algorithms.

    This paper came out of much discussion between Malkhi, Zhou, and myself about reconfiguration. Some day, what we did may result in a long paper about state-machine reconfiguration containing these results and others that have not yet been published. The ideas here are related to the original, unpublished version of [151].},
    publisher = {Microsoft},
    _url = {https://www.microsoft.com/en-us/research/publication/vertical-paxos-and-primary-backup-replication/},
    number = {MSR-TR-2009-63},
    refname = {Vertical Paxos II},
    scholarcluster = {12255443511267289537},
}
@inproceedings{ChainReplication,
    author = {Robbert Van Renesse and Fred B. Schneider},
    title = {Chain Replication for Supporting High Throughput and Availability},
    booktitle = {6th Symposium on Operating Systems Design \& Implementation (OSDI 04)},
    year = {2004},
    address = {San Francisco, CA},
    _url = {https://www.usenix.org/conference/osdi-04/chain-replication-supporting-high-throughput-and-availability},
    publisher = {USENIX Association},
    month = dec,
    refname = {Chain Replication},
    scholarcluster = {3366172945601823602},
}
@inproceedings {CRAQ,
    author = {Jeff Terrace and Michael J. Freedman},
    title = {Object Storage on {CRAQ}: {High-Throughput} Chain Replication for {Read-Mostly} Workloads},
    booktitle = {2009 USENIX Annual Technical Conference (USENIX ATC 09)},
    year = {2009},
    address = {San Diego, CA},
    _url = {https://www.usenix.org/conference/usenix-09/object-storage-craq-high-throughput-chain-replication-read-mostly-workloads},
    publisher = {USENIX Association},
    month = jun,
    scholarcluster = {9297968548710093419},
}
@techreport{PacificA,
    author = {Lin, Wei and Yang, Mao and Zhang, Lintao and Zhou, Lidong},
    title = {PacificA: Replication in Log-Based Distributed Storage Systems},
    year = {2008},
    month = {February},
    abstract = {Large-scale distributed storage systems have gained popularity for storing and processing ever increasing amount of data. Replication mechanisms are often key to achieving high availability and high throughput in such systems. Research on fundamental problems such as consensus has laid out a solid foundation for replication protocols. Yet, both the architectural design and engineering issues of practical replication mechanisms remain an art. This paper describes our experience in designing and implementing replication for commonly used log-based storage systems. We advocate a general replication framework that is simple, practical, and strongly consistent. We show that the framework is flexible enough to accommodate a variety of different design choices that we explore. Using a prototype system called PacificA, we implemented three different replication strategies, all using the same replication framework. The paper reports detailed performance evaluation results, especially on system behavior during failure, reconciliation, and recovery.},
    _url = {https://www.microsoft.com/en-us/research/publication/pacifica-replication-in-log-based-distributed-storage-systems/},
    pages = {14},
    number = {MSR-TR-2008-25},
    scholarcluster = {15826444170581946812},
}
@inproceedings{Hermes,
    author = {Katsarakis, Antonios and Gavrielatos, Vasilis and Katebzadeh, M.R. Siavash and Joshi, Arpit and Dragojevic, Aleksandar and Grot, Boris and Nagarajan, Vijay},
    title = {Hermes: A Fast, Fault-Tolerant and Linearizable Replication Protocol},
    year = {2020},
    isbn = {9781450371025},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    _url = {https://doi.org/10.1145/3373376.3378496},
    _doi = {10.1145/3373376.3378496},
    abstract = {Today's datacenter applications are underpinned by datastores that are responsible for providing availability, consistency, and performance. For high availability in the presence of failures, these datastores replicate data across several nodes. This is accomplished with the help of a reliable replication protocol that is responsible for maintaining the replicas strongly-consistent even when faults occur. Strong consistency is preferred to weaker consistency models that cannot guarantee an intuitive behavior for the clients. Furthermore, to accommodate high demand at real-time latencies, datastores must deliver high throughput and low latency.This work introduces Hermes, a broadcast-based reliable replication protocol for in-memory datastores that provides both high throughput and low latency by enabling local reads and fully-concurrent fast writes at all replicas. Hermes couples logical timestamps with cache-coherence-inspired invalidations to guarantee linearizability, avoid write serialization at a centralized ordering point, resolve write conflicts locally at each replica (hence ensuring that writes never abort) and provide fault-tolerance via replayable writes. Our implementation of Hermes over an RDMA-enabled reliable datastore with five replicas shows that Hermes consistently achieves higher throughput than state-of-the-art RDMA-based reliable protocols (ZAB and CRAQ) across all write ratios while also significantly reducing tail latency. At 5\% writes, the tail latency of Hermes is 3.6X lower than that of CRAQ and ZAB.},
    booktitle = {Proceedings of the Twenty-Fifth International Conference on Architectural Support for Programming Languages and Operating Systems},
    pages = {201-217},
    numpages = {17},
    keywords = {throughput, replication, rdma, linearizability, latency, fault-tolerant, consistency, availability},
    location = {Lausanne, Switzerland},
    series = {ASPLOS '20},
    scholarcluster = {13608264111814513293},
}
@inproceedings{HyperDex,
    author = {Escriva, Robert and Wong, Bernard and Sirer, Emin Gun},
    title = {HyperDex: a distributed, searchable key-value store},
    year = {2012},
    isbn = {9781450314190},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    _url = {https://doi.org/10.1145/2342356.2342360},
    _doi = {10.1145/2342356.2342360},
    abstract = {Distributed key-value stores are now a standard component of high-performance web services and cloud computing applications. While key-value stores offer significant performance and scalability advantages compared to traditional databases, they achieve these properties through a restricted API that limits object retrieval---an object can only be retrieved by the (primary and only) key under which it was inserted. This paper presents HyperDex, a novel distributed key-value store that provides a unique search primitive that enables queries on secondary attributes. The key insight behind HyperDex is the concept of hyperspace hashing in which objects with multiple attributes are mapped into a multidimensional hyperspace. This mapping leads to efficient implementations not only for retrieval by primary key, but also for partially-specified secondary attribute searches and range queries. A novel chaining protocol enables the system to achieve strong consistency, maintain availability and guarantee fault tolerance. An evaluation of the full system shows that HyperDex is 12-13x faster than Cassandra and MongoDB for finding partially specified objects. Additionally, HyperDex achieves 2-4x higher throughput for get/put operations.},
    booktitle = {Proceedings of the ACM SIGCOMM 2012 Conference on Applications, Technologies, Architectures, and Protocols for Computer Communication},
    pages = {25-36},
    numpages = {12},
    keywords = {strong consistency, performance, nosql, key-value store, fault-tolerance},
    location = {Helsinki, Finland},
    series = {SIGCOMM '12},
    scholarcluster = {8838739194584316753},
}
@inproceedings{Kafka,
    title = {Kafka: A distributed messaging system for log processing},
    author = {Kreps, Jay and Narkhede, Neha and Rao, Jun and others},
    booktitle = {Proceedings of the NetDB},
    volume = {11},
    number = {2011},
    pages = {1--7},
    year = {2011},
    organization = {Athens, Greece},
    scholarcluster = {5891925114546481347},
}
@inproceedings{FoundationDB,
    author = {Zhou, Jingyu and Xu, Meng and Shraer, Alexander and Namasivayam, Bala and Miller, Alex and Tschannen, Evan and Atherton, Steve and Beamon, Andrew J. and Sears, Rusty and Leach, John and Rosenthal, Dave and Dong, Xin and Wilson, Will and Collins, Ben and Scherer, David and Grieser, Alec and Liu, Young and Moore, Alvin and Muppana, Bhaskar and Su, Xiaoge and Yadav, Vishesh},
    title = {FoundationDB: A Distributed Unbundled Transactional Key Value Store},
    year = {2021},
    isbn = {9781450383431},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    _url = {https://doi.org/10.1145/3448016.3457559},
    _doi = {10.1145/3448016.3457559},
    abstract = {FoundationDB is an open source transactional key value store created more than ten years ago. It is one of the first systems to combine the flexibility and scalability of NoSQL architectures with the power of ACID transactions (a.k.a. NewSQL). FoundationDB adopts an unbundled architecture that decouples an in-memory transaction management system, a distributed storage system, and a built-in distributed configuration system. Each sub-system can be independently provisioned and configured to achieve the desired scalability, high-availability and fault tolerance properties. FoundationDB uniquely integrates a deterministic simulation framework, used to test every new feature of the system under a myriad of possible faults. This rigorous testing makes FoundationDB extremely stable and allows developers to introduce and release new features in a rapid cadence. FoundationDB offers a minimal and carefully chosen feature set, which has enabled a range of disparate systems (from semi-relational databases, document and object stores, to graph databases and more) to be built as layers on top. FoundationDB is the underpinning of cloud infrastructure at Apple, Snowflake and other companies, due to its consistency, robustness and availability for storing user data, system metadata and configuration, and other critical information.},
    booktitle = {Proceedings of the 2021 International Conference on Management of Data},
    pages = {2653-2666},
    numpages = {14},
    keywords = {multiversion concurrency control, oltp, optimistic concurrency control, simulation testing, strict serializability, unbundled database},
    location = {Virtual Event, China},
    series = {SIGMOD '21},
    scholarcluster = {4197497039785350505},
}
@inproceedings{RSPaxos,
    author = {Mu, Shuai and Chen, Kang and Wu, Yongwei and Zheng, Weimin},
    title = {When paxos meets erasure code: reduce network and storage cost in state machine replication},
    year = {2014},
    isbn = {9781450327497},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    _url = {https://doi.org/10.1145/2600212.2600218},
    _doi = {10.1145/2600212.2600218},
    abstract = {Paxos-based state machine replication is a key technique to build highly reliable and available distributed services, such as lock servers, databases and other data storage systems. Paxos can tolerate any minority number of node crashes in an asynchronous network environment. Traditionally, Paxos is used to perform a full copy replication across all participants. However, full copy is expensive both in term of network and storage cost, especially in wide area with commodity hard drives.In this paper, we discussed the non-triviality and feasibility of combining erasure code into Paxos protocol, and presented an improved protocol named RS-Paxos (Reed Solomon Paxos). To the best of our knowledge, we are the first to propose such a combination. Compared to Paxos, RS-Paxos requires a limitation on the number of possible failures. If the number of tolerated failures decreases by 1, RS-Paxos can save over 50\% of network transmission and disk I/O. To demonstrate the benefits of our protocol, we designed and built a key-value store based on RS-Paxos, and evaluated it on EC2 with various settings. Experiment results show that RS-Paxos achieves at most 2.5x improvement on write throughput and as much as 30\% reduction on latency, in common configurations.},
    booktitle = {Proceedings of the 23rd International Symposium on High-Performance Parallel and Distributed Computing},
    pages = {61-72},
    numpages = {12},
    keywords = {asynchronous message passing model, consensus, erasure code, paxos, state machine replication},
    location = {Vancouver, BC, Canada},
    series = {HPDC '14},
    refname = {RS-Paxos},
    scholarcluster = {16520033292975033789},
}
@inproceedings{GrayFailureAchillesHeel,
    author = {Huang, Peng and Guo, Chuanxiong and Zhou, Lidong and Lorch, Jacob R. and Dang, Yingnong and Chintalapati, Murali and Yao, Randolph},
    title = {Gray Failure: The Achilles' Heel of Cloud-Scale Systems},
    year = {2017},
    isbn = {9781450350686},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    _url = {https://doi.org/10.1145/3102980.3103005},
    _doi = {10.1145/3102980.3103005},
    abstract = {Cloud scale provides the vast resources necessary to replace failed components, but this is useful only if those failures can be detected. For this reason, the major availability breakdowns and performance anomalies we see in cloud environments tend to be caused by subtle underlying faults, i.e., gray failure rather than fail-stop failure. In this paper, we discuss our experiences with gray failure in production cloud-scale systems to show its broad scope and consequences. We also argue that a key feature of gray failure is differential observability: that the system's failure detectors may not notice problems even when applications are afflicted by them. This realization leads us to believe that, to best deal with them, we should focus on bridging the gap between different components' perceptions of what constitutes failure.},
    booktitle = {Proceedings of the 16th Workshop on Hot Topics in Operating Systems},
    pages = {150-155},
    numpages = {6},
    location = {Whistler, BC, Canada},
    series = {HotOS '17},
    refname = {Gray Failure Achilles Heel},
    scholarcluster = {4369373863260707505},
}
@inproceedings{ScalableButWasteful,
    title = {Scalable but wasteful: Current state of replication in the cloud},
    author = {Matte, Venkata Swaroop and Charapko, Aleksey and Aghayev, Abutalib},
    booktitle = {Proceedings of the 13th ACM Workshop on Hot Topics in Storage and File Systems},
    pages = {42--49},
    year = {2021},
    refname = {Scalable But Wasteful},
    scholarcluster = {16327886782851538912},
}
@inproceedings{EPaxos,
    author = {Moraru, Iulian and Andersen, David G. and Kaminsky, Michael},
    title = {There is more consensus in Egalitarian parliaments},
    year = {2013},
    isbn = {9781450323888},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    _url = {https://doi.org/10.1145/2517349.2517350},
    _doi = {10.1145/2517349.2517350},
    abstract = {This paper describes the design and implementation of Egalitarian Paxos (EPaxos), a new distributed consensus algorithm based on Paxos. EPaxos achieves three goals: (1) optimal commit latency in the wide-area when tolerating one and two failures, under realistic conditions; (2) uniform load balancing across all replicas (thus achieving high throughput); and (3) graceful performance degradation when replicas are slow or crash.Egalitarian Paxos is to our knowledge the first protocol to achieve the previously stated goals efficiently---that is, requiring only a simple majority of replicas to be non-faulty, using a number of messages linear in the number of replicas to choose a command, and committing commands after just one communication round (one round trip) in the common case or after at most two rounds in any case. We prove Egalitarian Paxos's properties theoretically and demonstrate its advantages empirically through an implementation running on Amazon EC2.},
    booktitle = {Proceedings of the Twenty-Fourth ACM Symposium on Operating Systems Principles},
    pages = {358-372},
    numpages = {15},
    location = {Farminton, Pennsylvania},
    series = {SOSP '13},
    scholarcluster = {13655117037814714535},
}
@inproceedings{Ceph,
    author = {Weil, Sage A. and Leung, Andrew W. and Brandt, Scott A. and Maltzahn, Carlos},
    title = {RADOS: a scalable, reliable storage service for petabyte-scale storage clusters},
    year = {2007},
    isbn = {9781595938992},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    _url = {https://doi.org/10.1145/1374596.1374606},
    _doi = {10.1145/1374596.1374606},
    abstract = {Brick and object-based storage architectures have emerged as a means of improving the scalability of storage clusters. However, existing systems continue to treat storage nodes as passive devices, despite their ability to exhibit significant intelligence and autonomy. We present the design and implementation of RADOS, a reliable object storage service that can scales to many thousands of devices by leveraging the intelligence present in individual storage nodes. RADOS preserves consistent data access and strong safety semantics while allowing nodes to act semi-autonomously to self-manage replication, failure detection, and failure recovery through the use of a small cluster map. Our implementation offers excellent performance, reliability, and scalability while providing clients with the illusion of a single logical object store.},
    booktitle = {Proceedings of the 2nd International Workshop on Petascale Data Storage: Held in Conjunction with Supercomputing '07},
    pages = {35-44},
    numpages = {10},
    keywords = {petabyte-scale storage, object-based storage, clustered storage},
    location = {Reno, Nevada},
    series = {PDSW '07},
    scholarcluster = {12064684978898371724},
}
@article{GFS,
    author = {Ghemawat, Sanjay and Gobioff, Howard and Leung, Shun-Tak},
    title = {The Google file system},
    year = {2003},
    issue_date = {December 2003},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    volume = {37},
    number = {5},
    issn = {0163-5980},
    _url = {https://doi.org/10.1145/1165389.945450},
    _doi = {10.1145/1165389.945450},
    journal = {SIGOPS Oper. Syst. Rev.},
    month = {oct},
    pages = {29-43},
    numpages = {15},
    keywords = {clustered storage, data storage, fault tolerance, scalability},
    scholarcluster = {98210925508218371},
}
@inproceedings{HDFS,
    author = {Shvachko, Konstantin and Kuang, Hairong and Radia, Sanjay and Chansler, Robert},
    booktitle = {2010 IEEE 26th Symposium on Mass Storage Systems and Technologies (MSST)}, 
    title = {The Hadoop Distributed File System}, 
    year = {2010},
    volume = {},
    number = {},
    pages = {1-10},
    keywords = {File systems;Bandwidth;Concurrent computing;Distributed computing;Computer architecture;File servers;Facebook;Protocols;Protection;Clustering algorithms;Hadoop;HDFS;distributed file system},
    _doi = {10.1109/MSST.2010.5496972},
    scholarcluster = {5895019987520378688},
}
@inproceedings {AlibabaEBS,
    author = {Weidong Zhang and Erci Xu and Qiuping Wang and Xiaolu Zhang and Yuesheng Gu and Zhenwei Lu and Tao Ouyang and Guanqun Dai and Wenwen Peng and Zhe Xu and Shuo Zhang and Dong Wu and Yilei Peng and Tianyun Wang and Haoran Zhang and Jiasheng Wang and Wenyuan Yan and Yuanyuan Dong and Wenhui Yao and Zhongjie Wu and Lingjun Zhu and Chao Shi and Yinhu Wang and Rong Liu and Junping Wu and Jiaji Zhu and Jiesheng Wu},
    title = {What's the Story in {EBS} Glory: Evolutions and Lessons in Building Cloud Block Store},
    booktitle = {22nd USENIX Conference on File and Storage Technologies (FAST 24)},
    year = {2024},
    isbn = {978-1-939133-38-0},
    address = {Santa Clara, CA},
    pages = {277-291},
    _url = {https://www.usenix.org/conference/fast24/presentation/zhang-weidong},
    publisher = {USENIX Association},
    month = feb,
    refname = {Alibaba EFS},
    scholarcluster = {8248321054680879292},
}
@article{PolarFS,
    author = {Cao, Wei and Liu, Zhenjun and Wang, Peng and Chen, Sen and Zhu, Caifeng and Zheng, Song and Wang, Yuhui and Ma, Guoqing},
    title = {PolarFS: an ultra-low latency and failure resilient distributed file system for shared storage cloud database},
    year = {2018},
    issue_date = {August 2018},
    publisher = {VLDB Endowment},
    volume = {11},
    number = {12},
    issn = {2150-8097},
    _url = {https://doi.org/10.14778/3229863.3229872},
    _doi = {10.14778/3229863.3229872},
    abstract = {PolarFS is a distributed file system with ultra-low latency and high availability, designed for the POLARDB database service, which is now available on the Alibaba Cloud. PolarFS utilizes a lightweight network stack and I/O stack in user-space, taking full advantage of the emerging techniques like RDMA, NVMe, and SPDK. In this way, the end-to-end latency of PolarFS has been reduced drastically and our experiments show that the write latency of PolarFS is quite close to that of local file system on SSD. To keep replica consistency while maximizing I/O throughput for PolarFS, we develop ParallelRaft, a consensus protocol derived from Raft, which breaks Raft's strict serialization by exploiting the out-of-order I/O completion tolerance capability of databases. ParallelRaft inherits the understand-ability and easy implementation of Raft while providing much better I/O scalability for PolarFS. We also describe the shared storage architecture of PolarFS, which gives a strong support for POLARDB.},
    journal = {Proc. VLDB Endow.},
    month = {aug},
    pages = {1849-1862},
    numpages = {14},
    scholarcluster = {4921679856437073694},
}
@inproceedings {LimpingTolerantClouds,
    author = {Thanh Do and Haryadi S. Gunawi},
    title = {The Case for {Limping-Hardware} Tolerant Clouds},
    booktitle = {5th USENIX Workshop on Hot Topics in Cloud Computing (HotCloud 13)},
    year = {2013},
    address = {San Jose, CA},
    _url = {https://www.usenix.org/conference/hotcloud13/workshop-program/presentations/do},
    publisher = {USENIX Association},
    month = jun,
    refname = {Limping Hardware Tolerant Clouds},
    scholarcluster = {9138890906893078068},
}
@misc{CASPaxos,
    title = {CASPaxos: Replicated State Machines without logs}, 
    author = {Denis Rystsov},
    year = {2018},
    eprint = {1802.07000},
    archivePrefix = {arXiv},
    primaryClass = {cs.DC},
    _url = {https://arxiv.org/abs/1802.07000}, 
    scholarcluster = {1991685638971222231},
    arxiv = {1802.07000},
}
@misc{MultiPaxosMadeComplete,
    title = {MultiPaxos Made Complete}, 
    author = {Zhiying Liang and Vahab Jabrayilov and Aleksey Charapko and Abutalib Aghayev},
    year = {2024},
    eprint = {2405.11183},
    archivePrefix = {arXiv},
    primaryClass = {cs.DC},
    _url = {https://arxiv.org/abs/2405.11183}, 
    refname = {MultiPaxos Made Complete},
    scholarcluster = {9564126909260879908},
    arxiv = {2405.11183},
}
@InProceedings{MultiPaxos,
    author = {De Prisco, Roberto and Lampson, Butler and Lynch, Nancy},
    editor = {Mavronicolas, Marios and Tsigas, Philippas},
    title = {Revisiting the Paxos algorithm},
    booktitle = {Distributed Algorithms},
    year = {1997},
    publisher = {Springer Berlin Heidelberg},
    address = {Berlin, Heidelberg},
    pages = {111--125},
    abstract = {This paper develops a new I/O automaton model called the Clock General Timed Automaton (Clock GTA) model. The Clock GTA is based on the General Timed Automaton (GTA) of Lynch and Vaandrager. The Clock GTA provides a systematic way of describing timing-based systems in which there is a notion of ``normal'' timing behavior, but that do not necessarily always exhibit this ``normal'' behavior. It can be used for practical time performance analysis based on the stabilization of the physical system.},
    isbn = {978-3-540-69600-1},
    scholarcluster = {5465885118770091369},
}
@inproceedings{Quoracle,
    author = {Whittaker, Michael and Charapko, Aleksey and Hellerstein, Joseph M. and Howard, Heidi and Stoica, Ion},
    title = {Read-Write Quorum Systems Made Practical},
    year = {2021},
    isbn = {9781450383387},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    _url = {https://mwhittaker.github.io/publications/quoracle.html},
    _doi = {10.1145/3447865.3457962},
    abstract = {Quorum systems are a powerful mechanism for ensuring the consistency of replicated data. Production systems usually opt for majority quorums due to their simplicity and fault tolerance, but majority quorum systems provide poor throughput and scalability. Alternatively, researchers have invented a number of theoretically "optimal" quorum systems, but the underlying theory ignores many practical complexities such as machine heterogeneity and workload skew. In this paper, we conduct a pragmatic re-examination of quorum systems. We enrich the current theory on quorum systems with a number of practical refinements to find quorum systems that provide higher throughput, lower latency, and lower network load. We also develop a library Quoracle that precisely quantifies the available trade-offs between quorum systems to empower engineers to find optimal quorum systems, given a set of objectives for specific deployments and workloads. Our tool is available online at: https://github.com/mwhittaker/quoracle.},
    booktitle = {Proceedings of the 8th Workshop on Principles and Practice of Consistency for Distributed Data},
    articleno = {7},
    numpages = {8},
    keywords = {Consensus, Distributed Systems, Quorum Systems, Read-write Quorum Systems, State Machine Replication},
    location = {Online, United Kingdom},
    series = {PaPoC '21},
    scholarcluster = {3478742809774653989}
}
@article{ReconfigurationTutorial,
    author = {Aguilera, Marcos K. and Keidar, Idit and Malkhi, Dahlia and Martin, Jean-Philippe and Shraer, Alexander},
    title = {Reconfiguring Replicated Atomic Storage: A Tutorial},
    year = {2010},
    month = {October},
    abstract = {We live in a world of Internet services such as email, social networks, web searching, and more, which must store increasingly larger volumes of data. These services must run on cheap infrastructure, hence they must use distributed storage systems; and they have to provide reliability of data for long periods as well as availability, hence they must support online reconfiguration to remove failed nodes and add healthy ones. The knowledge needed to implement online reconfiguration is subtle and simple techniques often fail to work well. This tutorial provides an introductory overview of this topic, including a description of the main technical challenges, as well as the various approaches that are used to address these challenges.},
    publisher = {European Association for Theoretical Computer Science},
    _url = {https://www.microsoft.com/en-us/research/publication/reconfiguring-replicated-atomic-storage-a-tutorial/},
    journal = {Bulletin of the EATCS: The Distributed Computing Column},
    refname = {Reconfiguring Replicated Storage Tutorial},
    scholarcluster = {1032413045313132237},
}
@inproceedings {Gryff,
    author = {Matthew Burke and Audrey Cheng and Wyatt Lloyd},
    title = {Gryff: Unifying Consensus and Shared Registers },
    booktitle = {17th USENIX Symposium on Networked Systems Design and Implementation (NSDI 20)},
    year = {2020},
    isbn = {978-1-939133-13-7},
    address = {Santa Clara, CA},
    pages = {591--617},
    _url = {https://www.usenix.org/conference/nsdi20/presentation/burke},
    publisher = {USENIX Association},
    month = feb,
    scholarcluster = {7190613653689476521},
}
@inproceedings{Falcon,
    author = {Leners, Joshua B. and Wu, Hao and Hung, Wei-Lun and Aguilera, Marcos K. and Walfish, Michael},
    title = {Detecting failures in distributed systems with the Falcon spy network},
    year = {2011},
    isbn = {9781450309776},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    _url = {https://doi.org/10.1145/2043556.2043583},
    _doi = {10.1145/2043556.2043583},
    abstract = {A common way for a distributed system to tolerate crashes is to explicitly detect them and then recover from them. Interestingly, detection can take much longer than recovery, as a result of many advances in recovery techniques, making failure detection the dominant factor in these systems' unavailability when a crash occurs.This paper presents the design, implementation, and evaluation of Falcon, a failure detector with several features. First, Falcon's common-case detection time is sub-second, which keeps unavailability low. Second, Falcon is reliable: it never reports a process as down when it is actually up. Third, Falcon sometimes kills to achieve reliable detection but aims to kill the smallest needed component. Falcon achieves these features by coordinating a network of spies, each monitoring a layer of the system. Falcon's main cost is a small amount of platform-specific logic. Falcon is thus the first failure detector that is fast, reliable, and viable. As such, it could change the way that a class of distributed systems is built.},
    booktitle = {Proceedings of the Twenty-Third ACM Symposium on Operating Systems Principles},
    pages = {279-294},
    numpages = {16},
    keywords = {reliable detection, layer-specific probes, layer-specific monitors, high availability, failure detectors, STONITH},
    location = {Cascais, Portugal},
    series = {SOSP '11},
    scholarcluster = {10985406861603977252},
}
@inproceedings {LocalizingPartialFailures,
    author = {Chang Lou and Peng Huang and Scott Smith},
    title = {Understanding, Detecting and Localizing Partial Failures in Large System Software },
    booktitle = {17th USENIX Symposium on Networked Systems Design and Implementation (NSDI 20)},
    year = {2020},
    isbn = {978-1-939133-13-7},
    address = {Santa Clara, CA},
    pages = {559--574},
    _url = {https://www.usenix.org/conference/nsdi20/presentation/lou},
    publisher = {USENIX Association},
    month = feb,
    refname = {Localizing Partial Failures},
    scholarcluster = {2530981925243201785},
}
@article{MatchmakerPaxos,
  type = {Solution},
  title = {Matchmaker {{Paxos}}: {{A Reconfigurable Consensus Protocol}}},
  author = {Whittaker, Michael and Giridharan, Neil and Szekeres, Adriana and Hellerstein, Joseph and Howard, Heidi and Nawab, Faisal and Stoica, Ion},
  year = {2021},
  month = sep,
  journal = {Journal of Systems Research},
  volume = {1},
  number = {1},
  issn = {2770-5501},
  _doi = {10.5070/SR31154842},
  _url = {https://escholarship.org/uc/item/8wk3343k},
  urldate = {2021-09-21},
  area = {Distributed Consensus},
  artifacts_url = {https://github.com/mwhittaker/frankenpaxos/},
  langid = {english},
  review_url = {https://openreview.net/forum?id=bXe1agiq9LN}
  refname = {Matchmaker Paxos},
  scholarcluster = {17247972881109586860},
  arxiv = {2007.09468},
}
