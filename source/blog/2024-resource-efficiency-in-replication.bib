@INPROCEEDINGS{740474,
    author={Malkhi, D. and Reiter, M.K.},
    booktitle={Proceedings Seventeenth IEEE Symposium on Reliable Distributed Systems (Cat. No.98CB36281)}, 
    title={Secure and scalable replication in Phalanx}, 
    year={1998},
    volume={},
    number={},
    pages={51-58},
    keywords={Public key;Voting;Identity-based encryption;Publishing;Reactive power;Application software;Software systems;Buildings;Large-scale systems;Online services},
    doi={10.1109/RELDIS.1998.740474}
}
@inproceedings{10.5555/2643634.2643666,
    author = {Ongaro, Diego and Ousterhout, John},
    title = {In search of an understandable consensus algorithm},
    year = {2014},
    isbn = {9781931971102},
    publisher = {USENIX Association},
    address = {USA},
    abstract = {Raft is a consensus algorithm for managing a replicated log. It produces a result equivalent to (multi-)Paxos, and it is as efficient as Paxos, but its structure is different from Paxos; this makes Raft more understandable than Paxos and also provides a better foundation for building practical systems. In order to enhance understandability, Raft separates the key elements of consensus, such as leader election, log replication, and safety, and it enforces a stronger degree of coherency to reduce the number of states that must be considered. Results from a user study demonstrate that Raft is easier for students to learn than Paxos. Raft also includes a new mechanism for changing the cluster membership, which uses overlapping majorities to guarantee safety.},
    booktitle = {Proceedings of the 2014 USENIX Conference on USENIX Annual Technical Conference},
    pages = {305-320},
    numpages = {16},
    location = {Philadelphia, PA},
    series = {USENIX ATC'14}
}
@article{10.1145/2673577,
    author = {Van Renesse, Robbert and Altinbuken, Deniz},
    title = {Paxos Made Moderately Complex},
    year = {2015},
    issue_date = {April 2015},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    volume = {47},
    number = {3},
    issn = {0360-0300},
    url = {https://doi.org/10.1145/2673577},
    doi = {10.1145/2673577},
    abstract = {This article explains the full reconfigurable multidecree Paxos (or multi-Paxos) protocol. Paxos is by no means a simple protocol, even though it is based on relatively simple invariants. We provide pseudocode and explain it guided by invariants. We initially avoid optimizations that complicate comprehension. Next we discuss liveness, list various optimizations that make the protocol practical, and present variants of the protocol.},
    journal = {ACM Comput. Surv.},
    month = feb,
    articleno = {42},
    numpages = {36},
    keywords = {consensus, voting, Replicated state machines}
}
@inproceedings{10.1109/DSN.2011.5958223,
    author = {Junqueira, Flavio P. and Reed, Benjamin C. and Serafini, Marco},
    title = {Zab: High-performance broadcast for primary-backup systems},
    year = {2011},
    isbn = {9781424492329},
    publisher = {IEEE Computer Society},
    address = {USA},
    url = {https://doi.org/10.1109/DSN.2011.5958223},
    doi = {10.1109/DSN.2011.5958223},
    abstract = {Zab is a crash-recovery atomic broadcast algorithm we designed for the ZooKeeper coordination service. ZooKeeper implements a primary-backup scheme in which a primary process executes clients operations and uses Zab to propagate the corresponding incremental state changes to backup processes1. Due the dependence of an incremental state change on the sequence of changes previously generated, Zab must guarantee that if it delivers a given state change, then all other changes it depends upon must be delivered first. Since primaries may crash, Zab must satisfy this requirement despite crashes of primaries.},
    booktitle = {Proceedings of the 2011 IEEE/IFIP 41st International Conference on Dependable Systems&Networks},
    pages = {245-256},
    numpages = {12},
    series = {DSN '11}
}
@inproceedings{10.1145/3318464.3386134,
    author = {Taft, Rebecca and Sharif, Irfan and Matei, Andrei and VanBenschoten, Nathan and Lewis, Jordan and Grieger, Tobias and Niemi, Kai and Woods, Andy and Birzin, Anne and Poss, Raphael and Bardea, Paul and Ranade, Amruta and Darnell, Ben and Gruneir, Bram and Jaffray, Justin and Zhang, Lucy and Mattis, Peter},
    title = {CockroachDB: The Resilient Geo-Distributed SQL Database},
    year = {2020},
    isbn = {9781450367356},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    url = {https://doi.org/10.1145/3318464.3386134},
    doi = {10.1145/3318464.3386134},
    abstract = {We live in an increasingly interconnected world, with many organizations operating across countries or even continents. To serve their global user base, organizations are replacing their legacy DBMSs with cloud-based systems capable of scaling OLTP workloads to millions of users. CockroachDB is a scalable SQL DBMS that was built from the ground up to support these global OLTP workloads while maintaining high availability and strong consistency. Just like its namesake, CockroachDB is resilient to disasters through replication and automatic recovery mechanisms. This paper presents the design of CockroachDB and its novel transaction model that supports consistent geo-distributed transactions on commodity hardware. We describe how CockroachDB replicates and distributes data to achieve fault tolerance and high performance, as well as how its distributed SQL layer automatically scales with the size of the database cluster while providing the standard SQL interface that users expect. Finally, we present a comprehensive performance evaluation and share a couple of case studies of CockroachDB users. We conclude by describing lessons learned while building CockroachDB over the last five years.},
    booktitle = {Proceedings of the 2020 ACM SIGMOD International Conference on Management of Data},
    pages = {1493-1509},
    numpages = {17},
    location = {Portland, OR, USA},
    series = {SIGMOD '20}
}
@article{10.14778/3415478.3415535,
    author = {Huang, Dongxu and Liu, Qi and Cui, Qiu and Fang, Zhuhe and Ma, Xiaoyu and Xu, Fei and Shen, Li and Tang, Liu and Zhou, Yuxing and Huang, Menglong and Wei, Wan and Liu, Cong and Zhang, Jian and Li, Jianjun and Wu, Xuelian and Song, Lingyu and Sun, Ruoxi and Yu, Shuaipeng and Zhao, Lei and Cameron, Nicholas and Pei, Liquan and Tang, Xin},
    title = {TiDB: a Raft-based HTAP database},
    year = {2020},
    issue_date = {August 2020},
    publisher = {VLDB Endowment},
    volume = {13},
    number = {12},
    issn = {2150-8097},
    url = {https://doi.org/10.14778/3415478.3415535},
    doi = {10.14778/3415478.3415535},
    abstract = {Hybrid Transactional and Analytical Processing (HTAP) databases require processing transactional and analytical queries in isolation to remove the interference between them. To achieve this, it is necessary to maintain different replicas of data specified for the two types of queries. However, it is challenging to provide a consistent view for distributed replicas within a storage system, where analytical requests can efficiently read consistent and fresh data from transactional workloads at scale and with high availability.To meet this challenge, we propose extending replicated state machine-based consensus algorithms to provide consistent replicas for HTAP workloads. Based on this novel idea, we present a Raft-based HTAP database: TiDB. In the database, we design a multi-Raft storage system which consists of a row store and a column store. The row store is built based on the Raft algorithm. It is scalable to materialize updates from transactional requests with high availability. In particular, it asynchronously replicates Raft logs to learners which transform row format to column format for tuples, forming a real-time updatable column store. This column store allows analytical queries to efficiently read fresh and consistent data with strong isolation from transactions on the row store. Based on this storage system, we build an SQL engine to process large-scale distributed transactions and expensive analytical queries. The SQL engine optimally accesses row-format and column-format replicas of data. We also include a powerful analysis engine, TiSpark, to help TiDB connect to the Hadoop ecosystem. Comprehensive experiments show that TiDB achieves isolated high performance under CH-benCHmark, a benchmark focusing on HTAP workloads.},
    journal = {Proc. VLDB Endow.},
    month = {aug},
    pages = {3072-3084},
    numpages = {13}
}
@inproceedings {267345,
    author = {Patrick Hunt and Mahadev Konar and Flavio P. Junqueira and Benjamin Reed},
    title = {{ZooKeeper}: Wait-free Coordination for Internet-scale Systems},
    booktitle = {2010 USENIX Annual Technical Conference (USENIX ATC 10)},
    year = {2010},
    url = {https://www.usenix.org/conference/usenix-atc-10/zookeeper-wait-free-coordination-internet-scale-systems},
    publisher = {USENIX Association},
    month = jun
}
@inproceedings{33002,
    title	= {Paxos Made Live - An Engineering Perspective (2006 Invited Talk)},
    author	= {Tushar Deepak Chandra and Robert Griesemer and Joshua Redstone},year	= {2007},
    URL	= {http://dx.doi.org/10.1145/1281100.1281103},
    booktitle	= {Proceedings of the 26th Annual ACM Symposium on Principles of Distributed Computing}
}
@inproceedings{39966,
    title	= {Spanner: Google's Globally-Distributed Database},author	= {James C. Corbett and Jeffrey Dean and Michael Epstein and Andrew Fikes and Christopher Frost and JJ Furman and Sanjay Ghemawat and Andrey Gubarev and Christopher Heiser and Peter Hochschild and Wilson Hsieh and Sebastian Kanthak and Eugene Kogan and Hongyi Li and Alexander Lloyd and Sergey Melnik and David Mwaura and David Nagle and Sean Quinlan and Rajesh Rao and Lindsay Rolig and Dale Woodford and Yasushi Saito and Christopher Taylor and Michal Szymaniak and Ruth Wang},
    year	= {2012},
    booktitle	= {OSDI}
}
@article{10.1145/121133.121169,
    author = {Liskov, Barbara and Ghemawat, Sanjay and Gruber, Robert and Johnson, Paul and Shrira, Liuba and Williams, Michael},
    title = {Replication in the harp file system},
    year = {1991},
    issue_date = {Oct. 1991},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    volume = {25},
    number = {5},
    issn = {0163-5980},
    url = {https://doi.org/10.1145/121133.121169},
    doi = {10.1145/121133.121169},
    abstract = {This paper describes the design and implementation of the Harp file system. Harp is a replicated Unix file system accessible via the VFS interface. It provides highly available and reliable storage for files and guarantees that file operations are executed atomically in spite of concurrency and failures. It uses a novel variation of the primary copy replication technique that provides good performance because it allows us to trade disk accesses for network communication. Harp is intended to be used within a file service in a distributed network; in our current implementation, it is accessed via NFS. Preliminary performance results indicate that Harp provides equal or better response time and system capacity than an unreplicated implementation of NFS that uses Unix files directly.},
    journal = {SIGOPS Oper. Syst. Rev.},
    month = {sep},
    pages = {226-238},
    numpages = {13}
}
@article{10.1145/200836.200869,
    author = {Attiya, Hagit and Bar-Noy, Amotz and Dolev, Danny},
    title = {Sharing memory robustly in message-passing systems},
    year = {1995},
    issue_date = {Jan. 1995},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    volume = {42},
    number = {1},
    issn = {0004-5411},
    url = {https://doi.org/10.1145/200836.200869},
    doi = {10.1145/200836.200869},
    abstract = {Emulators that translate algorithms from the shared-memory model to two different message-passing models are presented. Both are achieved by implementing a wait-free, atomic, single-writer multi-reader register in unreliable, asynchronous networks. The two message-passing models considered are a complete network with processor failures and an arbitrary network with dynamic link failures.These results make it possible to view the shared-memory model as a higher-level language for designing algorithms in asynchronous distributed systems. Any wait-free algorithm based on atomic, single-writer multi-reader registers can be automatically emulated in message-passing systems, provided that at least a majority of the processors are not faulty and remain connected. The overhead introduced by these emulations is polynomial in the number of processors in the system.Immediate new results are obtained by applying the emulators to known shared-memory algorithms. These include, among others, protocols to solve the following problems in the message-passing model in the presence of processor or link failures: multi-writer multi-reader registers, concurrent time-stamp systems, l-exclusion, atomic snapshots, randomized consensus, and implementation of data structures.},
    journal = {J. ACM},
    month = {jan},
    pages = {124-142},
    numpages = {19},
    keywords = {atomic registers, emulation, fault-tolerance, message passing, processor and link failures, shared memory, wait-freedom}
}
@inproceedings{36971,
    title	= {Megastore: Providing Scalable, Highly Available Storage for Interactive Services},
    author	= {Jason Baker and Chris Bond and James C. Corbett and JJ Furman and Andrey Khorlin and James Larson and Jean-Michel Leon and Yawei Li and Alexander Lloyd and Vadim Yushprakh},
    year	= {2011},
    URL	= {http://www.cidrdb.org/cidr2011/Papers/CIDR11_Paper32.pdf},
    booktitle	= {Proceedings of the Conference on Innovative Data system Research (CIDR)},
    pages	= {223--234}
}
@article{10.14778/3137765.3137778,
    author = {Zheng, Jianjun and Lin, Qian and Xu, Jiatao and Wei, Cheng and Zeng, Chuwei and Yang, Pingan and Zhang, Yunfan},
    title = {PaxosStore: high-availability storage made practical in WeChat},
    year = {2017},
    issue_date = {August 2017},
    publisher = {VLDB Endowment},
    volume = {10},
    number = {12},
    issn = {2150-8097},
    url = {https://doi.org/10.14778/3137765.3137778},
    doi = {10.14778/3137765.3137778},
    abstract = {In this paper, we present PaxosStore, a high-availability storage system developed to support the comprehensive business of WeChat. It employs a combinational design in the storage layer to engage multiple storage engines constructed for different storage models. PaxosStore is characteristic of extracting the Paxos-based distributed consensus protocol as a middleware that is universally accessible to the underlying multi-model storage engines. This facilitates tuning, maintaining, scaling and extending the storage engines. According to our experience in engineering practice, to achieve a practical consistent read/write protocol is far more complex than its theory. To tackle such engineering complexity, we propose a layered design of the Paxos-based storage protocol stack, where PaxosLog, the key data structure used in the protocol, is devised to bridge the programming-oriented consistent read/write to the storage-oriented Paxos procedure. Additionally, we present optimizations based on Paxos that made fault-tolerance more efficient. Discussion throughout the paper primarily focuses on pragmatic solutions that could be insightful for building practical distributed storage systems.},
    journal = {Proc. VLDB Endow.},
    month = {aug},
    pages = {1730-1741},
    numpages = {12}
}
@article{10.1145/1773912.1773922,
    author = {Lakshman, Avinash and Malik, Prashant},
    title = {Cassandra: a decentralized structured storage system},
    year = {2010},
    issue_date = {April 2010},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    volume = {44},
    number = {2},
    issn = {0163-5980},
    url = {https://doi.org/10.1145/1773912.1773922},
    doi = {10.1145/1773912.1773922},
    abstract = {Cassandra is a distributed storage system for managing very large amounts of structured data spread out across many commodity servers, while providing highly available service with no single point of failure. Cassandra aims to run on top of an infrastructure of hundreds of nodes (possibly spread across different data centers). At this scale, small and large components fail continuously. The way Cassandra manages the persistent state in the face of these failures drives the reliability and scalability of the software systems relying on this service. While in many ways Cassandra resembles a database and shares many design and implementation strategies therewith, Cassandra does not support a full relational data model; instead, it provides clients with a simple data model that supports dynamic control over data layout and format. Cassandra system was designed to run on cheap commodity hardware and handle high write throughput while not sacrificing read efficiency.},
    journal = {SIGOPS Oper. Syst. Rev.},
    month = {apr},
    pages = {35-40},
    numpages = {6}
}
@article{10.1145/37499.37515,
    author = {Birman, K. and Joseph, T.},
    title = {Exploiting virtual synchrony in distributed systems},
    year = {1987},
    issue_date = {Nov. 1987},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    volume = {21},
    number = {5},
    issn = {0163-5980},
    url = {https://doi.org/10.1145/37499.37515},
    doi = {10.1145/37499.37515},
    abstract = {We describe applications of a virtually synchronous environment for distributed programming, which underlies a collection of distributed programming tools in the ISIS2 system. A virtually synchronous environment allows processes to be structured into process groups, and makes events like broadcasts to the group as an entity, group membership changes, and even migration of an activity from one place to another appear to occur instantaneously — in other words, synchronously. A major advantage to this approach is that many aspects of a distributed application can be treated independently without compromising correctness. Moreover, user code that is designed as if the system were synchronous can often be executed concurrently. We argue that this approach to building distributed and fault-tolerant software is more straightforward, more flexible, and more likely to yield correct solutions than alternative approaches.},
    journal = {SIGOPS Oper. Syst. Rev.},
    month = {nov},
    pages = {123-138},
    numpages = {16}
}
@inproceedings{10.1145/41457.37515,
    author = {Birman, K. and Joseph, T.},
    title = {Exploiting virtual synchrony in distributed systems},
    year = {1987},
    isbn = {089791242X},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    url = {https://doi.org/10.1145/41457.37515},
    doi = {10.1145/41457.37515},
    abstract = {We describe applications of a virtually synchronous environment for distributed programming, which underlies a collection of distributed programming tools in the ISIS2 system. A virtually synchronous environment allows processes to be structured into process groups, and makes events like broadcasts to the group as an entity, group membership changes, and even migration of an activity from one place to another appear to occur instantaneously — in other words, synchronously. A major advantage to this approach is that many aspects of a distributed application can be treated independently without compromising correctness. Moreover, user code that is designed as if the system were synchronous can often be executed concurrently. We argue that this approach to building distributed and fault-tolerant software is more straightforward, more flexible, and more likely to yield correct solutions than alternative approaches.},
    booktitle = {Proceedings of the Eleventh ACM Symposium on Operating Systems Principles},
    pages = {123-138},
    numpages = {16},
    location = {Austin, Texas, USA},
    series = {SOSP '87}
}
@techreport{liskov12vr,
    author = {Barbara Liskov and James Cowling},
    title = {Viewstamped Replication Revisited},
    institution = {MIT},
    number = {MIT-CSAIL-TR-2012-021},
    month = jul,
    year = {2012}
}
@inproceedings {246190,
    author = {Zizhong Wang and Tongliang Li and Haixia Wang and Airan Shao and Yunren Bai and Shangming Cai and Zihan Xu and Dongsheng Wang},
    title = {{CRaft}: An Erasure-coding-supported Version of Raft for Reducing Storage Cost and Network Cost},
    booktitle = {18th USENIX Conference on File and Storage Technologies (FAST 20)},
    year = {2020},
    isbn = {978-1-939133-12-0},
    address = {Santa Clara, CA},
    pages = {297--308},
    url = {https://www.usenix.org/conference/fast20/presentation/wang-zizhong},
    publisher = {USENIX Association},
    month = feb
}
@INPROCEEDINGS{9820714,
  author={Jia, Yulei and Xu, Guangping and Sung, Chi Wan and Mostafa, Salwa and Wu, Yulei},
  booktitle={2022 IEEE International Parallel and Distributed Processing Symposium (IPDPS)}, 
  title={HRaft: Adaptive Erasure Coded Data Maintenance for Consensus in Distributed Networks}, 
  year={2022},
  volume={},
  number={},
  pages={1316-1326},
  keywords={Fault tolerance;Costs;System performance;Fault tolerant systems;Distributed databases;Switches;Throughput;Erasure coding;Consensus protocol;Raft;Paxos;Fault tolerance;Network storage},
  doi={10.1109/IPDPS53621.2022.00130}
}
@inproceedings{10.1145/3427796.3427815,
    author = {Howard, Heidi and Charapko, Aleksey and Mortier, Richard},
    title = {Fast Flexible Paxos: Relaxing Quorum Intersection for Fast Paxos},
    year = {2021},
    isbn = {9781450389334},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    url = {https://doi.org/10.1145/3427796.3427815},
    doi = {10.1145/3427796.3427815},
    abstract = {Paxos, the de facto standard approach to solving distributed consensus, operates in two phases, each of which requires an intersecting quorum of nodes. Multi-Paxos reduces this to one phase by electing a leader but this leader is also a performance bottleneck. Fast Paxos bypasses the leader but has stronger quorum intersection requirements. In this paper we observe that Fast Paxos’ intersection requirements can be safely relaxed, reducing to just one additional intersection requirement between phase-1 quorums and any pair of fast round phase-2 quorums. We thus find that the quorums used with Fast Paxos are larger than necessary, allowing alternative quorum systems to obtain new tradeoffs between performance and fault-tolerance.},
    booktitle = {Proceedings of the 22nd International Conference on Distributed Computing and Networking},
    pages = {186-190},
    numpages = {5},
    keywords = {Distributed computing, Distributed consensus, Paxos},
    location = {Nara, Japan},
    series = {ICDCN '21}
}
@inproceedings {234735,
    author = {Aleksey Charapko and Ailidani Ailijiang and Murat Demirbas},
    title = {Linearizable Quorum Reads in Paxos},
    booktitle = {11th USENIX Workshop on Hot Topics in Storage and File Systems (HotStorage 19)},
    year = {2019},
    address = {Renton, WA},
    url = {https://www.usenix.org/conference/hotstorage19/presentation/charapko},
    publisher = {USENIX Association},
    month = jul
}
@inproceedings{10.1145/2670979.2671001,
    author = {Moraru, Iulian and Andersen, David G. and Kaminsky, Michael},
    title = {Paxos Quorum Leases: Fast Reads Without Sacrificing Writes},
    year = {2014},
    isbn = {9781450332521},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    url = {https://doi.org/10.1145/2670979.2671001},
    doi = {10.1145/2670979.2671001},
    abstract = {This paper describes quorum leases, a new technique that allows Paxos-based systems to perform reads with high throughput and low latency. Quorum leases do not sacrifice consistency and have only a small impact on system availability and write latency. Quorum leases allow a majority of replicas to perform strongly consistent local reads, which substantially reduces read latency at those replicas (e.g., by two orders of magnitude in wide-area scenarios). Previous techniques for performing local reads in Paxos systems either (a) sacrifice consistency; (b) allow only one replica to read locally; or (c) decrease the availability of the system and increase the latency of all updates by requiring all replicas to be notified synchronously. We describe the design of quorum leases and evaluate their benefits compared to previous approaches through an implementation running in five geo-distributed Amazon EC2 datacenters.},
    booktitle = {Proceedings of the ACM Symposium on Cloud Computing},
    pages = {1–13},
    numpages = {13},
    location = {Seattle, WA, USA},
    series = {SOCC '14}
}
@inproceedings {265017,
    author = {Siyuan Zhou and Shuai Mu},
    title = {{Fault-Tolerant} Replication with {Pull-Based} Consensus in {MongoDB}},
    booktitle = {18th USENIX Symposium on Networked Systems Design and Implementation (NSDI 21)},
    year = {2021},
    isbn = {978-1-939133-21-2},
    pages = {687--703},
    url = {https://www.usenix.org/conference/nsdi21/presentation/zhou},
    publisher = {USENIX Association},
    month = apr
}
@inproceedings{10.1145/3448016.3452834,
    author = {Charapko, Aleksey and Ailijiang, Ailidani and Demirbas, Murat},
    title = {PigPaxos: Devouring the Communication Bottlenecks in Distributed Consensus},
    year = {2021},
    isbn = {9781450383431},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    url = {https://doi.org/10.1145/3448016.3452834},
    doi = {10.1145/3448016.3452834},
    abstract = {Strongly consistent replication helps keep application logic simple and provides significant benefits for correctness and manageability. Unfortunately, the adoption of strongly-consistent replication protocols has been curbed due to their limited scalability and performance. To alleviate the leader bottleneck in strongly-consistent replication protocols, we introduce Pig, an in-protocol communication aggregation and piggybacking technique. Pig employs randomly selected nodes from follower subgroups to relay the leader's message to the rest of the followers in the subgroup, and to perform in-network aggregation of acknowledgments back from these followers. By randomly alternating the relay nodes across replication operations, Pig shields the relay nodes as well as the leader from becoming hotspots and improves throughput scalability. We showcase Pig in the context of classical Paxos protocols employed for strongly consistent replication by many cloud computing services and databases. We implement and evaluate PigPaxos, in comparison to Paxos and EPaxos protocols under various workloads over clusters of size 5 to 25 nodes. We show that the aggregation at the relay has little latency overhead, and PigPaxos can provide more than 3 folds improved throughput over Paxos and EPaxos with little latency deterioration. We support our experimental observations with the analytical modeling of the bottlenecks and show that the communication bottlenecks are minimized when employing only one randomly rotating relay node.},
    booktitle = {Proceedings of the 2021 International Conference on Management of Data},
    pages = {235–247},
    numpages = {13},
    keywords = {distributed consensus, linearizability, paxos, replication},
    location = {Virtual Event, China},
    series = {SIGMOD '21}
}
@inproceedings {227651,
    author = {Seo Jin Park and John Ousterhout},
    title = {Exploiting Commutativity For Practical Fast Replication},
    booktitle = {16th USENIX Symposium on Networked Systems Design and Implementation (NSDI 19)},
    year = {2019},
    isbn = {978-1-931971-49-2},
    address = {Boston, MA},
    pages = {47--64},
    url = {https://www.usenix.org/conference/nsdi19/presentation/park},
    publisher = {USENIX Association},
    month = feb
}
@techreport{lamport2009vertical,
    author = {Lamport, Leslie and Malkhi, Dahlia and Zhou, Lidong},
    title = {Vertical Paxos and Primary-Backup Replication},
    year = {2009},
    month = {May},
    abstract = {We introduce a class of Paxos algorithms called Vertical Paxos, in which reconfiguration can occur in the middle of reaching agreement on an individual state-machine command. Vertical Paxos algorithms use an auxiliary configuration master that facilitates agreement on reconfiguration. A special case of these algorithms leads to traditional primary-backup protocols. We show how primary-backup systems in current use can be viewed, and shown to be correct, as instances of Vertical Paxos algorithms.

    This paper came out of much discussion between Malkhi, Zhou, and myself about reconfiguration. Some day, what we did may result in a long paper about state-machine reconfiguration containing these results and others that have not yet been published. The ideas here are related to the original, unpublished version of [151].},
    publisher = {Microsoft},
    url = {https://www.microsoft.com/en-us/research/publication/vertical-paxos-and-primary-backup-replication/},
    number = {MSR-TR-2009-63},
}
@inproceedings {267684,
    author = {Jeff Terrace and Michael J. Freedman},
    title = {Object Storage on {CRAQ}: {High-Throughput} Chain Replication for {Read-Mostly} Workloads},
    booktitle = {2009 USENIX Annual Technical Conference (USENIX ATC 09)},
    year = {2009},
    address = {San Diego, CA},
    url = {https://www.usenix.org/conference/usenix-09/object-storage-craq-high-throughput-chain-replication-read-mostly-workloads},
    publisher = {USENIX Association},
    month = jun
}
@techreport{lin2008pacifica,
    author = {Lin, Wei and Yang, Mao and Zhang, Lintao and Zhou, Lidong},
    title = {PacificA: Replication in Log-Based Distributed Storage Systems},
    year = {2008},
    month = {February},
    abstract = {Large-scale distributed storage systems have gained popularity for storing and processing ever increasing amount of data. Replication mechanisms are often key to achieving high availability and high throughput in such systems. Research on fundamental problems such as consensus has laid out a solid foundation for replication protocols. Yet, both the architectural design and engineering issues of practical replication mechanisms remain an art. This paper describes our experience in designing and implementing replication for commonly used log-based storage systems. We advocate a general replication framework that is simple, practical, and strongly consistent. We show that the framework is flexible enough to accommodate a variety of different design choices that we explore. Using a prototype system called PacificA, we implemented three different replication strategies, all using the same replication framework. The paper reports detailed performance evaluation results, especially on system behavior during failure, reconciliation, and recovery.},
    url = {https://www.microsoft.com/en-us/research/publication/pacifica-replication-in-log-based-distributed-storage-systems/},
    pages = {14},
    number = {MSR-TR-2008-25},
}
@inproceedings{10.1145/3373376.3378496,
    author = {Katsarakis, Antonios and Gavrielatos, Vasilis and Katebzadeh, M.R. Siavash and Joshi, Arpit and Dragojevic, Aleksandar and Grot, Boris and Nagarajan, Vijay},
    title = {Hermes: A Fast, Fault-Tolerant and Linearizable Replication Protocol},
    year = {2020},
    isbn = {9781450371025},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    url = {https://doi.org/10.1145/3373376.3378496},
    doi = {10.1145/3373376.3378496},
    abstract = {Today's datacenter applications are underpinned by datastores that are responsible for providing availability, consistency, and performance. For high availability in the presence of failures, these datastores replicate data across several nodes. This is accomplished with the help of a reliable replication protocol that is responsible for maintaining the replicas strongly-consistent even when faults occur. Strong consistency is preferred to weaker consistency models that cannot guarantee an intuitive behavior for the clients. Furthermore, to accommodate high demand at real-time latencies, datastores must deliver high throughput and low latency.This work introduces Hermes, a broadcast-based reliable replication protocol for in-memory datastores that provides both high throughput and low latency by enabling local reads and fully-concurrent fast writes at all replicas. Hermes couples logical timestamps with cache-coherence-inspired invalidations to guarantee linearizability, avoid write serialization at a centralized ordering point, resolve write conflicts locally at each replica (hence ensuring that writes never abort) and provide fault-tolerance via replayable writes. Our implementation of Hermes over an RDMA-enabled reliable datastore with five replicas shows that Hermes consistently achieves higher throughput than state-of-the-art RDMA-based reliable protocols (ZAB and CRAQ) across all write ratios while also significantly reducing tail latency. At 5\% writes, the tail latency of Hermes is 3.6X lower than that of CRAQ and ZAB.},
    booktitle = {Proceedings of the Twenty-Fifth International Conference on Architectural Support for Programming Languages and Operating Systems},
    pages = {201–217},
    numpages = {17},
    keywords = {throughput, replication, rdma, linearizability, latency, fault-tolerant, consistency, availability},
    location = {Lausanne, Switzerland},
    series = {ASPLOS '20}
}
@inproceedings{10.1145/2342356.2342360,
    author = {Escriva, Robert and Wong, Bernard and Sirer, Emin G\"{u}n},
    title = {HyperDex: a distributed, searchable key-value store},
    year = {2012},
    isbn = {9781450314190},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    url = {https://doi.org/10.1145/2342356.2342360},
    doi = {10.1145/2342356.2342360},
    abstract = {Distributed key-value stores are now a standard component of high-performance web services and cloud computing applications. While key-value stores offer significant performance and scalability advantages compared to traditional databases, they achieve these properties through a restricted API that limits object retrieval---an object can only be retrieved by the (primary and only) key under which it was inserted. This paper presents HyperDex, a novel distributed key-value store that provides a unique search primitive that enables queries on secondary attributes. The key insight behind HyperDex is the concept of hyperspace hashing in which objects with multiple attributes are mapped into a multidimensional hyperspace. This mapping leads to efficient implementations not only for retrieval by primary key, but also for partially-specified secondary attribute searches and range queries. A novel chaining protocol enables the system to achieve strong consistency, maintain availability and guarantee fault tolerance. An evaluation of the full system shows that HyperDex is 12-13x faster than Cassandra and MongoDB for finding partially specified objects. Additionally, HyperDex achieves 2-4x higher throughput for get/put operations.},
    booktitle = {Proceedings of the ACM SIGCOMM 2012 Conference on Applications, Technologies, Architectures, and Protocols for Computer Communication},
    pages = {25–36},
    numpages = {12},
    keywords = {strong consistency, performance, nosql, key-value store, fault-tolerance},
    location = {Helsinki, Finland},
    series = {SIGCOMM '12}
}
@inproceedings{kreps2011kafka,
  title={Kafka: A distributed messaging system for log processing},
  author={Kreps, Jay and Narkhede, Neha and Rao, Jun and others},
  booktitle={Proceedings of the NetDB},
  volume={11},
  number={2011},
  pages={1--7},
  year={2011},
  organization={Athens, Greece}
}
@inproceedings{10.1145/3448016.3457559,
    author = {Zhou, Jingyu and Xu, Meng and Shraer, Alexander and Namasivayam, Bala and Miller, Alex and Tschannen, Evan and Atherton, Steve and Beamon, Andrew J. and Sears, Rusty and Leach, John and Rosenthal, Dave and Dong, Xin and Wilson, Will and Collins, Ben and Scherer, David and Grieser, Alec and Liu, Young and Moore, Alvin and Muppana, Bhaskar and Su, Xiaoge and Yadav, Vishesh},
    title = {FoundationDB: A Distributed Unbundled Transactional Key Value Store},
    year = {2021},
    isbn = {9781450383431},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    url = {https://doi.org/10.1145/3448016.3457559},
    doi = {10.1145/3448016.3457559},
    abstract = {FoundationDB is an open source transactional key value store created more than ten years ago. It is one of the first systems to combine the flexibility and scalability of NoSQL architectures with the power of ACID transactions (a.k.a. NewSQL). FoundationDB adopts an unbundled architecture that decouples an in-memory transaction management system, a distributed storage system, and a built-in distributed configuration system. Each sub-system can be independently provisioned and configured to achieve the desired scalability, high-availability and fault tolerance properties. FoundationDB uniquely integrates a deterministic simulation framework, used to test every new feature of the system under a myriad of possible faults. This rigorous testing makes FoundationDB extremely stable and allows developers to introduce and release new features in a rapid cadence. FoundationDB offers a minimal and carefully chosen feature set, which has enabled a range of disparate systems (from semi-relational databases, document and object stores, to graph databases and more) to be built as layers on top. FoundationDB is the underpinning of cloud infrastructure at Apple, Snowflake and other companies, due to its consistency, robustness and availability for storing user data, system metadata and configuration, and other critical information.},
    booktitle = {Proceedings of the 2021 International Conference on Management of Data},
    pages = {2653-2666},
    numpages = {14},
    keywords = {multiversion concurrency control, oltp, optimistic concurrency control, simulation testing, strict serializability, unbundled database},
    location = {Virtual Event, China},
    series = {SIGMOD '21}
}
@inproceedings{10.1145/2600212.2600218,
    author = {Mu, Shuai and Chen, Kang and Wu, Yongwei and Zheng, Weimin},
    title = {When paxos meets erasure code: reduce network and storage cost in state machine replication},
    year = {2014},
    isbn = {9781450327497},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    url = {https://doi.org/10.1145/2600212.2600218},
    doi = {10.1145/2600212.2600218},
    abstract = {Paxos-based state machine replication is a key technique to build highly reliable and available distributed services, such as lock servers, databases and other data storage systems. Paxos can tolerate any minority number of node crashes in an asynchronous network environment. Traditionally, Paxos is used to perform a full copy replication across all participants. However, full copy is expensive both in term of network and storage cost, especially in wide area with commodity hard drives.In this paper, we discussed the non-triviality and feasibility of combining erasure code into Paxos protocol, and presented an improved protocol named RS-Paxos (Reed Solomon Paxos). To the best of our knowledge, we are the first to propose such a combination. Compared to Paxos, RS-Paxos requires a limitation on the number of possible failures. If the number of tolerated failures decreases by 1, RS-Paxos can save over 50\% of network transmission and disk I/O. To demonstrate the benefits of our protocol, we designed and built a key-value store based on RS-Paxos, and evaluated it on EC2 with various settings. Experiment results show that RS-Paxos achieves at most 2.5x improvement on write throughput and as much as 30\% reduction on latency, in common configurations.},
    booktitle = {Proceedings of the 23rd International Symposium on High-Performance Parallel and Distributed Computing},
    pages = {61-72},
    numpages = {12},
    keywords = {asynchronous message passing model, consensus, erasure code, paxos, state machine replication},
    location = {Vancouver, BC, Canada},
    series = {HPDC '14}
}
@inproceedings{matte2021scalable,
    title={Scalable but wasteful: Current state of replication in the cloud},
    author={Matte, Venkata Swaroop and Charapko, Aleksey and Aghayev, Abutalib},
    booktitle={Proceedings of the 13th ACM Workshop on Hot Topics in Storage and File Systems},
    pages={42--49},
    year={2021}
}
@inproceedings{10.1145/2517349.2517350,
    author = {Moraru, Iulian and Andersen, David G. and Kaminsky, Michael},
    title = {There is more consensus in Egalitarian parliaments},
    year = {2013},
    isbn = {9781450323888},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    url = {https://doi.org/10.1145/2517349.2517350},
    doi = {10.1145/2517349.2517350},
    abstract = {This paper describes the design and implementation of Egalitarian Paxos (EPaxos), a new distributed consensus algorithm based on Paxos. EPaxos achieves three goals: (1) optimal commit latency in the wide-area when tolerating one and two failures, under realistic conditions; (2) uniform load balancing across all replicas (thus achieving high throughput); and (3) graceful performance degradation when replicas are slow or crash.Egalitarian Paxos is to our knowledge the first protocol to achieve the previously stated goals efficiently---that is, requiring only a simple majority of replicas to be non-faulty, using a number of messages linear in the number of replicas to choose a command, and committing commands after just one communication round (one round trip) in the common case or after at most two rounds in any case. We prove Egalitarian Paxos's properties theoretically and demonstrate its advantages empirically through an implementation running on Amazon EC2.},
    booktitle = {Proceedings of the Twenty-Fourth ACM Symposium on Operating Systems Principles},
    pages = {358–372},
    numpages = {15},
    location = {Farminton, Pennsylvania},
    series = {SOSP '13}
}