= Data Replication Design Spectrum
:revdate: 2024-06-15
:page-hidden: true
:stem: latexmath
:page-features: stem, plot
:toc: preamble
:bibtex-file: 2024-resource-efficiency-in-replication.bib
:nospace:

Consistent replication algorithms can be placed on a sliding scale in terms of how they handle a replica failures.  Across the three common points on this spectrum, the resource and cost efficiency, availability, and latency are compared.  This all builds to the argument that Raft's strength is its consistent mediocrity across all metrics.

== Failure Masking vs Failure Detection

When designing a distributed database, one needs to choose a replication algorithm to replicate data across partitions while maintaining consistency.  Data replication algorithms fall into two categories: those designed to quietly tolerate failing nodes (failure masking), and those necessitating explicit reconfiguration around identified failures (failure detection).  This post mounts the argument that these are just two opposite points on a spectrum of design possibilities in-between.  We'll be taking a look at three points
sidenote:ref[][.aside]#sidenote:def[] Don't forget about the idea that there's more valid points along the spectrum than just these three!#
in particular: quorum-based leaderless replication for failure masking algorithms, reconfigurable primary-backup for failure detection algorithms, and leaderful consensus as the most well-known set of replication algorithms which blend the two.

[pikchr,align="center"]
----
down
text "Failure Masking"
TL: line invis down 25%
MID: circle rad 0.02 fill Black
text "Quorums" "(Leaderless)"
line from MID.c right 500%
R: circle rad 0.02 fill Black
down
text "Reconfigurable Primary-Backup" "(Virtual Synchrony)"
line from R.c invis up 25%
text "Failure Detection"

move right 110% from MID.c
circle rad 0.02 fill Black
down
text "Raft" "(Leaderful)"
----

Despite the tremendous mindshare of Raft, there isn't one correct answer.  Compiling together data on what replication algorithm is used by OSS, Proprietary, and internal-only Proprietary databases, we see a distribution of:
sidenote:ref[][.aside]#sidenote:def[] The chart is weighted by number of database systems, whereas a more realistic metric would be something that accounts for popularity such as the number of deployed machines or size of managed data.  With cloud vendors preferring leaderful replication by a large extent, it's likely that a popularity-weighted chart would be even more heavily skewed towards leaderful replication.  However, there's no way I can get that information to present such a graph.# 

++++
<div id="chart"></div>
++++

.Table of data from which the chart is derived
[%collapsible]
====

This table was assembled by

1. Reviewing https://db-engines.com/en/ranking, and looking for databases which manage their own storage (e.g. not HBase), and _support_ consistent writes (so Cassandra is included, but CouchDB isn't).
2. Reviewing cloud vendors for their public database offerings.
3. Looking for large companies which have internal-only databases, and reviewing their publications or blog posts.

[#repldata,cols="1,1,2"]
|===
| System | Replication Algorithm Family | Note

| MongoDB | Leaderful | 
| Redis Cluster | Leaderful | 
| Elasticsearch | Primary-Backup | Based off of PacificA per https://www.elastic.co/guide/en/elasticsearch/reference/current/docs-replication.html#_introduction[docs].
| Cassandra | Leaderless | Majority quorum for most operations.  LWT/Accord is leaderless consensus.
| Neo4j | Leaderful | Raft, per https://neo4j.com/docs/operations-manual/current/clustering/introduction/#clustering-primary-mode[docs].
| InfluxDB | Primary-Backup | Meta nodes run Raft.  Data nodes host data.  Per https://www.influxdata.com/blog/influxdb-clustering/[docs].
| CockroachDB | Leaderful |
| Aerospike | Primary-Backup | Per https://aerospike.com/docs/server/architecture/data-distribution[docs].
| Hazelcast | Leaderful | For its CP subsystem.  Per https://docs.hazelcast.com/imdg/4.2/consistency-and-replication/replication-algorithm[docs].
| Singlestore | Primary-Backup | Aggregators use Raft.  Leaf nodes store data. Per https://docs.singlestore.com/db/v7.5/introduction/faqs/clustering/[docs].
| TiKV | Leaderful |
| ScyllaDB | Leaderless |
| Riak KV | Leaderless |
| ArangoDB | Primary-Backup | https://docs.arangodb.com/3.11/deploy/cluster/#agents[Agents] serve as the consensus service, DB-Servers do synchronous replication within a shard.
| GraphDB | Leaderful | Raft, per https://graphdb.ontotext.com/documentation/10.0/cluster-basics.html[docs].
| Memgraph | Leaderful |  If I've understood the https://memgraph.com/docs/clustering/high-availability[docs] right?
| YugabyteDB | Leaderful |
| DGraph | Leaderful |
| FoundationDB | Primary-Backup |
| Apache Kudu | Leaderful |

| Google Spanner | Leaderful a| Per cite:[Spanner].
| Azure CosmosDB | Leaderful |
| Alibaba PolarDB | Leaderful | Per https://www.alibabacloud.com/help/en/polardb/polardb-for-postgresql/architecture-2[docs].
| Amazon DynamoDB | Leaderful | Per https://www.usenix.org/system/files/atc22-elhemali.pdf[paper].

|===

Systems such as HBase, which outsource their replication to another system (HDFS) are excluded from consideration.

====

++++
<script type="text/javascript">

const df = new dfjs.DataFrame(tableToData('repldata'));
df.show();
const df_count = df
  .rename('Replication Algorithm Family', 'replication')
  .groupBy('replication')
  .aggregate(x => x.count(), 'count');
const df_dbs = df
  .rename('Replication Algorithm Family', 'replication')
  .groupBy('replication')
  .aggregate(x => x.select('System').toArray().join(', '), 'tooltip');
const data = df_count.innerJoin(df_dbs, 'replication').toCollection();

var chart = new G2Plot.Pie('chart', {
  data,
  colorField: 'replication',
  angleField: 'count',
  radius: 0.9,
  label: { type: 'spider', formatter: (datum) => datum.replication },
  legend: false,
  interactions: [{ type: 'element-selected' }, { type: 'element-active' }],
});
chart.render();

</script>
++++

So leadered consensus, such as Raft, is indeed popular, but by no means the only valid choice.  But why the differences?  Why isn't there one correct choice?  What factors drove different databases to choose different solutions to their replication needs?  And less politely phrased, do Raft's merits actually deserve its incredible popularity?

Different replication algorithms have different characteristics. This post breaks down each replication algorithm into its resource efficiency, availability characteristics, and latency.  For resource efficiency, we'll be looking at the question: post-replication, how much of the storage capacity and read/write throughput remains?   As the counterpoints to resource efficiency, we'll also be looking at availability (given the loss of a random machine, what's the chance that a user will observe a transient service disruption), and latency (how many RTTs for a read or a write to complete).

With such an analysis, one would expect HN/Twitter/etc. to have replies of the form "But your analysis is invalid because you didn't account for _this_ replication algorithm optimization!".
And so to preempt any such replie, each section produces the analysis for both the most basic, classic, unoptimized form of the replication algorithm, and also walks through all of the potential optimizations that can be applied and how they affect the replication algorithm's resource efficiency, availability, and latency.  There is an excessive list of references at the bottom of this post which all link to the Google Scholar search result for each paper.

// replicated partition of data will need some number of machines, each of which if individually benchmarked as a one-machine cluster, can deliver a read throughput stem:[R_X], write throughput stem:[W_X], and has available storage capacity stem:[S_X].  The resulting replicated partition stem:[Y] can be benchmarked to determine its read throughput stem:[R_Y], write throughput stem:[W_Y] and storage capacity stem:[S_Y].  Throughout this post, we'll be examining what each class of replication algorithms can deliver in terms of the maximum values for read throughput efficiency stem:[R_Y / R_X], write throughput efficiency stem:[W_Y / W_X], and storage capacity efficiency stem:[S_Y / S_X].

// Failure detection-based algorithms require stem:[f+1] nodes to tolerate stem:[f] failures.  Failure masking-based algorithms require stem:[2f+1] nodes to tolerate stem:[f] failures.  For the common stem:[f=2], that's 40% more resources to accomplish the same task.  All the analysis in this post builds to one core discussion: are those extra nodes worth it?  How does that comparison look after we take the algorithm's resource efficiency into account?  Do the benefits of a leadered failure masking protocol like Raft outweigh the detriments, as opposed to leaderless consensus or reconfigurable primary-backup?

Lastly, the caveats: this post is one that lives purely in the land of theory.  All implementation efficiency costs are handwaved as negligible, which is blatantly false as evidenced by e.g. Red Panda's claims of being more resource efficient in aggregate than Kafka.  In the "write bandwidth efficiency" above, we're only concerning ourself with the network bandwidth.  Storage efficiency partly captures the disk write bandwidth efficiency, as storing 3 copies of data requires writing 3 copies of data (but realistically, far more considering storage engine write amplification).  It's tremendously more likely that in a real production deployment, the bottleneck for writes will be the disk and not the network.  However, we're discussing the theoretical write throughput tradeoffs only, because write network bandwidth throughput illustrates how replication topology matters for efficiency.

== Failure Masking: Leaderless Paxos / Majority Quorums

:uri-murat-is-this-consensus: https://muratbuffalo.blogspot.com/2019/06/is-this-consensus.html

The best known leaderless replication algorithms are the Paxos family of protocols.  Replication algorithms do not need to be consensus though, and majority quorums (with cite:[ReadRepair]{nospace}sidenote:ref[]) or cite:[ABD] are also in this category.
Leaderless replication is used in industry by systems like cite:[Megastore], cite:[PaxosStore], and cite:[Cassandra].
[.aside]#sidenote:def[] This post accidentally turned into an interesting adventure into finding the correct citation for a number of common concepts. This is the first, and by no means the last, citation where the source paper tangentially introduced a now fundamental concept.#

While leaderless Paxos and majority quorums differ in terms of consistency guarantees{nospace}sidenote:ref[], they're very similar in terms of resource efficiency.  All nodes store a full copy of all the data.  Reads and writes are broadcast to all replicas, and can make forward progress with only a majority of responses.  The need for a majority largely characterizes failure masking replication algorithms, as they require stem:[2f+1] nodes to tolerate stem:[f] failures.
[.aside]#sidenote:def[] See Murat Demirbas's {uri-murat-is-this-consensus}[Is this consensus?] for a refresher on consensus versus not.#

[graphviz]
----
digraph G {
  Client -> Replica1 [dir=both];
  Client -> Replica2 [dir=both];
  Client -> Replica3 [dir=both];
  Client -> Replica4 [dir=both, style=dashed];
  Client -> Replica5 [dir=both, style=dashed];
}
----

For storage efficiency, all replicas store a full copy of the data by default, and thus 1/5th of the total storage capacity is available post-replication.  There are a wide set of storage optimizations, and all have seen little adoption in industry. cite:[WitnessReplicas] permit removing the full copy of the data from a minority (2/5ths) of the replication group, with that minority instead only storing the version number for each replicated value.  However, this comes at the cost that a read cannot always successfully complete if the most up-to-date value is "known" only by witnesses.

Erasure encoding is an attractive possibility for reducing storage costs, as it would allow for encoding the data such that any three copies could reform the original data, and the total storage used across all 5 replicas would be the equivalent of 3 full copies of the data.
cite:[RSPaxos] examined applying erasure encoding to Paxos log entries, and concludes that space savings can only be obtained if fault tolerance is sacrificed.
However, I believe the ideas presented in cite:[ErasureEncodedRaft] should apply equally to leaderless consensus as well, and so we'll assume erasure encoding is feasible.sidenote:ref[]
This brings the storage efficiency for an erasure encoded Paxos to 33%.  
[.aside]#sidenote:def[] There's a number of issues being handwaved away here.  It's unclear how to apply operations from the log when any one replica only has the erasure encoded values stored.  cite:[ErasureEncodedRaft] falls back to full data replication when a single node stops responding, and this was improved in cite:[ErasureEncodedHRaft].  Erasure encoding in consensus has not received a significant amount of academic attention, and so I'm hopeful that other deficiencies can likely be similarly explored and improved.  This is mostly to show the theoretical maximum in an ideal world and less a claim that it's what _should_ be implemented.#

Majority quorums do a simple broadcast for both reads and writes, which earns a uniform 20% read bandwidth efficiency and 20% write bandwidth efficiency.  Applying the erasure encoding ideas above to the Paxos log entries could bring the write efficiency from 20% to 33%, and reading erasure encoded data also brings the read efficiency from 20% to 33%.

A major advantage of leaderless, quorum-based algorithms is the lack of dependence on a leader.  All failures can be masked, with no need of detecting or reconfiguring around the failure.  All leaderless replication algorithms earn a perfect 0% chance of unavailability on random node failure.

Though majority quorums has been repetitively stated to be a simple 1RTT broadcast for both reads and writes, that's not entirely accurate.  cite:[ReadRepair] means that reads must sometimes perform a subsequent round of writes before a value can be returned, thus earning a worst case 2RTT for reads.  The Paxos protocol has two rounds for writes, and reads are a one round broadcast.  There does exist the cite:[Megastore] optimization for making Paxos have 1RTT of write latency in the normal case, but it's still a worst case of 2RTT.

An implementation of majority quorums typically uses some form of a Last Writer Wins timestamping scheme, so that if a read returns three distinct values, it's possible to choose the "most recent" value as the correct read result.  This timestamp is generally a physical clock timestamp.  cite:[ABD] is an algorithm that's very similar to majority quorums in all aspects, except it uses a logical clock instead.  ABD ensures that its writes have a higher logical clock than all existing values by first reading the existing values, thus earning it 2RTT for writes, and does a similar read repair step after reads to earn it 2RTT for reads also.

[cols="1,1,1,1,1,1,1"]
|===
|
| Storage Efficiency
| Read Bandwidth Efficiency
| Write Bandwidth Efficiency
| Chance of Unavailability on Failure
| Read Latency
| Write Latency

| Majority Quorums
| 20%
| 20%
| 20%
| 0%
| 2RTT
| 1RTT

| ABD
| 20%
| 20%
| 20%
| 0%
| 2RTT
| 2RTT

| Paxos
| 20%
| 20%
| 20%
| 0%
| 1RTT
| 2RTT

| Erasure Encoded Paxos
| 33%
| 33%
| 33%
| 0%
| 1RTT
| 2RTT
|===

This table presents that the difference between majority quorums/ABD and Paxos is one of read and write latency, but again, don't forget that there's a very significant difference in data consistency between the two replication algorithms.  Erasure Encoded Paxos outwardly appears optimal, but isn't an algorithm that exists neither in academia nor in industry.

== Failure Detection: Reconfigurable Primary-Backup

:uri-apache-pegasus: https://pegasus.apache.org/
:uri-hibari: https://github.com/hibari/hibari
:uri-dan-luu-limplock: https://danluu.com/limplock/

Lastly, there is the lesser known class of failure detection-based replication algorithms.  These are algorithms in which there is a set of nodes in a replication group, and on detected failure, these algorithms execute a _view change_ to reconfigure to a new set of nodes with no failures.  There's a broad examination of such distributed protocols stemming from cite:[VirtualSynchrony], and this perspective of view changes on detected failures is even present in consensus protocols such as cite:[ViewstampedReplication]. However, our dividing line for this analysis is that the failure-detection algorithms use stem:[f+1] nodes to tolerate stem:[f] failures.  If stem:[2f+1] nodes are required, then the algorithm is likely better covered by one of the two failure-masking classes of algorithms discussed above.  Viewstamped Replication itself is very much like Raft and Multi-Paxos, and covered by Leadered Consensus above.

As a consequence of only having stem:[f+1] nodes for stem:[f] failures, there is a consistent theme in that all algorithms examined are _not consensus_.  This also means that they cannot solve consensus problems, such as deciding which replicas are responsible for a shard of data, or which node is the primary.  They all rely on an external consensus service to help with those issues.  Think of this as a control plane / data plane split: there's one instance of a consensus service in the control plane orchestrating the small amount of metadata deciding which nodes are in which replication groups responsible for which shards of data, and the horizontally scalable data plane replicates each shard of data within its assigned group.

There's two shapes of algorithms in this class of failure detection replication protocols: those that look like some form of primary-backup replication where a leader fans out requests to one or more backup nodes, or chain replication-like algorithms where each node is responsible for forwarding each piece of replicated data to the next node in the chain.  In academia, evolving Paxos into a reconfigurable primary-backup replication was examined in cite:[VerticalPaxosII]. cite:[PacificA] and cite:[Hermes] are more recent but different views on reconfigurable primary-backup replication. cite:[CRAQ] is the most famous chain replication algorithm, with cite:[HyperDex] being a more recently proposed chain-based system.  In industry, cite:[Kafka] and cite:[FoundationDB] use different variants of reconfigurable primary-backup, {uri-apache-pegasus}[Apache Pegasus] uses PacificA.  Nearly all of the chain replication databases in industry have died out, as {uri-hibari}[hibari] was one of the last but appears abandoned now, and HyperDex almost become a startup.

// TODO: Explicitly disambiguate that this is synchronous primary-backup.

[cols="1,1"]
|===
^| Chain ^| Parallel

a|
[graphviz]
----
digraph G {
  Client -> Replica1 -> Replica2 -> Replica3 -> Client;
}
----
a|
[graphviz]
----
digraph G {
  Client -> Primary   [dir=both];
  Primary -> Replica1 [dir=both];
  Primary -> Replica2 [dir=both];
}
----
|===

Unlike the quorum systems, there's no opportunity for erasure encoding.  When the number of replicas is stem:[F+1], we expect that a single alive replica can serve reads for all of its data.  Erasure encoding would require multiple pieces, and thus multiple nodes to be available.  Thus, the full cost of 3-way replication is consistently paid, yielding a uniform 33% storage efficiency.

With naive chain replication, only the tail of the chain is allowed to answer read requests, which would give it a read bandwidth efficiency of 33%.  cite:[CRAQ] permits any node to answer reads, and thus it gets 100% read bandwidth efficiency.  cite:[Hermes] permits any replica to serve reads independently, so it directly gains a 100% read bandwidth efficiency.

// TODO write bandwidth

Unavailability is the weakpoint of reconfigurable primary-backup systems.  The dependence on all nodes being functioning, and detecting and reconfiguring around failures, means that 

[cols="1,1,1,1,1"]
|===
|
| Storage Efficiency
| Read Bandwidth Efficiency
| Write Bandwidth Efficiency
| Chance of Unavailability on Failure

| CRAQ
| 33%
| 100%
| 33%
| 100%

| Hermes
| 33%
| 100%
| 16.5%
| 100%
|===

It's important to note that the surface level simplicity of replication algorithms rooted in cite:[VirtualSynchrony] hide their complexity via glossing over two nontrivial topics: group membership and failure detection.



Failure detectors have their own rich history that warrants a separate post sometime.  The simplest failure detector is a periodic heartbeat with a timeout.  However, by depending on all replicas to be functioning correctly, one needs a very precise definition of what "functioning correctly" means.  If the disk is failing and its throughput drops by 90% or if there's a bad switch causing packet loss and thus TCP throughput drops significantly{nospace}sidenote:ref[], that's not a "correctly functioning" machine, and one would wish to reconfigure around the failure. cite:[GrayFailureAchillesHeel] discusses gray failure issues in more detail.  cite:[LimpingTolerantClouds] offers more concrete examples.  {uri-dan-luu-limplock}[Dan Luu has written about this as well].
[.aside]#sidenote:def[] The most frequent singular cause of times I've been paged awake by a service in the middle of the night has been some networking equipment deciding to drop 1% of packets, and TCP thus slowing down to approximately dial-up speeds.  Heartbeats can still be sent, so the service isn't "unavailable", but it sure wasn't working well.#

Lastly, it's important to note that there's a notable complexity of handling gray failures gained by all failure detection-based replication algorithms.  The complexity of Paxos has been widely discussed, and Reconfigurable Primary-Backup has its own source of complexity instead.    This is by no means an impossible problem to solve, but how to "correctly" identify and adapt to a gray failure is a problem that's less well researched and with solutions less well understood by industry, than the alternative of just masking the failures.


== Hybrid: Raft / Multi-Paxos

:uri-cockroach-follower-reads: https://www.cockroachlabs.com/blog/follower-reads-stale-data/
:uri-edb-pgdist-witness: https://www.enterprisedb.com/docs/pgd/latest/node_management/witness_nodes/
:uri-spanner-witness: https://cloud.google.com/spanner/docs/replication#witness
:uri-tikv-follower-reads: https://tikv.org/blog/double-system-read-throughput/
:uri-ydb-erasure-encode: https://ydb.tech/docs/en/concepts/cluster/distributed_storage

//:uri-tigerbeetle-fpaxos: https://docs.tigerbeetle.com/deploy/hardware/
//Applying cite:[FlexiblePaxos] allows one to run with 4 replicas and require 3 to be alive for an election and still replicate across 2, which as far as I know only {uri-tigerbeetle-fpaxos}[TigerBeetle] implements.  

Leaderful consensus what is generally brought to mind when one mentions "consensus".  It is best known as cite:[Raft], cite:[MultiPaxos] or cite:[ZAB], and exemplified by distributed databases such as cite:[CockroachDB], cite:[TiDB] and cite:[Spanner], or configuration management systems such as cite:[PaxosMadeLive] and cite:[Zookeeper].  (Among _many_ other high-quality, production systems.)

In the simplest Raft implementation, all operations sent to the leader, and the leader broadcasts the replication stream to its followers.  Tolerating stem:[f=2] failures requires stem:[2f+1 = 5] nodes.  All nodes store and write the same data to disk.  At most two of the replicas are permitted to be unavailable.  Across any and all flavors of Raft and Multi-Paxos, the presence of a leader is fundamental, and gives a 1 in 5 chance of transient unavailability if a node fails.

[graphviz]
----
digraph G {
  Client -> Leader   [dir=both];
  Leader -> Replica1 [dir=both];
  Leader -> Replica2 [dir=both];
  Leader -> Replica3 [dir=both, style=dashed];
  Leader -> Replica4 [dir=both, style=dashed];
}
----

All replicas store a full copy of the data, and thus 1/5th of the total storage capacity is available post-replication.  The storage optimizations available are similar to what was discussed for leaderless replication.
cite:[WitnessReplicas] permit removing the full copy of the data from a minority (2/5ths) of the replication group, and the leaderful consensus variant of witness replicas is always able to serve reads from the leader even with a simple majority of replicas alive.  Note though, that removing storage means that witness replicas can't serve reads.  
Only {uri-edb-pgdist-witness}[EnterpriseDB Postgres Distributed] and {uri-spanner-witness}[(Cloud) Spanner] implement support for witness replicas as part of Raft and Multi-Paxos, respectively.

cite:[ErasureEncodedRaft] again allows storing the equivalent of 3 copies spread across 5 replicas, thus achieving 33% storage efficiency a different way.
The other possible direction for storage efficiency improvement is cite:[ErasureEncodedRaft].  Erasure encoding is popular in distributed filesystems and blob storage systems, but incredibly rare in distributed databases; I am only aware of {uri-ydb-erasure-encode}[YDB] using it.  Thus, as 99% of the Raft implementations one might ever encounter have a storage efficiency of 1/5th, that is the value that will be used for storage efficiency for the rest of the analysis.

Read throughput can be improved by implementing cite:[LinearizableQuorumReads] for 2/5ths read throughput, cite:[PaxosQuorumLeases] for 3/5ths read throughput, or cite:[FollowerReads] for 5/5ths read throughput at the cost of increased latency.  We'll disregard the latency implications, and keep 5/5ths as Raft's read throughput, which is realistic given that it's been implemented in production systems such as {uri-cockroach-follower-reads}[Cockroach] and {uri-tikv-follower-reads}[TiKV].

In classic Raft, all proposals go to the leader, and then the leader broadcasts the proposals to all followers.  This means Raft is first constraining to utilizing only stem:[1/(2f+1)] or 1/5th of the available incoming bandwidth.  Then the bottleneck becomes the leader's outgoing bandwidth, further reduction of stem:[1/2f], so 1/4th.  This means a write bandwidth efficiency of stem:[1/(4f^2 + 2f)] or 1/20th.

There have been ways discussed to scale the write bandwidth.  cite:[PullBasedConsensus] presents an argument that a fixed topology is not needed, replicas can fetch from other replicas, and thus even a linear chain replicas could work.  cite:[ScalingReplication] shows another view that the work of broadcasting to all replicas can be delegated to other replicas.  cite:[CommutativeRaft] presents a different approach, in which clients are allowed to directly send to all replicas, and the leader only arbitrates ordering when there's conflicts.  Of these, only pull-based consensus is implemented in industry, but I'm not aware that even MongoDB itself runs in a linear chain configuration.  (It's mostly about saving WAN costs.)  Thus, 1/4th is the value that will be used for write bandwidth efficiency for the rest of the analysis.

A more resource efficient Raft implementation could combine cite:[FollowerReads] (enabling all replicas to provide full read throughput), cite:[PullBasedConsensus] (to set up a chain-organized replication stream), and cite:[WitnessReplicas] (to store only 3 full copies of data).  However, our maximal resource efficiency comes from adopting cite:[ErasureEncodedRaft] instead of cite:[WitnessReplicas] for the storage savings.  

In summary, our resource efficiency for stem:[f=2] for a minimal Raft implementation, Raft with the set of improvements that one will commonly encounter in industry, and our theoretical and maximally resource efficient Raft is:

[cols="1,1,1,1,1"]
|===
|
| Storage Efficiency
| Read Bandwidth Efficiency
| Write Bandwidth Efficiency
| Chance of Unavailability on Failure

| Simplest
| 20%
| 20%
| 5%
| 20%

| Common Improvements
| 20%
| 100%
| 5%
| 20%

| Follower+Pull+Witness
| 33%
| 60%
| 20%
| 20%

| Follower+Pull+Erasure
| 33%
| 100%
| 20%
| 20%
|===

One of the major arguments of this post is that however you feel about reconfigurable primary-backup and failure detection-based distributed system design is _exactly_ how you should feel about the leader in Raft/Multi-Paxos.  Raft is 1/5th Failure Detection + 4/5ths Failure Masking.  Some folk really don't like failure detection and having a reconfiguration step during which the partition is unavailable, and that's okay.  But any failure pattern you might have thought of and felt concerned about while reading the failure detection section applies precisely the same to the leader in Raft.  


== Comparison

Taking the most common selection across each category, we have:

[cols="1,1,1,1,1,1"]
|===
|
| Nodes Required for stem:[F=2]
| Storage Efficiency
| Read Bandwidth Efficiency
| Write Bandwidth Efficiency
| Chance of Unavailability on Failure

| Leadered
| 5
| 20%
| 100%
| 6.25%
| 20%

| Leaderless
| 5
| 20%
| 20%
| 20%
| 0%

| Primary-Backup
| 3
| 33%
| 100%
| 16.5%
| 100%
|===

This is using "Common Improvements" from the Leadered section, "Paxos" from the Leaderless section, and "Hermes" from the Reconfigurable Primary-Backup section.  (Using Hermes over CRAQ is a bit of an arbitrary selection, but the write latency of Hermes more closely matches that of Raft, and so it's a bit more of an apples-to-apples comparison.)

Immediately apparent is that Reconfigurable Primary-Backup both requires less nodes, and delivers a uniformly equal-or-better resource efficiency for the less resources used.

Even when examining the most resource optimal variants of each class of replication algorithm (the erasure encoded variants):

[cols="1,1,1,1,1,1"]
|===
|
| Nodes Required for stem:[F=2]
| Storage Efficiency
| Read Bandwidth Efficiency
| Write Bandwidth Efficiency
| Chance of Unavailability on Failure

| Follower+Pull+Erasure
| 5
| 33%
| 100%
| 20%
| 20%

| Erasure Encoded Paxos
| 5
| 33%
| 33%
| 33%
| 0%

| CRAQ
| 3
| 33%
| 100%
| 33%
| 100%
|===

We see that erasure encoding just brings each quorum algorithm to the resource efficacy of Reconfigurable Primary-Backup, but still requires 66% more nodes than Reconfigurable Primary-Backup.

Leaderless Paxos struggles with livelock when there's high contention on updating a single item, and Raft doesn't.  There are a number of other resources to consider in a real environment other than what was presented in this post.  CPU, memory, disk IOPS, etc., are all finite resources, which were not discussed, but if those become the limiting factor for performance, then that is the bottleneck and efficiency metric to be mindful of. cite:[ScalableButWasteful] notes that CPU constraints can lead cite:[MultiPaxos] to have 2x more throughput than cite:[EPaxos].  If throughput is what determines the amount of hardware you need to buy/rent for your database deployment, and the hardware is CPU constrained, then this is a more impactful efficiency to keep in mind for leaderful vs leaderless quorum replication.  (However, I still claim reconfigurable primary-backup would be even more cost effective!)

Correspondingly, reconfigurable primary-backup is common in block storage products where I suspect there's been more pressure on price; it's databases specifically which feels more like a monoculture.  cite:[Ceph] implements both parallel and chain replication.  cite:[GFS] implements parallel replication where the client is responsible for sending the data to each replica to be buffered, and the primary broadcast is used to tell each replica to write the buffered data.  cite:[HDFS] similarly follows suit.  That's not to say that all block storage products do, as for example cite:[AlibabaEBS] and cite:[PolarFS] use leaderful consensus, but just that reconfigurable primary-backup is comparatively a much more frequently chosen solution for replication in the different domain.

And I do understand that there's reasons why we don't see a large number of reconfigurable-primary backup databases.  Requiring an external consensus service to manage replication groups requires first having a consensus solution implemented and production ready.  This could either be using an existing solution like Zookeeper or Etcd, but then those might have existing issues, you're then required to become an expert in running them, and using a separate project as the core of your service adds a significant hassle to testing.  The alternative is to implement your own, and then you're doing all the work of just deploying leaderless Paxos or leaderful Raft, only to turn and do even more work before being able to deploy to production.  And there still isn't a single great reference for how to safely identify and reconfigure around gray failures.

== Raft is Okay

:uri-rystsov-simple-consensus: https://web.archive.org/web/20240121170711/http://rystsov.info/2017/02/15/simple-consensus.html

Raft blends together failure masking and failure detection into one replication algorithm, and thus exists as a middle-ground between resource efficiency and high availability.  But this blending leaves Raft in an odd place. By partly being a failure detection-based replication algorithm, a reliable Raft implementation has to pick up all the complexity of detecting gray failures in the same way as a Reconfigurable Primary-Backup algorithm, but without anywhere near the corresponding resource efficiency advantages to justify the complexity.  However, by also being a Failure Masking algorithm, it also must deal with nodes being transiently unavailable, and the corresponding error handling complexity and state space explosion that occurs in tracking that.  

Raft also blends the worst aspects of failure masking (poor read/write throughput efficiency and poor storage efficiency) with the worst aspects of failure detection (transient unavailability on failure) into one replication algorithm that's consistently mediocre.  Looking over the tables, it starts to become hard to justify the complexity in terms of the rewards.  Raft gains over leaderless Paxos a 5x increase in read bandwidth, and accepts a 20% chance of transient unavailability on node loss.  Raft gains over Hermes the 20% chance of transient unavailability, but at the cost of 66% more hardware and worse-or-equal resource efficiency across the board.  It's not feeling like an assuredly good trade.

This isn't to say that Raft is a poor choice of replication algorithm.  

I find it hard to believe that paying for 66% more resources, in exchange for seeing a decrease in p99.9 from better transient unavailability handling is as universally the correct tradeoff for how pervasive Raft's usage is.  Both alternatives are simpler to implement as well.  Denis Rystsov {uri-rystsov-simple-consensus}[compared Raft versus Paxos] in the form of cite:[CASPaxos] and showed the leaderless version of consensus to be less complex and more available.

Raft, however, is always a _safe_ choice.  Choosing Raft over leaderless Paxos means that you don't have to worry about livelock.  Choosing Raft over Hermes means you can likely move the leader away from a machine causing persistent gray failure issues, and then mask the failures away instead.  You can build a story where you can remove your largest source of potential outages on both side, by just paying some extra money for stem:[2f+1] replicas.  Most use cases are read-heavy, and Raft lets you deliver 100% read bandwidth efficiency via follower reads, so it's not money entirely wasted either.  Raft's safety is only further increased by there being a single text giving sufficient description on how to implement Raft, and complemented by a rich number of blog posts detailing subtle issues which caused outages elsewhere.  No one is doing to get in trouble for choosing Raft, as it's not _bad_ at anything.

My only ask is that before choosing Raft by default to solve your next replication need, please do spend a moment thinking about if it strikes the right set of tradeoffs for your use case.

== References

bibliography::[]

////
* [[[Raft]]] https://scholar.google.com/scholar?cluster=12646889551697084617[Diego Ongaro and John Ousterhout. 2014. In search of an understandable consensus algorithm. In Proceedings of the 2014 USENIX Conference on USENIX Annual Technical Conference (USENIX ATC'14), USENIX Association, Philadelphia, PA, 305-320.]
* [[[MultiPaxos,Multi-Paxos]]] https://scholar.google.com/scholar?cluster=5393275675498127693[Robbert Van Renesse and Deniz Altinbuken. 2015. Paxos Made Moderately Complex. ACM Comput. Surv. 47, 3 (February 2015). DOI:https://doi.org/10.1145/2673577]
* [[[CockroachDB]]] https://scholar.google.com/scholar?cluster=13649983341597312439[Rebecca Taft, Irfan Sharif, Andrei Matei, Nathan VanBenschoten, Jordan Lewis, Tobias Grieger, Kai Niemi, Andy Woods, Anne Birzin, Raphael Poss, Paul Bardea, Amruta Ranade, Ben Darnell, Bram Gruneir, Justin Jaffray, Lucy Zhang, and Peter Mattis. 2020. CockroachDB: The Resilient Geo-Distributed SQL Database. In Proceedings of the 2020 ACM SIGMOD International Conference on Management of Data (SIGMOD '20), Association for Computing Machinery, Portland, OR, USA, 1493-1509. DOI:https://doi.org/10.1145/3318464.3386134]
* [[[TiDB]]] https://scholar.google.com/scholar?cluster=4024782010863299783[Dongxu Huang, Qi Liu, Qiu Cui, Zhuhe Fang, Xiaoyu Ma, Fei Xu, Li Shen, Liu Tang, Yuxing Zhou, Menglong Huang, Wan Wei, Cong Liu, Jian Zhang, Jianjun Li, Xuelian Wu, Lingyu Song, Ruoxi Sun, Shuaipeng Yu, Lei Zhao, Nicholas Cameron, Liquan Pei, and Xin Tang. 2020. TiDB: a Raft-based HTAP database. Proc. VLDB Endow. 13, 12 (August 2020), 3072-3084. DOI:https://doi.org/10.14778/3415478.3415535]
* [[[Zookeeper]]] https://scholar.google.com/scholar?cluster=16979330189653726967[Patrick Hunt, Mahadev Konar, Flavio P. Junqueira, and Benjamin Reed. 2010. ZooKeeper: Wait-free Coordination for Internet-scale Systems. In 2010 USENIX Annual Technical Conference (USENIX ATC 10), USENIX Association. Retrieved from https://www.usenix.org/conference/usenix-atc-10/zookeeper-wait-free-coordination-internet-scale-systems]
* [[[PaxosMadeLive,Google Chubby]]] https://scholar.google.com/scholar?cluster=17465339664204453932[Tushar Deepak Chandra, Robert Griesemer, and Joshua Redstone. 2007. Paxos Made Live - An Engineering Perspective (2006 Invited Talk). In Proceedings of the 26th Annual ACM Symposium on Principles of Distributed Computing. Retrieved from http://dx.doi.org/10.1145/1281100.1281103]
* [[[Spanner,Google Spanner]]] https://scholar.google.com/scholar?cluster=3523173873845838643[James C. Corbett, Jeffrey Dean, Michael Epstein, Andrew Fikes, Christopher Frost, JJ Furman, Sanjay Ghemawat, Andrey Gubarev, Christopher Heiser, Peter Hochschild, Wilson Hsieh, Sebastian Kanthak, Eugene Kogan, Hongyi Li, Alexander Lloyd, Sergey Melnik, David Mwaura, David Nagle, Sean Quinlan, Rajesh Rao, Lindsay Rolig, Dale Woodford, Yasushi Saito, Christopher Taylor, Michal Szymaniak, and Ruth Wang. 2012. Spanner: Google's Globally-Distributed Database. In OSDI.]
* [[[ReadRepair,Read Repair]]] https://scholar.google.com/scholar?cluster=9927566946845895796[Dahlia Malkhi and Michael K. Reiter. 1998. Secure and scalable replication in Phalanx. In Proceedings Seventeenth IEEE Symposium on Reliable Distributed Systems (Cat. No.98CB36281), 51-58. DOI:https://doi.org/10.1109/RELDIS.1998.740474]
* [[[ABD]]] https://scholar.google.com/scholar?cluster=8138971298707379383[Hagit Attiya, Amotz Bar-Noy, and Danny Dolev. 1995. Sharing memory robustly in message-passing systems. J. ACM 42, 1 (January 1995), 124-142. DOI:https://doi.org/10.1145/200836.200869]
* [[[Megastore]]] https://scholar.google.com/scholar?cluster=75122057060478473[Jason Baker, Chris Bond, James C. Corbett, JJ Furman, Andrey Khorlin, James Larson, Jean-Michel Leon, Yawei Li, Alexander Lloyd, and Vadim Yushprakh. 2011. Megastore: Providing Scalable, Highly Available Storage for Interactive Services. In Proceedings of the Conference on Innovative Data system Research (CIDR), 223-234. Retrieved from http://www.cidrdb.org/cidr2011/Papers/CIDR11_Paper32.pdf]
* [[[PaxosStore]]] https://scholar.google.com/scholar?cluster=12164791380407440973[Jianjun Zheng, Qian Lin, Jiatao Xu, Cheng Wei, Chuwei Zeng, Pingan Yang, and Yunfan Zhang. 2017. PaxosStore: high-availability storage made practical in WeChat. Proc. VLDB Endow. 10, 12 (August 2017), 1730-1741. DOI:https://doi.org/10.14778/3137765.3137778]
* [[[Cassandra]]] https://scholar.google.com/scholar?cluster=9829178954647343079[Avinash Lakshman and Prashant Malik. 2010. Cassandra: a decentralized structured storage system. SIGOPS Oper. Syst. Rev. 44, 2 (April 2010), 35-40. DOI:https://doi.org/10.1145/1773912.1773922]
* [[[VirtualSynchrony,Virtual Synchrony]]] https://scholar.google.com/scholar?cluster=2271986924920893419[K. Birman and T. Joseph. 1987. Exploiting virtual synchrony in distributed systems. In Proceedings of the Eleventh ACM Symposium on Operating Systems Principles (SOSP '87), Association for Computing Machinery, Austin, Texas, USA, 123-138. DOI:https://doi.org/10.1145/41457.37515]
* [[[ViewstampedReplication,Viewstamped Replication]]] https://scholar.google.com/scholar?cluster=13000400770252658813[Barbara Liskov and James Cowling. 2012. Viewstamped Replication Revisited. MIT.]
* [[[WitnessReplicas,Witness Replicas]]] https://scholar.google.com/scholar?cluster=1415144878608869709[Jehan-Francois Paris. 1990. Efficient voting protocols with witnesses. In ICDT '90, Springer Berlin Heidelberg, Berlin, Heidelberg, 305-317.]
* [[[ErasureEncodedRaft,Erasure Encoded Raft]]] https://scholar.google.com/scholar?cluster=10123939731603884260[Zizhong Wang, Tongliang Li, Haixia Wang, Airan Shao, Yunren Bai, Shangming Cai, Zihan Xu, and Dongsheng Wang. 2020. CRaft: An Erasure-coding-supported Version of Raft for Reducing Storage Cost and Network Cost. In 18th USENIX Conference on File and Storage Technologies (FAST 20), USENIX Association, Santa Clara, CA, 297-308. Retrieved from https://www.usenix.org/conference/fast20/presentation/wang-zizhong]
* [[[ErasureEncodedHRaft,Erasure Encoded HRaft]]] https://scholar.google.com/scholar?cluster=15724086733201598850[Yulei Jia, Guangping Xu, Chi Wan Sung, Salwa Mostafa, and Yulei Wu. 2022. HRaft: Adaptive Erasure Coded Data Maintenance for Consensus in Distributed Networks. In 2022 IEEE International Parallel and Distributed Processing Symposium (IPDPS), 1316-1326. DOI:https://doi.org/10.1109/IPDPS53621.2022.00130]
* [[[FlexiblePaxos,Flexible Paxos]]] https://scholar.google.com/scholar?cluster=6509870440808150538[Heidi Howard, Aleksey Charapko, and Richard Mortier. 2021. Fast Flexible Paxos: Relaxing Quorum Intersection for Fast Paxos. In Proceedings of the 22nd International Conference on Distributed Computing and Networking (ICDCN '21), Association for Computing Machinery, Nara, Japan, 186-190. DOI:https://doi.org/10.1145/3427796.3427815]
* [[[LinearizableQuorumReads,Linearizable Quorum Reads]]] https://scholar.google.com/scholar?cluster=10098760952745259234[Aleksey Charapko, Ailidani Ailijiang, and Murat Demirbas. 2019. Linearizable Quorum Reads in Paxos. In 11th USENIX Workshop on Hot Topics in Storage and File Systems (HotStorage 19), USENIX Association, Renton, WA. Retrieved from https://www.usenix.org/conference/hotstorage19/presentation/charapko]
* [[[PaxosQuorumLeases,Paxos Quorum Leases]]] https://scholar.google.com/scholar?cluster=2618624974148224118[Iulian Moraru, David G. Andersen, and Michael Kaminsky. 2014. Paxos Quorum Leases: Fast Reads Without Sacrificing Writes. In Proceedings of the ACM Symposium on Cloud Computing (SOCC '14), Association for Computing Machinery, Seattle, WA, USA, 1-13. DOI:https://doi.org/10.1145/2670979.2671001]
* [[[PullBasedConsensus,Pull-Based Consensus in MongoDB]]] https://scholar.google.com/scholar?cluster=3477252701158690968[Siyuan Zhou and Shuai Mu. 2021. Fault-Tolerant Replication with Pull-Based Consensus in MongoDB. In 18th USENIX Symposium on Networked Systems Design and Implementation (NSDI 21), USENIX Association, 687-703. Retrieved from https://www.usenix.org/conference/nsdi21/presentation/zhou]
* [[[ScalingReplication,Scaling Strongly Consistent Replication]]] https://scholar.google.com/scholar?cluster=1909096821088376701[Aleksey Charapko, Ailidani Ailijiang, and Murat Demirbas. 2021. PigPaxos: Devouring the Communication Bottlenecks in Distributed Consensus. In Proceedings of the 2021 International Conference on Management of Data (SIGMOD '21), Association for Computing Machinery, Virtual Event, China, 235-247. DOI:https://doi.org/10.1145/3448016.3452834]
* [[[CommutativeRaft,Exploiting Commutativity For Practical Fast Replication]]] https://scholar.google.com/scholar?cluster=3451458773692631815[Seo Jin Park and John Ousterhout. 2019. Exploiting Commutativity For Practical Fast Replication. In 16th USENIX Symposium on Networked Systems Design and Implementation (NSDI 19), USENIX Association, Boston, MA, 47-64. Retrieved from https://www.usenix.org/conference/nsdi19/presentation/park]
* [[[VerticalPaxosII,Vertical Paxos II]]] https://scholar.google.com/scholar?cluster=12255443511267289537[Leslie Lamport, Dahlia Malkhi, and Lidong Zhou. 2009. Vertical Paxos and Primary-Backup Replication. Microsoft. Retrieved from https://www.microsoft.com/en-us/research/publication/vertical-paxos-and-primary-backup-replication/]
* [[[CRAQ]]] https://scholar.google.com/scholar?cluster=9297968548710093419[Jeff Terrace and Michael J. Freedman. 2009. Object Storage on CRAQ: High-Throughput Chain Replication for Read-Mostly Workloads. In 2009 USENIX Annual Technical Conference (USENIX ATC 09), USENIX Association, San Diego, CA. Retrieved from https://www.usenix.org/conference/usenix-09/object-storage-craq-high-throughput-chain-replication-read-mostly-workloads]
* [[[PacificA]]] https://scholar.google.com/scholar?cluster=15826444170581946812[Wei Lin, Mao Yang, Lintao Zhang, and Lidong Zhou. 2008. PacificA: Replication in Log-Based Distributed Storage Systems. Retrieved from https://www.microsoft.com/en-us/research/publication/pacifica-replication-in-log-based-distributed-storage-systems/]
* [[[Hermes]]] https://scholar.google.com/scholar?cluster=13608264111814513293[Antonios Katsarakis, Vasilis Gavrielatos, M.R. Siavash Katebzadeh, Arpit Joshi, Aleksandar Dragojevic, Boris Grot, and Vijay Nagarajan. 2020. Hermes: A Fast, Fault-Tolerant and Linearizable Replication Protocol. In Proceedings of the Twenty-Fifth International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS '20), Association for Computing Machinery, Lausanne, Switzerland, 201-217. DOI:https://doi.org/10.1145/3373376.3378496]
* [[[HyperDex]]] https://scholar.google.com/scholar?cluster=8838739194584316753[Robert Escriva, Bernard Wong, and Emin GÃ¼n Sirer. 2012. HyperDex: a distributed, searchable key-value store. In Proceedings of the ACM SIGCOMM 2012 Conference on Applications, Technologies, Architectures, and Protocols for Computer Communication (SIGCOMM '12), Association for Computing Machinery, Helsinki, Finland, 25-36. DOI:https://doi.org/10.1145/2342356.2342360]
* [[[Kafka]]] https://scholar.google.com/scholar?cluster=5891925114546481347[Jay Kreps, Neha Narkhede, Jun Rao, and others. 2011. Kafka: A distributed messaging system for log processing. In Proceedings of the NetDB, Athens, Greece, 1-7.]
* [[[FoundationDB]]] https://scholar.google.com/scholar?cluster=4197497039785350505[Jingyu Zhou, Meng Xu, Alexander Shraer, Bala Namasivayam, Alex Miller, Evan Tschannen, Steve Atherton, Andrew J. Beamon, Rusty Sears, John Leach, Dave Rosenthal, Xin Dong, Will Wilson, Ben Collins, David Scherer, Alec Grieser, Young Liu, Alvin Moore, Bhaskar Muppana, Xiaoge Su, and Vishesh Yadav. 2021. FoundationDB: A Distributed Unbundled Transactional Key Value Store. In Proceedings of the 2021 International Conference on Management of Data (SIGMOD '21), Association for Computing Machinery, Virtual Event, China, 2653-2666. DOI:https://doi.org/10.1145/3448016.3457559]
* [[[RSPaxos,RS-Paxos]]] https://scholar.google.com/scholar?cluster=16520033292975033789[Shuai Mu, Kang Chen, Yongwei Wu, and Weimin Zheng. 2014. When paxos meets erasure code: reduce network and storage cost in state machine replication. In Proceedings of the 23rd International Symposium on High-Performance Parallel and Distributed Computing (HPDC '14), Association for Computing Machinery, Vancouver, BC, Canada, 61-72. DOI:https://doi.org/10.1145/2600212.2600218]
* [[[GrayFailureAchillesHeel,Gray Failure: The Achilles' Heel of Cloud-Scale Systems]]] https://scholar.google.com/scholar?cluster=4369373863260707505[Peng Huang, Chuanxiong Guo, Lidong Zhou, Jacob R. Lorch, Yingnong Dang, Murali Chintalapati, and Randolph Yao. 2017. Gray Failure: The Achilles' Heel of Cloud-Scale Systems. In Proceedings of the 16th Workshop on Hot Topics in Operating Systems (HotOS '17), Association for Computing Machinery, Whistler, BC, Canada, 150-155. DOI:https://doi.org/10.1145/3102980.3103005]
* [[[ScalableButWasteful,Scalable But Wasteful]]] https://scholar.google.com/scholar?cluster=16327886782851538912[Venkata Swaroop Matte, Aleksey Charapko, and Abutalib Aghayev. 2021. Scalable but wasteful: Current state of replication in the cloud. In Proceedings of the 13th ACM Workshop on Hot Topics in Storage and File Systems, 42-49.]
* [[[EPaxos,Egalitarian Paxos]]] https://scholar.google.com/scholar?cluster=13655117037814714535[Iulian Moraru, David G. Andersen, and Michael Kaminsky. 2013. There is more consensus in Egalitarian parliaments. In Proceedings of the Twenty-Fourth ACM Symposium on Operating Systems Principles (SOSP '13), Association for Computing Machinery, Farminton, Pennsylvania, 358-372. DOI:https://doi.org/10.1145/2517349.2517350]
* [[[Ceph]]] https://scholar.google.com/scholar?cluster=12064684978898371724[Sage A. Weil, Andrew W. Leung, Scott A. Brandt, and Carlos Maltzahn. 2007. RADOS: a scalable, reliable storage service for petabyte-scale storage clusters. In Proceedings of the 2nd International Workshop on Petascale Data Storage: Held in Conjunction with Supercomputing '07 (PDSW '07), Association for Computing Machinery, Reno, Nevada, 35-44. DOI:https://doi.org/10.1145/1374596.1374606]
* [[[GFS,Google File System]]] https://scholar.google.com/scholar?cluster=98210925508218371[Sanjay Ghemawat, Howard Gobioff, and Shun-Tak Leung. 2003. The Google file system. In Proceedings of the Nineteenth ACM Symposium on Operating Systems Principles (SOSP '03), Association for Computing Machinery, Bolton Landing, NY, USA, 29-43. DOI:https://doi.org/10.1145/945445.945450]
* [[[AlibabaEBS,Alibaba Cloud EBS]]] https://scholar.google.com/scholar?cluster=8248321054680879292[Weidong Zhang, Erci Xu, Qiuping Wang, Xiaolu Zhang, Yuesheng Gu, Zhenwei Lu, Tao Ouyang, Guanqun Dai, Wenwen Peng, Zhe Xu, Shuo Zhang, Dong Wu, Yilei Peng, Tianyun Wang, Haoran Zhang, Jiasheng Wang, Wenyuan Yan, Yuanyuan Dong, Wenhui Yao, Zhongjie Wu, Lingjun Zhu, Chao Shi, Yinhu Wang, Rong Liu, Junping Wu, Jiaji Zhu, and Jiesheng Wu. 2024. What's the Story in EBS Glory: Evolutions and Lessons in Building Cloud Block Store. In 22nd USENIX Conference on File and Storage Technologies (FAST 24), USENIX Association, Santa Clara, CA, 277-291. Retrieved from https://www.usenix.org/conference/fast24/presentation/zhang-weidong]
* [[[PolarFS]]] https://scholar.google.com/scholar?cluster=4921679856437073694[Wei Cao, Zhenjun Liu, Peng Wang, Sen Chen, Caifeng Zhu, Song Zheng, Yuhui Wang, and Guoqing Ma. 2018. PolarFS: an ultra-low latency and failure resilient distributed file system for shared storage cloud database. Proc. VLDB Endow. 11, 12 (August 2018), 1849-1862. DOI:https://doi.org/10.14778/3229863.3229872]
* [[[LimpingTolerantClouds,Limping-Hardware Tolerant Clouds]]] https://scholar.google.com/scholar?cluster=9138890906893078068[Thanh Do and Haryadi S. Gunawi. 2013. The Case for Limping-Hardware Tolerant Clouds. In 5th USENIX Workshop on Hot Topics in Cloud Computing (HotCloud 13), USENIX Association, San Jose, CA. Retrieved from https://www.usenix.org/conference/hotcloud13/workshop-program/presentations/do]
* [[[CASPaxos]]] https://scholar.google.com/scholar?cluster=1991685638971222231[Denis Rystsov. 2018. CASPaxos: Replicated State Machines without logs. Retrieved from https://arxiv.org/abs/1802.07000]
////

link:2024-resource-efficency-in-replication.bib[References as BibTex]
