= Data Replication Design Spectrum
:revdate: 2024-06-15
:page-hidden: true
:stem: latexmath
:page-features: stem, plot
:toc: preamble
:bibtex-file: 2024-resource-efficiency-in-replication.bib

:nospace:

[.aside]#With thanks to AJ Werner for pointers to Cockroach's optimizations, Reuben Bond for links to good Virtual Synchrony overview papers, and Phil Eaton for early reads and feedback.#

[#chosen_preamble]
--
Consistent replication algorithms can be placed on a sliding scale in terms of how they handle a replica failures.  Across the three common points on this spectrum, the resource and cost efficiency, availability, and latency are compared.  This all builds to the argument that Raft's strength is its consistent mediocrity across all metrics.
--

== Failure Masking vs Failure Detection

When designing a distributed database, one needs to choose a replication algorithm to replicate data across partitions while maintaining consistency.  Data replication algorithms fall across two categories: those designed to quietly tolerate failing nodes (failure masking), and those necessitating explicit reconfiguration around identified failures (failure detection).  This post mounts the argument that these are just two opposite points on a spectrum of design possibilities in-between.  We'll be taking a look at three points{nospace}sidenote:ref[] in particular: quorum-based leaderless replication for failure masking algorithms, reconfiguration-based replication for failure detection algorithms, and leaderful consensus as the most well-known set of replication algorithms which blend the two.
[.aside]#sidenote:def[] Don't forget about the idea that there's more valid points along the spectrum than just these three!#

[pikchr,align="center"]
----
down
text "Failure Masking"
TL: line invis down 25%
MID: circle rad 0.02 fill Black
text "Quorums"
line from MID.c right 500%
R: circle rad 0.02 fill Black
down
text "Reconfiguration"
line from R.c invis up 25%
text "Failure Detection"

move right 110% from MID.c
circle rad 0.02 fill Black
down
text "Leaders"
----

Despite the tremendous mindshare of Raft, there isn't one correct answer.  Compiling together data on what replication algorithm is used by OSS, Proprietary, and internal-only Proprietary databases, we see a distribution of:
sidenote:ref[][.aside]#sidenote:def[] The chart is weighted by number of database systems, whereas a more realistic metric would be something that accounts for popularity such as the number of deployed machines or size of managed data.  With cloud vendors preferring leaderful replication by a large extent, it's likely that a popularity-weighted chart would be even more heavily skewed towards leaderful replication.  However, there's no way I can get that information to present such a graph.# 

++++
<div id="chart"></div>
++++

.Table of data from which the chart is derived
[%collapsible]
====

This table was assembled by

1. Reviewing https://db-engines.com/en/ranking, and looking for databases which manage their own storage (e.g. not HBase), and _support_ consistent writes (so Cassandra is included, but CouchDB isn't).
2. Reviewing cloud vendors for their public database offerings.
3. Looking for large companies which have internal-only databases, and reviewing their publications or blog posts.

[#repldata,cols="1,1,2"]
|===
| System | Replication Algorithm Family | Note

| MongoDB | Leaderful | 
| Redis Cluster | Leaderful | 
| Elasticsearch | Primary-Backup | Based off of PacificA per https://www.elastic.co/guide/en/elasticsearch/reference/current/docs-replication.html#_introduction[docs].
| Cassandra | Leaderless | Majority quorum for most operations.  LWT/Accord is leaderless consensus.
| Neo4j | Leaderful | Raft, per https://neo4j.com/docs/operations-manual/current/clustering/introduction/#clustering-primary-mode[docs].
| InfluxDB | Primary-Backup | Meta nodes run Raft.  Data nodes host data.  Per https://www.influxdata.com/blog/influxdb-clustering/[docs].
| CockroachDB | Leaderful |
| Aerospike | Primary-Backup | Per https://aerospike.com/docs/server/architecture/data-distribution[docs].
| Hazelcast | Leaderful | For its CP subsystem.  Per https://docs.hazelcast.com/imdg/4.2/consistency-and-replication/replication-algorithm[docs].
| Singlestore | Primary-Backup | Aggregators use Raft.  Leaf nodes store data. Per https://docs.singlestore.com/db/v7.5/introduction/faqs/clustering/[docs].
| TiKV | Leaderful |
| ScyllaDB | Leaderless |
| Riak KV | Leaderless |
| ArangoDB | Primary-Backup | https://docs.arangodb.com/3.11/deploy/cluster/#agents[Agents] serve as the consensus service, DB-Servers do synchronous replication within a shard.
| GraphDB | Leaderful | Raft, per https://graphdb.ontotext.com/documentation/10.0/cluster-basics.html[docs].
| Memgraph | Leaderful |  If I've understood the https://memgraph.com/docs/clustering/high-availability[docs] right?
| YugabyteDB | Leaderful |
| DGraph | Leaderful |
| FoundationDB | Primary-Backup |
| Apache Kudu | Leaderful |

| Google Spanner | Leaderful a| Per https://cloud.google.com/spanner/docs/replication[docs].
| Azure CosmosDB | Leaderful |
| Alibaba PolarDB | Leaderful | Per https://www.alibabacloud.com/help/en/polardb/polardb-for-postgresql/architecture-2[docs].
| Amazon DynamoDB | Leaderful | Per https://www.usenix.org/system/files/atc22-elhemali.pdf[paper].

|===

Systems such as HBase, which outsource their replication to another system (HDFS) are excluded from consideration.

====

++++
<script type="text/javascript">

const df = new dfjs.DataFrame(tableToData('repldata'));
const df_count = df
  .rename('Replication Algorithm Family', 'replication')
  .groupBy('replication')
  .aggregate(x => x.count(), 'count');
const df_dbs = df
  .rename('Replication Algorithm Family', 'replication')
  .groupBy('replication')
  .aggregate(x => x.select('System').toArray().join(', '), 'tooltip');
const data = df_count.innerJoin(df_dbs, 'replication').toCollection();

var chart = new G2Plot.Pie('chart', {
  data,
  colorField: 'replication',
  angleField: 'count',
  radius: 0.9,
  label: { type: 'spider', formatter: (datum) => datum.replication },
  legend: false,
  interactions: [{ type: 'element-selected' }, { type: 'element-active' }],
});
chart.render();

</script>
++++

So leaderful consensus, such as Raft, is indeed popular, but by no means the only valid choice.  But why the differences?  Why isn't there one correct choice?  What factors drove different databases to choose different solutions to their replication needs?  Does Raft's popularity correspond to its resource efficiency?

Different replication algorithms have different characteristics. This post breaks down each replication algorithm into its resource efficiency, availability characteristics, and latency.  For resource efficiency, we'll be looking at the question: post-replication, how much of the storage capacity and read/write throughput remains?   As the counterpoints to resource efficiency, we'll also be looking at availability (given the loss of a random machine, what's the chance that a user will observe a transient service disruption), and latency (how many RTTs for a read or a write to complete).

With such an analysis, one would expect HN/Twitter/etc. to have replies of the form "But your analysis is invalid because you didn't account for _this_ replication algorithm optimization!".
And so to preempt any such replies, each section produces the analysis for both the most classic, unoptimized form of the replication algorithm, and also walks through improved designs or potential optimizations that can be applied and how they affect the replication algorithm's resource efficiency, availability, and latency.  Correspondingly, this post is a sizable read.  There is an excessive list of references at the bottom of this post which all link to the Google Scholar search result for each paper.

// Failure detection-based algorithms require stem:[f+1] nodes to tolerate stem:[f] failures.  Failure masking-based algorithms require stem:[2f+1] nodes to tolerate stem:[f] failures.  For the common stem:[f=2], that's 40% more resources to accomplish the same task.  All the analysis in this post builds to one core discussion: are those extra nodes worth it?  How does that comparison look after we take the algorithm's resource efficiency into account?  Do the benefits of a leadered failure masking protocol like Raft outweigh the detriments, as opposed to leaderless consensus or reconfigurable primary-backup?

Lastly, the caveats: this post is one that lives purely in the land of theory.  All implementation efficiency costs are handwaved as negligible, which is blatantly false as evidenced by e.g. Red Panda's claims of being more resource efficient in aggregate than Kafka.  In the "write bandwidth efficiency" above, we're only concerning ourself with the network bandwidth.  Storage efficiency partly captures the disk write bandwidth efficiency, as storing 3 copies of data requires writing 3 copies of data (but realistically, far more considering storage engine write amplification).  It's tremendously more likely that in a real production deployment, the bottleneck for writes will be the disk and not the network.  However, we're discussing the theoretical write throughput tradeoffs only, because write network bandwidth throughput illustrates how replication topology matters for efficiency.

== Failure Masking: Quorums

:uri-murat-is-this-consensus: https://muratbuffalo.blogspot.com/2019/06/is-this-consensus.html
:uri-riak-quorums: https://docs.riak.com/riak/kv/latest/developing/usage/replication/index.html#a-primer-on-n-r-and-w

Replication algorithms that rely on failure masking for all of their replicas use quorums, so that a majority of replicas may make progress despite a minority of failed replicas of working replicas.  The cite:[Paxos] family of consensus protocols are well known quorum-based replication algorithms.  However, achieving consensus is not required to replicate data consistently, and simple majority quorum{nospace}sidenote:ref[]{nospace}-based algorithms such as cite:[ABD] are in this category.
Quorum-based replication is used in industry by systems like cite:[Megastore], cite:[PaxosStore], and cite:[Cassandra].
[.aside]#sidenote:def[] cite:[ReadRepair] is required for majority quorums to be linearizable, and we're exclusively considering replication algorithms which support linearizable operations.#

While leaderless cite:[Paxos] and cite:[ABD] differ in terms of consistency guarantees{nospace}sidenote:ref[], they're very similar in terms of resource efficiency.  All nodes store a full copy of all the data.  Reads and writes are broadcast to all replicas, and can make forward progress with only a majority of responses.  The need for a majority largely characterizes failure masking replication algorithms, as they require stem:[2f+1] nodes to tolerate stem:[f] failures.
[.aside]#sidenote:def[] See Murat Demirbas's {uri-murat-is-this-consensus}[Is this consensus?] for a refresher on consensus versus not.#

[graphviz]
----
digraph G {
  Client -> Replica1 [dir=both];
  Client -> Replica2 [dir=both];
  Client -> Replica3 [dir=both];
  Client -> Replica4 [dir=both, style=dashed];
  Client -> Replica5 [dir=both, style=dashed];
}
----

For storage efficiency, all replicas store a full copy of the data by default, and thus 1/5th of the total storage capacity is available post-replication.  There are a wide set of storage optimizations, and all have seen little adoption in industry. cite:[WitnessReplicas] permit removing the full copy of the data from a minority (2/5ths) of the replication group, with that minority instead only storing the version number for each replicated value.  However, this comes at the cost that a read cannot always successfully complete if the most up-to-date value is "known" only by witnesses.

Erasure coding is an attractive possibility for reducing storage costs, as it would allow for encoding the data such that any three copies could reform the original data, and the total storage used across all 5 replicas would be the equivalent of 3 full copies of the data.
cite:[RSPaxos] examined applying erasure coding to Paxos log entries, and concludes that space savings can only be obtained if fault tolerance is sacrificed.
However, I believe the ideas presented in cite:[ErasureCodedRaft] should apply equally to leaderless consensus as well, and so we'll assume erasure coding is feasible.sidenote:ref[]
This brings the storage efficiency for an erasure coded Paxos to 33%.  
[.aside]#sidenote:def[] There's a number of issues being handwaved away here.  It's unclear how to apply operations from the log when any one replica only has the erasure coded values stored.  cite:[ErasureCodedRaft] falls back to full data replication when a single node stops responding, and this was improved in cite:[ErasureCodedHRaft].  Erasure coding in consensus has not received a significant amount of academic attention, and so I'm hopeful that other deficiencies can likely be similarly explored and improved.  This is mostly to show the theoretical maximum in an ideal world and less a claim that it's what _should_ be implemented.#

Majority quorums{nospace}sidenote:ref[] do a simple broadcast for both reads and writes, which earns a uniform 20% read bandwidth efficiency and 20% write bandwidth efficiency.  Applying the erasure coding ideas above to the Paxos log entries could bring the write efficiency from 20% to 33%, and reading erasure coded data also brings the read efficiency from 20% to 33%.
[.aside]#sidenote:def[] There's many ways of arranging quorums that aren't a simple majority, and all the variations affect the read and write throughput calculations.  It used to be more popular to allow for tuning the read quorum and write quorum sizes, but many of those systems have since died out, {uri-riak-quorums}[such as Riak].  More esoteric quorum setups exist, but they aren't commonly used and thus out of scope for this post. cite:[Quoracle] is a fun read on alternative schemes though.#

A major advantage of leaderless, quorum-based algorithms is the lack of dependence on a leader.  All failures can be masked, with no need of detecting or reconfiguring around the failure.  All leaderless replication algorithms earn a perfect 0% chance of unavailability on random node failure.

Though majority quorums has been repetitively stated to be a simple 1RTT broadcast for both reads and writes, that's not entirely accurate.  cite:[ReadRepair] means that reads must sometimes perform a subsequent round of writes before a value can be returned, thus earning a worst case 2RTT for reads.  The Paxos protocol has two rounds for writes, and reads are a one round broadcast.  There does exist the cite:[Megastore] optimization for making Paxos have 1RTT of write latency in the normal case by weakly assigning a leader, but it's still a worst case of 2RTT.

An implementation of majority quorums typically uses some form of a Last Writer Wins timestamping scheme, so that if a read returns three distinct values, it's possible to choose the "most recent" value as the correct read result.  This timestamp is generally a physical clock timestamp.  cite:[ABD] is an algorithm that's very similar to majority quorums in all aspects, except it uses a logical clock instead.  ABD ensures that its writes have a higher logical clock than all existing values by first reading the existing values, thus earning it 2RTT for writes, and does a similar read repair step after reads to earn it 2RTT for reads also.

[cols="1,1,1,1,1,1,1"]
|===
|
| Storage Efficiency
| Read Bandwidth Efficiency
| Write Bandwidth Efficiency
| Chance of Unavailability on Failure
| Read Latency
| Write Latency

| Majority Quorums
| 20%
| 20%
| 20%
| 0%
| 2RTT
| 1RTT

| ABD
| 20%
| 20%
| 20%
| 0%
| 2RTT
| 2RTT

| Paxos
| 20%
| 20%
| 20%
| 0%
| 1RTT
| 2RTT

| Erasure Coded Paxos
| 33%
| 33%
| 33%
| 0%
| 1RTT
| 2RTT
|===

This table presents that the difference between majority quorums/ABD and Paxos is one of read and write latency, but again, don't forget that there's a very significant difference in data consistency between the two replication algorithms.  The write phase of majority quorums or ABD will always succeed if the replica replies, but Paxos's write request will only succeed if it's the highest ballot, meaning that Paxos's 2RTT writes may need to be repeated multiple times when there's contention.  Erasure Coded Paxos outwardly appears optimal, but isn't an algorithm that actually exists neither in academia nor in industry.

One of the largest concerns around deploying cite:[Paxos] to production is its vulnerability to livelock under contention.  Dueling proposals can mutually prevent forward progress, and so contention on a single replicated item is to be avoided if possible.  cite:[Megastore] is very contention prone as every proposal is trying to target the next slot in the replicated log, and thus they tried to include a weak leadership optimization.  cite:[PaxosStore] deployed only to geographically close replicas to minimize the latency from proposing to accepting, thus minimizing the window for proposals to conflict.  cite:[EPaxos] focuses on allowing concurrent updates to distinct entities, and only ordering conflicting proposals.  cite:[CASPaxos] avoids a log entirely, and thus trivially allows concurrent updates on distinct items.  cite:[Tempo] and cite:[Accord] assign client-generated timestamps to all requests so that all replicas process requests in a deterministic order, but at the cost of a fixed increase in latency to wait out clock skew bounds before processing any request.  If a use case requires handling potentially many concurrent update attempts to the same item, then it's possible that leaderless consensus is not a good choice of replication algorithm.

== Failure Detection: Reconfiguration

:uri-apache-pegasus: https://pegasus.apache.org/
:uri-hibari: https://github.com/hibari/hibari
:uri-dan-luu-limplock: https://danluu.com/limplock/
:uri-ydb-erasure-coding: https://ydb.tech/docs/en/concepts/cluster/distributed_storage

Failure detection-based replication algorithms have a chosen set of replicas in a replication group which must be live for the algorithm to make progress.  On detected replica failure, these algorithms reconfigure the replication group to exclude the failed replica and include a new, live replica.  Rather than allow replicas to be failed, a failed replica is evicted from the replication group.  All replicas are either working, or will be excommunicated.

All reconfiguration-based replication protocols share certain attributes.  Writes go to all replicas, and thus reads may go to a single replica.  As all writes are always sent to all replicas, only stem:[f+1] replicas are needed to tolerate stem:[f] failures, as a single replica will always have a full and consistent snapshot of data.

As a consequence of only having stem:[f+1] nodes for stem:[f] failures, there is a consistent theme in that all algorithms examined are _not consensus_.  This also means that they cannot solve consensus problems, such as deciding which replicas are responsible for a shard of data, or which node is the primary.  They all rely on an external consensus service to help with those issues.  Think of this as a control plane / data plane split: there's one instance of a consensus service in the control plane orchestrating the small amount of metadata deciding which nodes are in which replication groups responsible for which shards of data, and the horizontally scalable data plane replicates each shard of data within its assigned group.

There's two shapes of algorithms in this class of failure detection-based replication protocols: those in which inter-replica communication is done as a broadcast, those in which it is done as a linear chain.  Broadcast-based replication is well known as cite:[PrimaryBackup] replication, which we'll be examining through the lens of cite:[PacificA] which has move of an emphasis on the reconfiguration support, and cite:[Hermes] as a more recent improvement on broadcast-based replication.  For chain-based replication, we'll be examining the original cite:[ChainReplication], and cite:[CRAQ] as its more recent improvement.

In academia, many of this ideas are rooted in cite:[VirtualSynchrony].  Evolving Paxos into a reconfigurable primary-backup replication was examined in cite:[VerticalPaxosII]. cite:[PacificA] and cite:[Hermes] are more recent but different views on broadcast-based replication which we'll be examining in more detail. cite:[CRAQ] is the most famous chain replication algorithm, with cite:[HyperDex] being a more recently proposed chain-based system.  In industry, cite:[Kafka] and cite:[FoundationDB] use different variants of reconfiguration around failures, {uri-apache-pegasus}[Apache Pegasus] uses cite:[PacificA].  Nearly all of the chain replication databases in industry seem to have died out, as {uri-hibari}[hibari] was one of the last but appears abandoned now, and HyperDex almost become a startup.

[.aside]#And thanks to the cite:[Hermes] paper for how to succinctly express the variations in reads and writes in one diagram.#

image::failure-detection-replication.svg[align=center]

In failure detection-based replication algorithms, all three replicas store a full copy of the data, yielding 33% storage efficiency.  Unlike the quorum systems, there's no inherent opportunity for erasure coding.  When the number of replicas is stem:[F+1], we expect that a single alive replica can serve reads for all of its data.  Applying erasure coding requires increasing the set of replicas (while the erasure coding maintains the same aggregate storage efficiency), and then choosing the number of parity blocks to equal the number of failures one wishes to be able to recover from.  This effectively applies quorums for failure masking, though at the level of erasure coding rather than at the level of the replication algorithm.  Such a design is common in blob storage systems, but not in distributed databases, except for {uri-ydb-erasure-coding}[YDB].

With naive chain replication, only the tail of the chain is allowed to answer read requests, which means a read bandwidth efficiency of 33%.  cite:[CRAQ] permits any node to answer reads, and thus it gets 100% read bandwidth efficiency.  Both Primary-Backup and cite:[Hermes] are capable of serving reads from all nodes, so they gain a 100% read bandwidth efficiency.

Both chain replication and cite:[CRAQ] have 33% write bandwidth efficiency, as one replica accepts writes and each replica sends to only one more replica so there's no further bottleneck on outgoing bandwidth.  Primary-Backup only allows the primary to accept writes, and it must broadcast to two replicas, yielding a 16% write bandwidth efficiency.  cite:[Hermes] allows any replica to accept writes, and receives the replication broadcast from the other two replicas.  This balances the incoming and outgoing bandwidth requirements to allow 33% write bandwidth efficiency.

Unavailability is the weakpoint of reconfiguration-based systems.  In all examined systems, any failure requires detection (generally through a heartbeat timeout), and then a membership view change to a new set of non-failed replicas.  Any replica failure has a 100% chance of causing a client-visible spike in latency due to no requests being processed while the heartbeat times out and the view change protocol runs.

Chain replication serves reads in 1RTT but from only the tail. cite:[CRAQ] allows any member of the chain to serve reads, but if there's an ongoing write to the same key, the replica has to wait to hear back from the tail replica that the write was completed before it may respond to the read.sidenote:ref[] Primary-Backup can serve reads in 1RTT from the primary, or 2RTT from the replicas (as they need to contact the primary for the most recently applied log update).  cite:[Hermes] allows serving reads in 1RTT (but possibly requires waiting for up to 1RTT while a write finishes).
[.aside]#sidenote:def[] This means CRAQ is optimal for 100% read or 100% write workloads, and degrades read latency in between, which is a trade off I haven't seen in any other replication algorithm.  It'd be ideal for large data loads (100% writes), followed by an online serving workload (100% reads), and could serve reads with degraded latency as a data load is ongoing.#

Both chain replication and cite:[CRAQ] yield a chain's worth of latency for writes, so for 3 replicas that's 1RTT between the client and the head and 1.5RTT across the three replicas. Primary-Backup and cite:[Hermes] use a 2RTT broadcast.

[cols="1,1,1,1,1,1,1"]
|===
|
| Storage Efficiency
| Read Bandwidth Efficiency
| Write Bandwidth Efficiency
| Chance of Unavailability on Failure
| Read Latency
| Write Latency

a| cite:[ChainReplication]
| 33%
| 33%
| 33%
| 100%
| 1RTT
| 2.5RTT

a| cite:[CRAQ]
| 33%
| 100%
| 33%
| 100%
| 1-3RTT
| 2.5RTT

a| cite:[PacificA]
| 33%
| 100%
| 16.7%
| 100%
| 1-2RTT
| 2RTT

a| cite:[Hermes]
| 33%
| 100%
| 33%
| 100%
| 1-2RTT
| 2RTT
|===

The end result showing that cite:[CRAQ] is a better version of cite:[ChainReplication], and cite:[Hermes] is a better version of cite:[PacificA].  To optimize for latency, choose cite:[Hermes].  To optimize for throughput, choose cite:[CRAQ].

It's important to note that the surface level simplicity of replication algorithms rooted in cite:[VirtualSynchrony] instead hold their complexity in two nontrivial topics: group membership and failure detection.

There's a broad examination of such distributed protocols stemming from cite:[VirtualSynchrony], and this perspective of view changes on detected failures is even present in consensus protocols such as cite:[ViewstampedReplication]. However, our dividing line for this analysis is that the failure detection replication algorithms require only stem:[f+1] nodes to tolerate stem:[f] failures.  If stem:[2f+1] nodes are required, then the algorithm is likely better covered by one of the two failure-masking classes of algorithms discussed above.  Viewstamped Replication itself is very much like Raft and Multi-Paxos, and covered by Leadered Consensus above.

Failure detectors have their own rich history that warrants a separate post sometime.  The simplest failure detector is a periodic heartbeat with a timeout.  The ideal failure detector is one that is both precise in isolating what has failed, and reactive in minimizing the time between the failure and the detector alerting about it.  

By depending on all replicas to be functioning correctly, one needs a very precise definition of what "functioning correctly" means.  If the disk is failing and its throughput drops by 90% or if there's a bad switch causing packet loss and thus TCP throughput drops significantly{nospace}sidenote:ref[], that's not a "correctly functioning" machine, and one would wish to reconfigure around the failure. cite:[GrayFailureAchillesHeel] discusses gray failure issues in more detail.  cite:[LimpingTolerantClouds] offers more concrete examples.  {uri-dan-luu-limplock}[Dan Luu has written about this as well].
[.aside]#sidenote:def[] The most frequent singular cause of times I've been paged awake by a service in the middle of the night has been some networking equipment deciding to drop 1% of packets, and TCP thus slowing down to approximately dial-up speeds.  Heartbeats can still be sent, so the service isn't "unavailable", but it sure wasn't working well.#

== Hybrid: Leaders

:uri-cockroach-stale-reads: https://www.cockroachlabs.com/blog/follower-reads-stale-data/
:uri-cockroach-follower-reads: https://github.com/cockroachdb/cockroach/issues/72593
:uri-cockroach-global-table: https://www.cockroachlabs.com/blog/global-tables-in-cockroachdb/
:uri-edb-pgdist-witness: https://www.enterprisedb.com/docs/pgd/latest/node_management/witness_nodes/
:uri-spanner-witness: https://cloud.google.com/spanner/docs/replication#witness
:uri-tikv-follower-reads: https://tikv.org/blog/double-system-read-throughput/
:uri-spanner-follower-reads: https://cloud.google.com/spanner/docs/replication#read-only
:uri-pingcap-follower-read-blog: https://www.pingcap.com/blog/doubling-system-read-throughput-with-only-26-lines-of-code/

//:uri-tigerbeetle-fpaxos: https://docs.tigerbeetle.com/deploy/hardware/
//Applying cite:[FlexiblePaxos] allows one to run with 4 replicas and require 3 to be alive for an election and still replicate across 2, which as far as I know only {uri-tigerbeetle-fpaxos}[TigerBeetle] implements.  

Leaderful consensus what is generally brought to mind when one mentions "consensus".  It is best known as cite:[Raft], cite:[MultiPaxos]{nospace}sidenote:ref[] or cite:[ZAB], and exemplified by distributed databases such as cite:[CockroachDB], cite:[TiDB] and cite:[Spanner], or configuration management systems such as cite:[PaxosMadeLive] and cite:[Zookeeper].  (Among _many_ other high-quality, production systems.)
[.aside]#sidenote:def[] Though for learning about Multi-Paxos, I'd significantly recommend reading cite:[PaxosMadeModeratelyComplex] and cite:[MultiPaxosMadeComplete] instead.#

In the simplest Raft implementation, one replica is nominated as a leader.  All operations are sent to the leader, and the leader broadcasts the replication stream to its followers.  Raft tolerates stem:[f] failures using stem:[2f+1] replicas.  Thus, at most two of five replicas are permitted to be unavailable.
Throughout this section, I will be using "Raft" and "Multi-Paxos" interchangably.  The differences between the two algorithms (discussed in detail in cite:[PaxosVsRaft]) have no effect on resource efficiency, throughput or latency.

[graphviz]
----
digraph G {
  Client -> Leader   [dir=both];
  Leader -> Replica1 [dir=both];
  Leader -> Replica2 [dir=both];
  Leader -> Replica3 [dir=both, style=dashed];
  Leader -> Replica4 [dir=both, style=dashed];
}
----

All replicas store a full copy of the data, and thus 1/5th of the total storage capacity is available post-replication.  The storage optimizations available are similar to what was discussed for leaderless replication.
cite:[WitnessReplicas] permit removing the full copy of the data from a minority (2/5ths) of the replication group, and the leaderful consensus variant of witness replicas is always able to serve reads from the leader even with a simple majority of replicas alive.  Note though, that removing storage means that witness replicas can't serve reads.  
Only {uri-edb-pgdist-witness}[EnterpriseDB Postgres Distributed] and {uri-spanner-witness}[(Cloud) Spanner] implement support for witness replicas as part of Raft and Multi-Paxos, respectively.
The other possible direction for storage efficiency improvement is cite:[ErasureCodedRaft] which again allows storing the equivalent of 3 copies spread across 5 replicas, thus achieving 33% storage efficiency a different way.
As 99% of the Raft implementations one might ever encounter have a storage efficiency of 1/5th, that is the value that will be used for storage efficiency for the rest of the analysis.

Naive Raft has the leader serve all reads, yielding 1/5th read throughput.  cite:[LinearizableQuorumReads] pitches the idea that one can also perform linearizable reads by reading from a majority quorom of the non-leader replicas, and implementing this brings Raft to 2/5ths read throughput. cite:[PaxosQuorumLeases]{nospace}sidenote:ref[] pitches the idea of electing a leader and two more replicas to which the leader must replicated all commits, thus enabling those required followers to serve reads to clients with no further coordination, which brings Raft to 3/5ths read throughput at the cost of some tail latency on writes and increased risk of unavailability on failure. cite:[ConsistentFollowerReads]{nospace}sidenote:ref[]{nospace}sidenote:ref[] allows any follower to serve read requests by first checking with the leader for the most recently applied position in the replication log, allowing for 5/5ths read throughput at the cost of read latency increasing to 2RTTs.  Each has their own set of tradeoffs, but we'll use 5/5ths as Raft's optimal read throughput, which is realistic given that follower reads have been implemented in production systems such as {uri-spanner-follower-reads}[Spanner] and {uri-tikv-follower-reads}[TiKV].
[.aside]#sidenote:def[] cite:[PaxosQuorumLeases] is an example of a replication algorithm that's a hybrid of Failure Masking and Failure Detection, but strikes a trade-off more towards Failure Detection than where Raft sits.#
[.aside]#sidenote:def[] "Follower reads" can be colloquially used to mean any form of reading from followers.  Cockroach in particular uses a number of tricks around timestamps to allow replicas to locally serve data.  What they call {uri-cockroach-stale-reads}[follower reads] allows replicas to serve reads for older versions.  Global tables support local, consistent reads by {uri-cockroach-global-table}[writing in the future].  My focus is specifically on linearizable reads which don't overly compromise writes, and {uri-cockroach-follower-reads}[that specific cockroach feature] isn't yet implemented.  But I highlight all of this to show that there's ways to deliver increased read throughput when bending other constraints or leaning on the semantics of other components (e.g. hybrid clocks).#
[.aside]#sidenote:def[] There's frustratingly no good citation for follower reads which contact that leader to keep their replies consistent and linearizable.  cite:[ConsistentFollowerReads] links to the Raft thesis, which mentions it in passing, but I can't find a paper which actually details the optimization well.  PingCAP's {uri-pingcap-follower-read-blog}[blog post on implementing it] is a more detailed overview.#

In classic Raft, all proposals go to the leader, and then the leader broadcasts the proposals to all followers.  This means Raft is first constraining to utilizing only stem:[1/(2f+1)] or 1/5th of the available incoming bandwidth.  Then the bottleneck becomes the leader's outgoing bandwidth, further reduction of stem:[1/2f], so 1/4th.  This means a write bandwidth efficiency of stem:[1/(4f^2 + 2f)] or 1/20th.  There have been ways discussed to scale the write bandwidth.  cite:[PullBasedConsensus] presents an argument that a fixed topology is not needed, replicas can fetch from other replicas, and thus even a linear chain replicas could work.  cite:[ScalingReplication] shows another view that the work of broadcasting to all replicas can be delegated to other replicas.  cite:[CommutativeRaft] presents a different approach, in which clients are allowed to directly send to all replicas, and the leader only arbitrates ordering when there's conflicts.  Of these, only cite:[PullBasedConsensus] is implemented in industry, but I'm not aware that even MongoDB itself runs in a linear chain configuration.  (It's mostly about saving WAN costs.)  cite:[ErasureCodedRaft] applies to the Raft log as well, providing a 5/3rds increase in bandwidth.  However, 1/20th is still the write bandwidth efficiency that almost any real Raft implementation will exhibit.

Many optimizations strike different points along the pareto curve of latency versus throughput, so I've outlined them all below.  Optimizing Raft only for latency would be only implementing cite:[LinearizableQuorumReads], but I'll also note a "Throughput Optimized Raft" implementation as the effect of combining cite:[ConsistentFollowerReads], cite:[PullBasedConsensus] in a linear chain of replicas, and cite:[ErasureCodedRaft].

[cols="1,1,1,1,1,1,1"]
|===
|
| Storage Efficiency
| Read Bandwidth Efficiency
| Write Bandwidth Efficiency
| Chance of Unavailability on Failure
| Read Latency
| Write Latency

| Simplest
| 20%
| 20%
| 5%
| 20%
| 1RTT
| 2RTT

a| cite:[LinearizableQuorumReads]
| 20%
| 40%
| 5%
| 20%
| 1RTT
| 2RTT

a| cite:[PaxosQuorumLeases]
| 20%
| 60%
| 5%
| 60%
| 1RTT
| 2RTT

a| cite:[ConsistentFollowerReads]
| 20%
| 100%
| 5%
| 20%
| 1-2RTT
| 2RTT

a| cite:[ErasureCodedRaft]
| 33%
| 20%
| 8.3%
| 20%
| 2RTT
| 2RTT

| Throughput Optimized Raft
| 33%
| 100%
| 20%
| 20%
| 2RTT
| 3.5RTT
|===

Databases built around Multi-Paxos generally aren't picking _just_ one optimization to implement.  The exact tradeoff of reads versus writes and throughput versus latency is specific to each individual use case.  Thus, databases tend to implement multiple optimizations, and allow users to configure specific database deployments or tables within the database for how they wish for reads and writes to be done.  The optimizations covered above are also just those that affect the resource efficiency.  There's a tremendously larger set of published optimizations focusing on performance when geographically distributed, enhancing failure recovery, managing replicated log truncation, etc.

One of the major points of this post which will be emphasized below is that Raft is 1/5th failure detection + 4/5ths failure masking.  In the failure detection section, we discussed the complexity in failure detection-based replication algorithms is often centered around group membership changes and (gray) failure detectors.  How to safely implement leader election and group membership changes has been often discussed in consensus papers, but the need for a comprehensive failure detector for the Raft leader is often overlooked.  Notably however, cite:[MultiPaxosMadeComplete] gives the topic a proper treatment.

== Comparison

There isn't a single way to do a direct, fair, apples-to-apples comparison of different systems and optimizations across the different replication algorithms.  Instead, I'll be presenting comparisons choosing the optimal point for each class of replication algorithms in several categories: the most popular/common choice in each class, the latency-optimal choice, the throughput-optimal choice, and the storage-optimal choice.

For the popularity-based rankings, we'll use "Paxos" from the Quorums section, "PacificA" from the Reconfiguration section, and 

[cols="1,1,1,1,1,1,1,1"]
|===
|
| Nodes Required for stem:[F=2]
| Storage Efficiency
| Read Bandwidth Efficiency
| Write Bandwidth Efficiency
| Chance of Unavailability on Failure
| Read Latency
| Write Latency

| Paxos (Quorums)
| 5
| 20%
| 20%
| 20%
| 0%
| 1RTT
| 2RTT

| PacificA (Reconfiguration)
| 3
| 33%
| 100%
| 16.7%
| 100%
| 1-2RTT
| 2RTT

| Follower Reads (Hybrid)
| 5
| 20%
| 100%
| 5%
| 20%
| 1-2RTT
| 2RTT
|===

Immediately apparent is that all Reconfiguration-based both requires less nodes, and delivers a uniformly equal-or-better resource efficiency for the less resources used.

The storage optimized variants are all the erasure coded variants.  Except we get to make a completely arbitrary choice for reconfiguration-based replication as they're all the same:

[cols="1,1,1,1,1,1"]
|===
|
| Nodes Required for stem:[f=2]
| Storage Efficiency
| Read Bandwidth Efficiency
| Write Bandwidth Efficiency
| Chance of Unavailability on Failure
| Read Latency
| Write Latency

| Erasure Coded Paxos
| 5
| 33%
| 33%
| 33%
| 0%
| 1RTT
| 2RTT

| CRAQ
| 3
| 33%
| 100%
| 33%
| 100%
| 1-3RTT
| 2.5RTT

| Erasure Coded Raft
| 5
| 33%
| 20%
| 8.3%
| 20%
| 2RTT
| 2RTT
|===

We see that erasure coding just brings each quorum-based algorithm to the resource efficacy of the reconfiguration-based algorithm, but still requires 66% more nodes than a reconfiguration-based algorithm.

There are a number of other resources to consider in a real environment other than what was presented in this post.  CPU, memory, disk IOPS, etc., are all finite resources, which were not discussed, but if those become the limiting factor for performance, then that is the bottleneck and efficiency metric to be mindful of. cite:[ScalableButWasteful] notes that CPU constraints can lead cite:[MultiPaxos] to have 2x more throughput than cite:[EPaxos].  If throughput is what determines the amount of hardware you need to buy/rent for your database deployment, and the hardware is CPU constrained, then this is a more impactful efficiency to keep in mind for leaderful vs leaderless quorum replication.  (However, I still claim reconfiguration-based replication would be even more cost effective!)

Correspondingly, reconfiguration-based replication algorithms are more common in block storage products where I suspect there's been more pressure on price; it's databases specifically which feels more like a monoculture.  cite:[Ceph] implements both parallel and chain replication.  cite:[GFS] implements parallel replication where the client is responsible for sending the data to each replica to be buffered, and the primary broadcast is used to tell each replica to write the buffered data.  cite:[HDFS] similarly follows suit.  That's not to say that all block storage products do, as for example cite:[AlibabaEBS] and cite:[PolarFS] use leaderful consensus, but just that reconfiguration-based replication is comparatively a much more frequently chosen solution for replication in the different domain.

And I do understand that there's reasons why we don't see a large number of reconfigurable-primary backup databases.  Requiring an external consensus service to manage replication groups requires first having a consensus solution implemented and production ready.  This could either be using an existing solution like Zookeeper or Etcd, but then those might have existing issues, you're then required to become an expert in running them, and using a separate project as the core of your service adds a significant hassle to testing.  The alternative is to implement your own, and then you're doing all the work of just deploying leaderless Paxos or leaderful Raft, only to turn and do even more work before being able to deploy to production.  And there still isn't a single great reference for how to safely identify and reconfigure around gray failures.

== References

[.bibliography]
--
bibliography::[]
--

link:2024-resource-efficiency-in-replication.bib[References as BibTeX]
