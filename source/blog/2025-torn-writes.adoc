= Torn Write Detection and Protection
:revdate: 2025-04-12
:toc: right
:page-hook-preamble: false
:page-aside: With credit to Phil Eaton for some of the sources and links, Tanel Poder for Oracle's torn write detection mechanism and details on AlloyDB Omni's use of RWF_ATOMIC, Andres Freund for more details on Postgres's full page logging, and Claude for generating reasonable SVG diagrams with minimal effort.

Disks have a unit of data at which they promise atomicity: the sector.  Historically, this was 512 bytes. Disks also accept write requests for multiple contiguous sectors at a time.  In the event of a power failure, this can lead to only a subset of the sectors being written: a _torn write_. Databases operate on 4KB or larger pages, and thus are vulnerable to torn writes.  A database page affected by a torn write is called a _torn page_.  Various databases have different opinions and strategies on detecting torn pages during recovery, and how to restore the page to a correct, full page image.

image::torn-write-example.svg[inline=true]

For a more explanation of torn writes or torn pages, see content elsewhere such as
https://www.joshodgers.com/tag/torn-write/[blog posts],
https://learn.microsoft.com/en-us/troubleshoot/sql/database-engine/database-file-operations/logging-data-storage-algorithms#torn-page-detection[product documentation],
https://www.youtube.com/watch?v=OtxCzIHOMk4[videos],
or chat with your favorite LLM offering.

////
Using the color scheme of:

* Write ahead log background fill="#f6ffed" stroke="#52c41a"
* B-Tree background fill="#e6f7ff" stroke="#1890ff"
* Page A background fill="#91caff" stroke="#1890ff"
* Page B background fill="#b7eb8f" stroke="#52c41a"
* Page C background fill="#ffe58f" stroke="#fa8c16"

Describe the write-ahead log using the title "Write-Ahead Log".
Describe the double-write buffer using the title "Double-Write Buffer".
Describe the B-Tree using the title "B-Tree".

Do not include a title for the diagram, nor a summary at the bottom.
Do not set a background for the diagram as a whole.

////

== Detection Only

Identifying that a page is invalid because it had been subject to a torn write can be done in cheaper ways than treating it as a corruption caught by checksums.  One can include in written sectors a counter, which is required to be incremented by any amount on each write. If a page is read, and the counters in each sector aren't all the same value, then a torn page has been identified, and the page should be treated as corrupt or invalid.

image::torn-write-detection.svg[inline=true,align=center]

////
Make an SVG diagram showing torn write detection, via having a small counter at the beginning of every sector in a page write.  Show a page on the left being updated into a page on the right, where the page on the left is broken into 8 sectors, each of which starts with a small counter.  The page on the right should show the first half of the sectors as updated, and the second half as the same as before.  Put both pages within a B-Tree box.

Use the color scheme of:

* B-Tree background fill="#e6f7ff" stroke="#1890ff"
* Page (Before) background fill="#91caff" stroke="#1890ff"
* Page (After) background fill="#b7eb8f" stroke="#52c41a"

Describe the B-Tree using the title "B-Tree".

Do not include a title for the diagram, nor a summary at the bottom.
Do not set a background for the diagram as a whole.
////


:uri-sqlserver-page-size: https://learn.microsoft.com/en-us/sql/relational-databases/pages-and-extents-architecture-guide?view=sql-server-ver16#pages
:uri-sqlserver-torn-page-detection: https://learn.microsoft.com/en-us/troubleshoot/sql/database-engine/database-file-operations/logging-data-storage-algorithms#torn-page-detection
:uri-sqlserver-suspect-pages: https://learn.microsoft.com/en-us/sql/relational-databases/system-tables/suspect-pages-transact-sql?view=sql-server-ver16
:uri-sqlserver-page-restore: https://learn.microsoft.com/en-us/sql/relational-databases/backup-restore/restore-pages-sql-server?view=sql-server-ver16

SQL Server has an {uri-sqlserver-page-size}[8KB page size]. {uri-sqlserver-torn-page-detection}[Torn page detection] is a feature that can be enabled or disabled in SQL Server, which takes the first two bits of each 512 byte sector, concatenates them into one 32bit value, and stores it in the page header.  The first two bits are then replaced with a 2-bit counter that is incremented on each page write. Torn pages have counter values that don't match across sectors. Pages that have failed the torn page detection check are logged in a {uri-sqlserver-suspect-pages}[suspect pages table], and {uri-sqlserver-page-restore}[page restore] seems to be the most recommended solution for repairing the pages.  Torn page detection has been largely superseded by page-level checksums (`PAGE_VERIFY=CHECKSUM`), which is also a valid way to view torn pages: it's just another type of potential data corruption.

:uri-oracle-bbed: https://www.orafaq.com/papers/dissassembling_the_data_block.pdf
:uri-oracle-dbms-repair: https://docs.oracle.com/en/database/oracle/oracle-database/19/admin/repairing-corrupted-data.html
:uri-oracle-rman: https://docs.oracle.com/en/database/oracle/oracle-database/19/bradv/rman-block-media-recovery.html
:uri-oracle-db-block-checksum: https://docs.oracle.com/en/database/oracle/oracle-database/23/refrn/DB_BLOCK_CHECKSUM.html
:uri-lmdb-sector-order: https://www.openldap.org/lists/openldap-devel/201410/msg00004.html
:uri-lmdb-sector-fsync: https://lists.openldap.org/hyperkitty/list/openldap-devel@openldap.org/thread/YUUKXVYXA347IWW3UKRS6NJHBU4FEE6M/

Oracle has an 8KB page size as well. The structure of data pages was only published as part of the {uri-oracle-bbed}[Block Browser and Editor documentation].  Oracle RDBMS can detect torn writes by comparing the System Change Number in the header of a page with a copy of it stored in the tail of the page.{nospace}sidenote:ref[]  (See pages 10-12 of the linked PDF.) After a torn page is detected, an administrator must perform _media recovery_ using {uri-oracle-dbms-repair}[DBMS_REPAIR] or {uri-oracle-rman}[RMAN] to fix the corrupt blocks. {uri-oracle-db-block-checksum}[`DB_BLOCK_CHECKSUM=TYPICAL`] will cause torn pages to be detected as checksum failures, but the head and tail SCNs can still catch torn pages even with the checksumming disabled.
[.aside]#sidenote:def[]Note that this assumes that sectors will be written and persisted by the firmware sequentially, in order, and that's not actually a promised property.  Howard Chu, of LMDB fame, has made a couple of arguments {uri-lmdb-sector-order}[[1\]] {uri-lmdb-sector-fsync}[[2\]] over time that relying on such behavior is safe in practice.#

:uri-ssd-power-fault: https://6826.csail.mit.edu/2017/papers/fast13-final80.pdf

There exists an argument that enterprise SSDs which include supercapacitors for power-loss safety, the same ones that are well-discussed for allowing `fsync()` to complete asynchronously, are also immune from torn writes.  The ATA/SATA/SCSI/NVME protocols themselves allow write commands to span multiple logical blocks, so a multi-block write command is either received or not. Even in the event of a power loss, the drive is expected to have enough power left to flush its volatile cache, and the argument is that cache write-back includes finishing any in-flight requests. The lack of the Force Unit Access flag on those writes means there's no explicit contract that they must have been fully persisted. This argument assumes perfectly working firmware, which is not a generally recommended assumption.{nospace}sidenote:ref[] However, in such real-world situations, torn writes leading to torn pages have been sufficiently uncommon in practice that widely deployed databases like Oracle and SQL Server have managed to get away with no automatic mitigation for torn pages.
[.aside]#sidenote:def[] I'm in the process of working through SSD failure analysis papers, but {uri-ssd-power-fault}[Understanding the Robustness of SSDs under Power Fault] includes testing of a 2009 SSD with capacitors that exhibited torn writes under power failure. I'll update this once I've concluded my paper series reading.#

== Sector-Sized Pages

:uri-nvme-nvm-spec: https://nvmexpress.org/wp-content/uploads/NVMe-NVM-Command-Set-Specification-1.0a-2021.07.26-Ratified.pdf
:uri-4kb-lba: https://www.bjonnh.net/article/20210721_nvme4k/

Storage device specifications require disks to make writes of an individual sector atomic.  If the sector size of the disk and the database page size match, then torn writes become impossible!  Historically, disks have used 512 byte sectors. Database page sizes are a balance between branching factor, risk of needing overflow pages to maintain a minimum branching factor, and write amplification for a small update.  512 bytes generally isn't enough space to be able to ensure a sufficient branching factor within a single page, so 512 bytes of atomicity isn't enough to be useful.

These days, some NVMe SSDs do natively support a 4096 byte physical sector size{nospace}sidenote:ref[], even among those made for the consumer market. SSDs with Phison controllers are highly likely to support 4096 byte logical block addressing, Western Digital and SanDisk are likely to have support, and some SK Hynix and Kingston drives do as well.  However, cloud support for this is questionable, as AWS's i4i line was confirmed to not support 4096 byte logical blocks.{nospace}sidenote:ref[] One may check NVMe drives' support by using `nvme-cli id-ns` and change it using `nvme-cli format` {uri-4kb-lba}[as described elsewhere].
[.aside]#sidenote:def[] And I'd love to hear reports on if other instance types or other cloud vendors' instances do or don't support 4096 byte sectors.#
[.aside]#sidenote:def[] Note that the {uri-nvme-nvm-spec}[NVMe NVM Command Set Specification]'s 2.1.4 Atomic Operation specifies that Atomic Write Unit Power Fail is defined in terms of the number of logical blocks, so configuring a 4KB logical block side does require 4KB writes to be atomic as per the NVMe spec.#
 
:uri-aws-torn-write-prevention: https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/storage-twp.html
:uri-gcp-torn-write-prevention: https://price2meet.com/gcp/docs/compute_docs_tutorials_16kb-mysql-best-practices.pdf
:uri-alicloud-torn-write: https://www.alibabacloud.com/help/en/rds/apsaradb-rds-for-mysql/optimized-write

The story in the cloud is a bit better when using the persistent disk offerings.  AWS advertises {uri-aws-torn-write-prevention}[torn write prevention] which permits configuring EBS volumes to support database page-sized atomic writes.  GCP had {uri-gcp-torn-write-prevention}[experimented with similar support], which they've since retracted from their own docs. Alibaba appears to include it as a {uri-alicloud-torn-write}[write optimization feature] as part of their AsparaDB RDS for MySQL offering.  As IOPS and throughput are limited on cloud persistent disks, it's a good feature to take advantage of if it's available.

== Log Full Pages

A simple way to be able to fix torn pages is to write the entire updated page into the write-ahead log, for each page that is updated.  If a failure causes a torn write, the recovery process will re-apply the pages from the WAL to the B-Tree, thus fixing the torn page.  If a failure affects the WAL, then the transaction never committed, and thus can be instead rolled back during recovery.

image::full-page-logging.svg[inline=true,align=center]

The downsides to this simplicity are notable. This incurs a 2x write amplification, as every page is now being written twice. Space amplification is significant, as now the entire page is being written instead of just the tuple modification. But, simplicity is often a worthwhile benefit.

:uri-sqlite-wal: https://sqlite.org/wal.html
:uri-sqlite-psow: https://sqlite.org/psow.html
:uri-sqlite-page-size: https://sqlite.org/pgszchng2016.html

SQLite defaults to a 4KB page size{nospace}sidenote:ref[] and follows this approach to torn page protection (which they call {uri-sqlite-psow}["powersafe overwrite"]). In {uri-sqlite-wal}[WAL mode], updates to pages cause the full page to be placed into the WAL.  During a checkpoint, performed at COMMIT or after 1000 pages are written, all pages are applied from the WAL back to the B-Tree.  Checkpointing can thus only be done when there are no open transactions, and long-running write transactions can cause the WAL to grow significantly.
[.aside]#sidenote:def[] Until 2016, {uri-sqlite-page-size}[the default was 1KB pages].#

== Log Page on First Write

Logging page deltas is preferable over full page contents for the reduced write and space amplification.  To permit any page to be reconstructed from the write-ahead log, a full copy of the page can be saved only once right before it is first modified. Then, if a torn write later causes a torn page, the older version of the page can be fetched from the write-ahead log, and all deltas applied to produce the correct page contents.

image::first-write-page-logging.svg[inline=true,align=center]

:uri-postgres-full-page-writes: https://wiki.postgresql.org/wiki/Full_page_writes
:uri-reading-is-writing: https://blog.danslimmon.com/2025/03/14/did-u-ever-read-so-hard-u-accidentally-wrote/
:uri-edb-full-page-writes: https://www.enterprisedb.com/blog/impact-full-page-writes
:uri-aws-full-page-writes: https://www.slideshare.net/slideshow/full-page-writes-in-postgresql-pgconfeu-2022/253854027
:uri-postgres-page-size: https://www.postgresql.org/message-id/200611281746.32245.peter_e%40gmx.net

As checkpointing removes previous write-ahead log files, pages will need to be copied to the write-ahead log on their first modification following each checkpoint. But copying the page to the write-ahead log only once per checkpoint still lowers the expected costs drastically. The normal state will trend towards ~1x write amplification, and most of the WAL can remain descriptions of page changes rather than full page images which helps space amplification.

Postgres has an 8KB page size{nospace}sidenote:ref[] and utilizes this torn write protection technique, which it calls {uri-postgres-full-page-writes}[full page writes]. The page images are compressed before being written into the write-ahead log. During recovery, postgres unconditionally{nospace}sidenote:ref[] applies the save full page images and re-applies all logged changes to the pages to avoid the random IO on the write-ahead log. The impact of `full_page_writes` has been studied in detail across blog posts such as {uri-edb-full-page-writes}[On the impact of full page writes] or talks like {uri-aws-full-page-writes}[Full page Writes in PostgreSQL]. For a tale of an interaction with full page writes causing issues, see {uri-reading-is-writing}[did u ever read so hard u accidentally wrote?]
[.aside]#sidenote:def[] The page size can be changed at compile time, with {uri-postgres-page-size}[4KB being the lowest probably safe value]. EBS charges for IOPS in units of rounding up to the nearest 16KB, so I'm surprised we don't see more 16KB page size postgres being deployed in the cloud, but Andres Freund noted he's seen a measurable latency difference when he's looked at page sizing before.#
[.aside]#sidenote:def[] This has tangential advantages of pre-populating the buffer pool during recovery, thus lessening the cold cache latency effect of a restarted postgres instance. Postgres replication is also WAL-based, and thus benefits similarly.#

== Double-Write Buffer

Rather than relying on the write-ahead log, it's possible to move the torn write protection responsibility entirely to the B-Tree.  By first writing all B-Tree pages to a dedicated scratch space on disk, one can ensure that any torn page has a full and correct copy of the page in the scratch space to recover from instead.

image::double-write-buffer.svg[inline=true,align=center]

:uri-mysql-double-write-buffer: https://dev.mysql.com/doc/refman/5.7/en/innodb-doublewrite-buffer.html
:uri-innodb-page-size: https://www.percona.com/blog/small-innodb_page_size-performance-boost-ssd/
:uri-percona-benchmark: https://www.percona.com/blog/improve-innodb-performance-write-bound-loads/
:uri-mysql-wl: https://dev.mysql.com/worklog/task/?id=5655
:uri-mysql-bug: https://bugs.mysql.com/bug.php?id=81376
:uri-mysql-new-dbw: http://dimitrik.free.fr/blog/posts/mysql-80-perf-new-dblwr.html

Double-write buffering takes an opposite set of trade-offs as logging pages into the write-ahead log.  There's no extra work involved with the write-ahead log, and instead, all responsibility is moved to the B-Tree page writes themselves. The cost though is the database must write every B-Tree page twice, and fsync the double-write buffer before writing to the B-Tree. In the best case, the working set fits in the page cache, and B-Tree pages will only _need_ to be persisted once per checkpoint.  In the worst case, the workload is largely out of memory, and the double-write buffer causing double the writes could be very noticeable. 

MySQL's InnoDB has a 16KB page size{nospace}sidenote:ref[] and is the most well-known user of the {uri-mysql-double-write-buffer}[double write buffer] strategy, and the only user (including its XtraDB fork). Using a double write buffer has the advantage of not blocking commits on writing full page images into the WAL.  It maintains the notable downside of 2x the write latency and 2x write amplification.
[.aside]#sidenote:def[] The InnoDB page size can be changed at database initialization time.  Percona once benchmarked the effect of {uri-innodb-page-size}[using a 4KB InnoDB page size].#

I've had a few conversations with Sunny Bains, who worked on InnoDB including the double write buffer implementation, and so to forward some historical context and wisdom on the subject:

* When looking at {uri-mysql-double-write-buffer}[benchmarks implying the double-write buffer is a significant slowdown], be aware that the double-write buffer was a source of mutex contention under high write throughput for single page flushes. This was addressed as part of MySQL 8.0.23 in 2021 by {uri-mysql-wl}[WL#5655] and see bug tracker discussion like {uri-mysql-bug}[Bug #81376]. {uri-mysql-new-dbw}[The New InnoDB Double Write Buffer in Action] shows the impact of the improvement.
* The last time the double-write buffer code was being overhauled, Intel Optane was coming out and it was thought that one would be able to place the double-write buffer on a separate Optane drive, thus utilizing its infinite IOPS and fast fsync()s to minimize the overhead of the double-write buffer. Instead, Intel killed Optane, and running MySQL on EBS became popular, where IOPS on EBS are instead highly constrained.
* Some prominent MySQL users had production environments where MySQL was already replicated, and replacing a replica was a sufficiently fast "fix" for torn pages. Thus, `innodb_doublewrite=DETECT_ONLY` was added to only check pages for torn writes during recovery but not do the full double-write buffer process to permit automatically fixing them. 

== Copy on Write

////
Create an SVG diagram showing Copy on Write being used as a way to avoid torn pages on failure.  It should show an initial B-Tree page of Page V1 and a Root V1 pointing to it in a leftmost column, and then Page V2 being written as the middle column, and then Root V2 being written in the rightmost column.  Connect Page V1 and Page V2 with an arrow, and Page V2 and Root V2 with an arrow to show the write order.

Use the color scheme of:

* B-Tree background fill="#e6f7ff" stroke="#1890ff"
* Page V1 background fill="#91caff" stroke="#1890ff"
* Page V2 background fill="#b7eb8f" stroke="#52c41a"
* Root Page background fill="#ffe58f" stroke="#fa8c16"

Do not include a title for the diagram, nor a summary at the bottom.
Do not set a background for the diagram as a whole.
////

// https://smalldatum.blogspot.com/2015/08/different-kinds-of-copy-on-write-for-b.html

A torn write can only turn into a torn page if it occurs while overwriting a page.  The torn write must affect a page being updated in-place in the B-Tree.{nospace}sidenote:ref[] This provides a clear way around the issue of torn writes: never update a page in-place. When one wishes to modify a B-Tree page, allocate a new page in the file, place the updated copy of the page there, and then adjust the parent to point to the newly "updated" child. Adjusting the parent is also an update, and so the copying will recurse up to the root of the B-Tree. Copy-on-Write B-Trees first write all pages except the root, and then a successful write of a new B-Tree root marks them all as committed updates to the file{nospace}sidenote:ref[]. This technique is also known as _shadow paging_.
[.aside]#sidenote:def[] This is also why this entire post is about B-Trees.  LSMs are structured entirely as append-only write-once files.  The lack of in-place page updates in an LSM means they're immune to torn pages, by design.#
[.aside]#sidenote:def[] Copy-on-Write B-Trees can thus perform atomic multi-page updates without the use of a write-ahead log, and thus very frequently skip implementing one.#

image::copy-on-write.svg[inline=true,align=center]

:uri-how-append-only-btree-works: https://www.bzero.se/ldapd/btree.html
:uri-wandering-trees: https://www.klennet.com/notes/2024-06-16-wandering-trees.aspx
:uri-cowr-cows: https://smalldatum.blogspot.com/2015/08/different-kinds-of-copy-on-write-for-b.html

For a better illustration of how Copy-on-Write B-Trees work, see {uri-how-append-only-btree-works}[how the append-only btree works] or {uri-wandering-trees}[Copy-on-Write, wandering trees, and data recovery]. Note that there are two {uri-cowr-cows}[different kinds of copy-on-write for a b-tree: CoW-Random and CoW-Sequential].

A major advantage of Copy-on-Write B-Trees is their simplicity. There are no torn pages to worry about, no separate write-ahead log, and any set of pages can be updated atomically in the tree.  The downside is everything else.  Copying from leaf to root means higher write amplification for single-page updates.  Each update requires two `fsync()` latencies before it can be considered durable, rather than one with a write-ahead log.  There's no clear way to allow multiple updates across the tree concurrently, as any update will eventually contend on updating the root node, which includes pointers to all of its children.

:uri-lmdb: http://www.lmdb.tech/doc/
:uri-lmdb-whitepaper: https://www.openldap.org/pub/hyc/mdb-paper.pdf
:uri-lmdb-talk: https://www.youtube.com/watch?v=tEa5sAh-kVk
:uri-lmdb-freelist: https://github.com/erigontech/erigon/wiki/LMDB-freelist-illustrated-guide

{uri-lmdb}[LMDB] is one of the most widely known Copy-on-Write B-Tree implementations. More details on LMDB internals can be found in {uri-lmdb-whitepaper}[its whitepaper], {uri-lmdb-talk}[talks], or {uri-lmdb-freelist}[third-party blog posts].

== Copy on First Write

:uri-orioledb-cow-checkpoint: https://github.com/orioledb/orioledb/blob/main/doc/architecture/overview.mdx#copy-on-write-checkpoints

////
Create an SVG diagram showing Copy on First Write being used as a way to avoid torn pages on failure.  It should show a Write-Ahead Log on the left, and a B-Tree on the right. The B-Tree should have an initial Root V1 and Page V1 in it, with the root pointing to the page, at Log Sequence Number 100.  The Delta for Page V2 should be written to the Write-Ahead Log at Log Sequence Number 200, and should cause Root V2 and Page V2 to be written to the B-Tree.  Then a subsequent write should have the delta for Page V3 written to the WAL at Log Sequence Number 300, and the B-Tree should then reflect Root V2 pointing to Page V3.

Use the color scheme of:

* Write ahead log background fill="#f6ffed" stroke="#52c41a"
* B-Tree background fill="#e6f7ff" stroke="#1890ff"
* Page V1 background fill="#91caff" stroke="#1890ff"
* Page V2 background fill="#b7eb8f" stroke="#52c41a"
* Page V3 background fill="#ffe58f" stroke="#fa8c16"

Do not include a title for the diagram, nor a summary at the bottom.
Do not set a background for the diagram as a whole.
////

Once can extend the Copy on Write technique to mitigate the throughput and latency issues mentioned above: use copy-on-write only for the first update of a page within each checkpoint interval. This is analogous to Log Page on First Write, except the page is maintained in the B-Tree instead of being copied into the WAL. If a torn write affects a page that was updated in-place, all of the deltas in the write-ahead log can be applied to the copied-from, older versioned page to produce the correct page contents.

image::copy-on-first-write.svg[inline=true,align=center]

Copy on First Write blends together the advantages of Log on First Write and Double Write Buffer. In the usual case, it has only a 1x write amplification and no penalty to write latency. After a checkpoint, new writes do need to do extra work of copying data in the B-Tree, but those writes do not block the commit into the WAL. The notable disadvantage is that old pages must be maintained for the duration until the next checkpoint completes, thus causing there to be space amplification on the order of the working set between checkpoints.

:uri-orioledb: https://www.orioledb.com/
:uri-orioledb-cow-checkpoints: https://www.orioledb.com/docs/architecture/overview#copy-on-write-checkpoints
:uri-orioledb-free-space: https://www.orioledb.com/docs/architecture/fsm

{uri-orioledb}[OrioleDB] is the only database I'm aware of that uses this strategy (and reading its code is how I learned about it).  They have {uri-orioledb-cow-checkpoints}[well-illustrated documents] that better describe its behavior. Reading about {uri-orioledb-free-space}[free space management] is also insightful, as copied-from pages can only be recycled into free space once a checkpoint is completed, which makes tracking free blocks more complicated.

== Atomic (Multi-)Block Writes

:uri-lwn-atomic-write: https://lwn.net/Articles/963742/
:uri-xfs-large-atomic-writes: https://patchwork.kernel.org/project/xfs/cover/20250102140411.14617-1-john.g.garry@oracle.com/
:uri-xfs-fix: https://patchwork.kernel.org/project/xfs/patch/20250102140411.14617-6-john.g.garry@oracle.com/
:uri-alloydb-omni: https://cloud.google.com/alloydb/omni
:uri-alloydb-atomic-writes: https://cloud.google.com/alloydb/omni/16.3.0/docs/improve-database-performance-using-io-acceleration#torn-write-protection
:uri-bsky-alloydb-broken: https://bsky.app/profile/alexmillerdb.bsky.social/post/3lmgecyjf4s2w

Linux has recently begun introducing support for {uri-lwn-atomic-write}[atomic writes for torn write protection]. man:pwritev2[2] has gained a `RWF_ATOMIC` flag, used to indicate that the submitted writes should be performed atomically. Linux 6.11 marked the introduction of `RWF_ATOMIC` and man:statx[2] being extended to allow checking for support.  XFS and ext4 gained support for `RWF_ATOMIC` with 6.13.  Work is slated to merge into 6.15 to allow {uri-xfs-large-atomic-writes}[atomic writes of data larger than the filesystem block size].

To utilize this feature, first invoke man:statx[2], and check the returned fields:

* `stx_atomic_write_unit_min`: The minimum size in bytes supported an atomic write. Guaranteed to be a power of 2.
* `stx_atomic_write_unit_max`: The maximum size in bytes supported an atomic write. Guaranteed to be a power of 2.
* `stx_atomic_write_segments_max`: The number of writes, within the min and max above, which can be submitted as part of `pwritev2()` that will be persisted together, atomically.

The values returned will include the result of querying the underlying storage hardware for what it supports.

Writes should then be performed via man:pwritev2[2] with `RWF_ATOMIC` specified in the flags, on a file descriptor opened with `O_DIRECT`, and the writes must match an alignment the same as the size of the write. Invocations of `pwritev2()` must voluntarily stay within the limits of the min and the max atomic write unit. If the vector count exceeds max write segments, `pwritev2()` will return `-EINVAL`, but if the size of a write exceeds the max atomic write unit, it is currently filesystem dependant as to if the `pwritev2()` will return an error or silently complete with lower guarantees.{nospace}sidenote:ref[]  There is no promise made on a specified level of write amplification or number of write latencies involved.
[.aside]#sidenote:def[] As of 6.13, ext4 will return an error, and XFS won't. XFS {uri-xfs-fix}[will be fixed], and then checking `statx()` first won't be mandatory.#

The currently in-progress extension of `RWF_ATOMIC` to multi-block atomicity in Linux 6.15+ will function via an approach similar to copy-on-write: the data will first be written to a new extent, and then the extent tree will be modified to remap the overwritten offset of the file to point to the new data.  Relying on this likely means the equivalent of two `fsync()` latencies, and increased overhead of extent metadata and offset lookup cost, so using one of the "on first write" torn write protection strategies described above will still yield better performance for the database.

{uri-alloydb-omni}[AlloyDB Omni] has {uri-alloydb-atomic-writes}[recently announced support] for `RWF_ATOMIC`. They advertise that enabling `alloydb_omni_atomic` means that one may disable `full_page_writes`, to avoid the overhead of the "Log Page on First Write" approach described above. Note that this still currently requires a storage device that supports atomically writing 8KB.
//[.aside]#sidenote:def[] Which it {uri-bsky-alloydb-broken}[appears to use unsafely]. AlloyDB checks to see if a `RWF_ATOMIC` write to a test file succeeds, but Linux 6.13 XFS permits block-sized `RWF_ATOMIC` writes even if the underlying disk doesn't support that level of atomicity.#

== Comparison

Our comparison is conducted along three dimensions:

* *Write Latency*: What is the minimum number of `fsync()` latencies required for a single row update to be made durable?
* *Write Amplification*: A single B-Tree leaf page update produces how many additional page-sized writes?
* *Space Amplification*: How much extra space, as compared to the total size of the database, does this approach require?

[cols="1,1,1,1"]
|===
|
h| Write Latency
h| Write Amplification
h| Space Amplification

| Detection Only
| 1x
| 1x
| 1x

| Sector-Sized Pages
| 1x
| 1x
| 1x

| Log Full Pages
| 1x
| 2x
| 1x-100x^[1]^

| Log Page on First Write
| ~1x^[2]^
| ~1x
| O(2x)

| Double-Write Buffer
| 2x
| 2x
| 1x

| Copy on Write
| 2x
| O(height of B-Tree)
| O(2x)

| Copy on First Write
| ~1x
| ~1x
| O(2x)

| Atomic Block Writes
| 1x
| 1x
| 1x

| Atomic Multi-Block Writes
| 2x
| 1x + ~1^[3]^
| ~1x^[4]^
|===

[1]: The WAL size would increase by a factor of `size of page / average size of update`, and the only bound on the additional space used is determined by the checkpointing interval or policy. +
[2]: ~1x, meaning sometimes it might be more than 1x, but most operations will experience 1x. +
[3]: Or whatever approximate constant the total write amplification of an extent update in the average filesystem works out to be. +
[4]: There will be some extra space overhead in storing extent metadata for every 8KB or 16KB page rather than at intervals of MB of data.