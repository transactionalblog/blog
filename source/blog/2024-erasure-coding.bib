@article{BlobStorageSurvey,
    author = {Nachiappan, Rekha and Javadi, Bahman and Calheiros, Rodrigo N. and Matawie, Kenan M.},
    title = {Cloud storage reliability for Big Data applications},
    year = {2017},
    issue_date = {November 2017},
    publisher = {Academic Press Ltd.},
    address = {GBR},
    volume = {97},
    number = {C},
    issn = {1084-8045},
    _url = {https://doi.org/10.1016/j.jnca.2017.08.011},
    _doi = {10.1016/j.jnca.2017.08.011},
    abstract = {Cloud storage systems are now mature enough to handle a massive volume of heterogeneous and rapidly changing data, which is known as Big Data. However, failures are inevitable in cloud storage systems as they are composed of large scale hardware components. Improving fault tolerance in cloud storage systems for Big Data applications is a significant challenge. Replication and Erasure coding are the most important data reliability techniques employed in cloud storage systems. Both techniques have their own trade-off in various parameters such as durability, availability, storage overhead, network bandwidth and traffic, energy consumption and recovery performance. This survey explores the challenges involved in employing both techniques in cloud storage systems for Big Data applications with respect to the aforementioned parameters. In this paper, we also introduce a conceptual hybrid technique to further improve reliability, latency, bandwidth usage, and storage efficiency of Big Data applications on cloud computing.},
    journal = {J. Netw. Comput. Appl.},
    month = {nov},
    pages = {35-47},
    numpages = {13},
    keywords = {Big Data applications, Cloud storage, Data reliability, Erasure coding, Fault tolerance, Replication},
    scholarcluster = {12723199345811969350}
}
@inproceedings{RSPaxos,
    author = {Mu, Shuai and Chen, Kang and Wu, Yongwei and Zheng, Weimin},
    title = {When paxos meets erasure code: reduce network and storage cost in state machine replication},
    year = {2014},
    isbn = {9781450327497},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    _url = {https://doi.org/10.1145/2600212.2600218},
    _doi = {10.1145/2600212.2600218},
    abstract = {Paxos-based state machine replication is a key technique to build highly reliable and available distributed services, such as lock servers, databases and other data storage systems. Paxos can tolerate any minority number of node crashes in an asynchronous network environment. Traditionally, Paxos is used to perform a full copy replication across all participants. However, full copy is expensive both in term of network and storage cost, especially in wide area with commodity hard drives.In this paper, we discussed the non-triviality and feasibility of combining erasure code into Paxos protocol, and presented an improved protocol named RS-Paxos (Reed Solomon Paxos). To the best of our knowledge, we are the first to propose such a combination. Compared to Paxos, RS-Paxos requires a limitation on the number of possible failures. If the number of tolerated failures decreases by 1, RS-Paxos can save over 50\% of network transmission and disk I/O. To demonstrate the benefits of our protocol, we designed and built a key-value store based on RS-Paxos, and evaluated it on EC2 with various settings. Experiment results show that RS-Paxos achieves at most 2.5x improvement on write throughput and as much as 30\% reduction on latency, in common configurations.},
    booktitle = {Proceedings of the 23rd International Symposium on High-Performance Parallel and Distributed Computing},
    pages = {61-72},
    numpages = {12},
    keywords = {asynchronous message passing model, consensus, erasure code, paxos, state machine replication},
    location = {Vancouver, BC, Canada},
    series = {HPDC '14},
    refname = {RS-Paxos},
    scholarcluster = {16520033292975033789},
}
@INPROCEEDINGS{ErasureCodedHRaft,
    author = {Jia, Yulei and Xu, Guangping and Sung, Chi Wan and Mostafa, Salwa and Wu, Yulei},
    booktitle = {2022 IEEE International Parallel and Distributed Processing Symposium (IPDPS)}, 
    title = {HRaft: Adaptive Erasure Coded Data Maintenance for Consensus in Distributed Networks}, 
    year = {2022},
    volume = {},
    number = {},
    pages = {1316-1326},
    keywords = {Fault tolerance;Costs;System performance;Fault tolerant systems;Distributed databases;Switches;Throughput;Erasure coding;Consensus protocol;Raft;Paxos;Fault tolerance;Network storage},
    _doi = {10.1109/IPDPS53621.2022.00130},
    refname = {Erasure Coded HRaft},
    scholarcluster = {15724086733201598850},
}
@inproceedings {ErasureCodingWindowsAzureStorage,
    author = {Cheng Huang and Huseyin Simitci and Yikang Xu and Aaron Ogus and Brad Calder and Parikshit Gopalan and Jin Li and Sergey Yekhanin},
    title = {Erasure Coding in Windows Azure Storage},
    booktitle = {2012 USENIX Annual Technical Conference (USENIX ATC 12)},
    year = {2012},
    isbn = {978-931971-93-5},
    address = {Boston, MA},
    pages = {15--26},
    _url = {https://www.usenix.org/conference/atc12/technical-sessions/presentation/huang},
    publisher = {USENIX Association},
    month = jun,
    refname = {Erasure Coding in Azure Storage},
    scholarcluster = {7930684733311413322},
}
@inproceedings {SDCodes,
    author = {James S. Plank and Mario Blaum and James L. Hafner},
    title = {{SD} Codes: Erasure Codes Designed for How Storage Systems Really Fail},
    booktitle = {11th USENIX Conference on File and Storage Technologies (FAST 13)},
    year = {2013},
    isbn = {978-1-931971-99-7},
    address = {San Jose, CA},
    pages = {95--104},
    _url = {https://www.usenix.org/conference/fast13/technical-sessions/presentation/plank},
    publisher = {USENIX Association},
    month = {feb},
    scholarcluster = {6762112190773483176},
}

@article{ErasureCodeEvaluation,
    author = {Chen, Rui and Xu, Lihao},
    title = {Practical Performance Evaluation of Space Optimal Erasure Codes for High-Speed Data Storage Systems},
    year = {2019},
    issue_date = {Jan 2020},
    publisher = {Springer-Verlag},
    address = {Berlin, Heidelberg},
    volume = {1},
    number = {1},
    _url = {https://doi.org/10.1007/s42979-019-0057-1},
    _doi = {10.1007/s42979-019-0057-1},
    abstract = {As erasure codes have been widely adopted in most large-scale data storage systems and applications, implementations of high-performance erasure codes have been improved significantly in recent years, especially by employing Intel’s Streaming SIMD Extensions (SSE) instructions. Augmenting the survey work in Plank et al. (Fast, 9:253–65, 2009) conducted almost a decade ago, this paper compares practical performance of three open-source or public domain erasure coding libraries, namely Jerasure and Intel’s ISA-L for RS code, and a STAR code implementation. The goal of this paper is to provide data storage practitioner a guideline when they choose a proper erasure code for storage applications and systems that need high performance in encoding and decoding operations in the order of GBs/s. Additionally, this paper identifies a practical technique that can further improve decoding performance of RS code greatly for both Jerasure and ISA-L for the most frequent disk failure pattern, i.e., one disk failure.},
    journal = {SN Comput. Sci.},
    month = {dec},
    numpages = {14},
    keywords = {Data storage systems, Performance evaluation, Erasure codes},
    scholarcluster = {9222594581704961566},
}

@article{Liberation,
    author = {James S. Plank},
    title ={The Raid-6 Liberation Code},
    journal = {The International Journal of High Performance Computing Applications},
    volume = {23},
    number = {3},
    pages = {242-251},
    year = {2009},
    _doi = {10.1177/1094342009106191},
    _URL = {https://doi.org/10.1177/1094342009106191},
    _eprint = {https://doi.org/10.1177/1094342009106191},
    scholarcluster = {1618850531970457419},
}
@INPROCEEDINGS{OptimalLiberation,
    author = {Huang, Zhijie and Jiang, Hong and Shen, Zhirong and Che, Hao and Xiao, Nong and Li, Ning},
    booktitle = {2020 IEEE International Parallel and Distributed Processing Symposium (IPDPS)}, 
    title = {Optimal Encoding and Decoding Algorithms for the RAID-6 Liberation Codes}, 
    year = {2020},
    volume = {},
    number = {},
    pages = {708-717},
    keywords = {Encoding;Decoding;Arrays;Complexity theory;Measurement;Standards;Reed-Solomon codes;MDS array codes;RAID-6;reliability;erasure codes},
    _doi = {10.1109/IPDPS47924.2020.00078},
    scholarcluster = {11061002431157637919},
}
@INPROCEEDINGS{HDP,
    author = {Wu, Chentao and He, Xubin and Wu, Guanying and Wan, Shenggang and Liu, Xiaohua and Cao, Qiang and Xie, Changsheng},
    booktitle = {2011 IEEE/IFIP 41st International Conference on Dependable Systems & Networks (DSN)}, 
    title = {HDP code: A Horizontal-Diagonal Parity Code to Optimize I/O load balancing in RAID-6}, 
    year = {2011},
    volume = {},
    number = {},
    pages = {209-220},
    keywords = {Reliability;Load management;Layout;Encoding;Mathematical model;Equations;Arrays;RAID-6;MDS Code;Load Balancing;Horizontal Parity;Diagonal/Anti-diagonal Parity;Performance Evaluation;Reliability},
    _doi = {10.1109/DSN.2011.5958220},
    scholarcluster = {13517830629039920707}
}
@article{EVENODD,
    author = {Blaum, M. and Brady, J. and Bruck, J. and Menon, J.},
    title = {EVENODD: an optimal scheme for tolerating double disk failures in RAID architectures},
    year = {1994},
    issue_date = {April 1994},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    volume = {22},
    number = {2},
    issn = {0163-5964},
    _url = {https://doi.org/10.1145/192007.192033},
    _doi = {10.1145/192007.192033},
    abstract = {We present a novel method, that we call EVENODD, for tolerating up to two disk failures in RAID architectures. EVENODD is the first known scheme for tolerating double disk failures that is optimal with regard to both storage and performance. EVENODD employs the addition of only two redundant disks and consists of simple exclusive-OR computations. A major advantage of EVENODD is that it only requires parity hardware, which is typically present in standard RAID-5 controllers. Hence, EVENODD can be implemented on standard RAID-5 controllers without any hardware changes. The only previously known scheme that employes optimal redundant storage (i.e. two extra disks) is based on Reed-Solomon (RS) error-correcting codes, requires computation over finite fields and results in a more complex implementation. For example, we show that the number of exclusive-OR operations involved in implementing EVENODD in a disk array with 15 disks is about 50\% of the number required when using the RS scheme.},
    journal = {SIGARCH Comput. Archit. News},
    month = {apr},
    pages = {245-254},
    numpages = {10},
    scholarcluster = {4478645138633899737},
}
@ARTICLE{XCode,
    author = {Lihao Xu and Bruck, J.},
    journal = {IEEE Transactions on Information Theory}, 
    title = {X-code: MDS array codes with optimal encoding}, 
    year = {1999},
    volume = {45},
    number = {1},
    pages = {272-276},
    keywords = {Encoding;Parity check codes;Error correction;Decoding;NASA;Space technology},
    _doi = {10.1109/18.746809},
    scholarcluster = {4609139222907026934},
}
@ARTICLE{STAR,
    author = {Huang, Cheng and Xu, Lihao},
    journal = {IEEE Transactions on Computers}, 
    title = {STAR : An Efficient Coding Scheme for Correcting Triple Storage Node Failures}, 
    year = {2008},
    volume = {57},
    number = {7},
    pages = {889-901},
    keywords = {Decoding;Encoding;Complexity theory;Reliability;Distributed databases;Arrays;Triples (Data structure);fault tolerance;high availability;error control;codes;storage systems.;fault tolerance;high availability;error control;codes;storage systems.},
    _doi = {10.1109/TC.2007.70830},
    scholarcluster = {6858253297316900018},
}

@article{Vandermonde,
    author = {Plank, James S. and Ding, Ying},
    title = {Note: Correction to the 1997 tutorial on Reed-Solomon coding},
    year = {2005},
    issue_date = {February 2005},
    publisher = {John Wiley \& Sons, Inc.},
    address = {USA},
    volume = {35},
    number = {2},
    issn = {0038-0644},
    journal = {Softw. Pract. Exper.},
    month = {feb},
    pages = {189-194},
    numpages = {6},
    scholarcluster = {2534031238828547715},
}
@INPROCEEDINGS{Cauchy,
    author = {Plank, James S. and Lihao Xu},
    booktitle = {Fifth IEEE International Symposium on Network Computing and Applications (NCA'06)}, 
    title = {Optimizing Cauchy Reed-Solomon Codes for Fault-Tolerant Network Storage Applications}, 
    year = {2006},
    volume = {},
    number = {},
    pages = {173-180},
    keywords = {Reed-Solomon codes;Fault tolerance;Peer to peer computing;Fault tolerant systems;Application software;Encoding;Computer science;Decoding;Testing;Data structures},
    doi = {10.1109/NCA.2006.43},
    scholarcluster = {3598717293970544773},
}
@article{CauchySolver,
    title = {Pivoting and backward stability of fast algorithms for solving Cauchy linear equations},
    journal = {Linear Algebra and its Applications},
    volume = {343-344},
    pages = {63-99},
    year = {2002},
    note = {Special Issue on Structured and Infinite Systems of Linear equations},
    issn = {0024-3795},
    _doi = {https://doi.org/10.1016/S0024-3795(01)00519-5},
    _url = {https://www.sciencedirect.com/science/article/pii/S0024379501005195},
    author = {Tibor Boros and Thomas Kailath and Vadim Olshevsky},
    keywords = {Displacement structure, Cauchy matrix, Vandermonde matrix, Fast algorithms, Pivoting, Rounding error analysis, Backward stability, Total positivity},
    scholarcluster = {10175688537445443079},
}
@ARTICLE{FFTEncoder,
    author = {Lin, Sian-Jheng and Chung, Wei-Ho},
    journal = {IEEE Communications Letters}, 
    title = {An Efficient (n, k) Information Dispersal Algorithm for High Code Rate System over Fermat Fields}, 
    year = {2012},
    volume = {16},
    number = {12},
    pages = {2036-2039},
    keywords = {Decoding;Encoding;Polynomials;Systematics;Reed-Solomon codes;Complexity theory;Erasure codes;fast Fourier transforms;information dispersal algorithm (IDA);Reed-Solomon codes},
    _doi = {10.1109/LCOMM.2012.112012.121322},
    scholarcluster = {14633484936377010627},
}
@ARTICLE{FFTDecoder,
    author = {Lin, Sian-Jheng and Al-Naffouri, Tareq Y. and Han, Yunghsiang S. and Chung, Wei-Ho},
    journal = {IEEE Transactions on Information Theory}, 
    title = {Novel Polynomial Basis With Fast Fourier Transform and Its Application to Reed-Solomon Erasure Codes}, 
    year = {2016},
    volume = {62},
    number = {11},
    pages = {6284-6299},
    keywords = {Additives;Complexity theory;Discrete Fourier transforms;Reed-Solomon codes;Decoding;STEM;Fast Fourier transform;polynomial basis;finite field;Reed-Solomon code},
    _doi={10.1109/TIT.2016.2608892},
    scholarcluster = {16077703619577843805},
}

@INPROCEEDINGS{FastGFSIMD,
    author = {J. S. Plank and K. M. Greenan and E. L. Miller},
    title = {Screaming Fast Galois Field Arithmetic Using Intel SIMD Instructions},
    booktitle = {FAST-2013: 11th Usenix Conference on File and Storage Technologies},
    month = {February},
    year = {2013},
    address = {San Jose},
    _url = {http://www.usenix.org/events/fast13,http://web.eecs.utk.edu/~jplank/plank/papers/FAST-2013-GF.html},
    scholarcluster = {3819343904230945639},
}
@inproceedings{MatrixOptimization,
    author = {Schuman, Catherine Dorothy},
    year = {2011},
    title = {An Exploration of Optimization Algorithms and Heuristics for the Creation of Encoding and Decoding Schedules in Erasure Coding},
    booktitle = {The Journal of Undergraduate Research at The University of Tennessee},
    note = {Volume 2: Issue 1, Article 6},
    scholarcluster = {9318649668827043603},
}
@INPROCEEDINGS{OptimizingXorCodes,
  author = {Huang, Cheng and Li, Jin and Chen, Minghua},
  booktitle = {2007 IEEE Information Theory Workshop}, 
  title = {On Optimizing XOR-Based Codes for Fault-Tolerant Storage Applications}, 
  year = {2007},
  volume = {},
  number = {},
  pages = {218-223},
  keywords = {Fault tolerance;Reed-Solomon codes;Encoding;Decoding;Design optimization;Application software;Redundancy;Lakes;Computer applications;Greedy algorithms},
  _doi = {10.1109/ITW.2007.4313077},
  scholarcluster = {18241938764952700948},
}
@inproceedings{ProgramOptimizationTechniques,
    author = {Uezato, Yuya},
    title = {Accelerating XOR-based erasure coding using program optimization techniques},
    year = {2021},
    isbn = {9781450384421},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    _url = {https://doi.org/10.1145/3458817.3476204},
    _doi = {10.1145/3458817.3476204},
    abstract = {Erasure coding (EC) affords data redundancy for large-scale systems. XOR-based EC is an easy-to-implement method for optimizing EC. This paper addresses a significant performance gap between the state-of-the-art XOR-based EC approach (~4.9 GB/s coding throughput) and Intel's high-performance EC library based on another approach (~6.7 GB/s). We propose a novel approach based on our observation that XOR-based EC virtually generates programs of a Domain Specific Language for XORing byte arrays. We formalize such programs as straight-line programs (SLPs) of compiler construction and optimize SLPs using various program optimization techniques. Our optimization flow is three-fold: 1) reducing the number of XORs using grammar compression algorithms; 2) reducing memory accesses using deforestation, a functional program optimization method; and 3) reducing cache misses using the (red-blue) pebble game of program analysis. We provide an experimental library, which outperforms Intel's library with an ~8.92 GB/s throughput.},
    booktitle = {Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis},
    articleno = {87},
    numpages = {14},
    location = {St. Louis, Missouri},
    series = {SC '21},
    scholarcluster = {9474485426969968372},
}
@inproceedings{OptimizedMachineLearning,
    author = {Hu, Jiyu and Kosaian, Jack and Rashmi, K. V.},
    title = {Rethinking Erasure-Coding Libraries in the Age of Optimized Machine Learning},
    year = {2024},
    isbn = {9798400706301},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    _url = {https://doi.org/10.1145/3655038.3665943},
    _doi = {10.1145/3655038.3665943},
    abstract = {Erasure codes are critical tools for building fault-tolerant and resource-efficient storage systems. However developing and maintaining optimized erasure-coding libraries are challenging. We make the case that the growth of fast machine-learning (ML) libraries may serve as a lifeboat for easing the development of current and future optimized erasure-coding libraries: fast erasure-coding libraries for various hardware platforms can be easily implemented by using existing optimized ML libraries. We show that the computation structure of many erasure codes mirrors that common to matrix multiplication, which is heavily optimized in ML libraries. Due to this similarity, one can implement erasure codes using ML libraries in few lines of code and with little knowledge of erasure codes, while immediately adopting the many optimizations within these libraries, without requiring expertise in high-performance programming. We develop prototypes of our proposed approach using an existing ML library. Our prototypes are up to 1.75\texttimes{} faster than state-of-the-art custom erasure-coding libraries.},
    booktitle = {Proceedings of the 16th ACM Workshop on Hot Topics in Storage and File Systems},
    pages = {23-30},
    numpages = {8},
    keywords = {erasure coding, machine learning, redundancy},
    location = {Santa Clara, CA, USA},
    series = {HotStorage '24},
    scholarcluster = {16618060823957372909},
}
@article{AccelerationTechniqueSurvey,
    author = {Zhou, Tianli and Tian, Chao},
    title = {Fast Erasure Coding for Data Storage: A Comprehensive Study of the Acceleration Techniques},
    year = {2020},
    issue_date = {February 2020},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    volume = {16},
    number = {1},
    issn = {1553-3077},
    _url = {https://doi.org/10.1145/3375554},
    _doi = {10.1145/3375554},
    abstract = {Various techniques have been proposed in the literature to improve erasure code computation efficiency, including optimizing bitmatrix design and computation schedule, common XOR (exclusive-OR) operation reduction, caching management techniques, and vectorization techniques. These techniques were largely proposed individually, and, in this work, we seek to use them jointly. To accomplish this task, these techniques need to be thoroughly evaluated individually and their relation better understood. Building on extensive testing, we develop methods to systematically optimize the computation chain together with the underlying bitmatrix. This led to a simple design approach of optimizing the bitmatrix by minimizing a weighted computation cost function, and also a straightforward coding procedure—follow a computation schedule produced from the optimized bitmatrix to apply XOR-level vectorization. This procedure provides better performances than most existing techniques (e.g., those used in ISA-L and Jerasure libraries), and sometimes can even compete against well-known but less general codes such as EVENODD, RDP, and STAR codes. One particularly important observation is that vectorizing the XOR operations is a better choice than directly vectorizing finite field operations, not only because of the flexibility in choosing finite field size and the better encoding throughput, but also its minimal migration efforts onto newer CPUs.},
    journal = {ACM Trans. Storage},
    month = {mar},
    articleno = {7},
    numpages = {24},
    keywords = {Erasure code, performance},
    scholarcluster = {15189943361362749273},
}