= Userland Disk I/O
:revdate: 2024-11-06
:page-order: 3
:bibtex-file: disk-io.bib
:page-aside: With thanks to Thomas Munro for the Windows corrections, and Pratyush Mishra for the F_NOCACHE correction.
:page-hook-preamble: false
:page-hook: Filesystems, file IO, and durability.

////
TODO:
- clarify that it's not "most" databases use O_DIRECT, it's that the modern best practice is to use O_DIRECT
- Give pointers to other O_DIRECT usage and discussion
    * Postgres definitely predates O_DIRECT, and didn't even use threads out of portability concerns.  They've looked into adopting async IO and O_DIRECT, but retrofitting it into postgres is understandably hard.  https://www.postgresql.org/message-id/20210223100344.llw5an2aklengrmn@alap3.anarazel.de
    * InnoDB can be configured into using O_DIRECT, but again, age and portability requirements mean that it also supports other ways of writing. https://dev.mysql.com/doc/refman/8.4/en/innodb-parameters.html#sysvar_innodb_flush_method
    * Embedded databases are referenced directly in the post, but sqlite is always a mystery as it also maintains its own page cache anyway.
    * MongoDB's WiredTiger doesn't use O_DIRECT as they appear to not implement async IO, and instead double buffer all pages in memory (and also give incorrect windows guidance...) https://source.wiredtiger.com/develop/tune_system_buffer_cache.html

    And if you look around at other databases, there's either direct usage of it, or discussion of adopting it:
    * Cassandra: https://issues.apache.org/jira/browse/CASSANDRA-14466
    * RocksDB: https://github.com/facebook/rocksdb/wiki/Direct-IO

- Discuss lack of support for O_DIRECT:
    - Mention ZFS added O_DIRECT support in Sept 2024 https://www.phoronix.com/news/OpenZFS-Direct-IO
    - Docker for Mac https://github.com/docker/for-mac/issues/1619

- https://despairlabs.com/blog/posts/2025-03-13-fsync-after-open-is-an-elaborate-no-op/
////

[.display-none]
== APIs

[.text-center.white-bg]
--
image::mental-model.svg[]
--

== File I/O
:uri-preadv2-pwritev2: https://man.archlinux.org/man/pwritev2.2.en#preadv2()_and_pwritev2()
:uri-linus-on-odirect: https://yarchive.net/comp/linux/o_direct.html
:uri-lkml-block-atomic-writes: https://lore.kernel.org/all/20240620125359.2684798-1-john.g.garry@oracle.com/
:uri-libeio: http://software.schmorp.de/pkg/libeio.html
:uri-spdk: https://spdk.io/
:uri-gist-nvme-ctrl: https://gist.github.com/thisismiller/203a3c622c8779cf2f73b86e7d31a650#file-nvme-id-ctrl-h-L210-L211
:uri-iocp: https://learn.microsoft.com/en-us/windows/win32/fileio/i-o-completion-ports
:uri-clarifying-direct-io: https://lwn.net/Articles/348739/

In database land, most databases man:open[2] their WAL and data files with `O_DIRECT` so that man:write[2]/man:writev[2]/man:pwritev[2] perform unbuffered IO, maintain their own page cache, and utilize `fdatasync()` for durability.  Doing so gives the most control over what data maintained in the page cache, allows directly modifying cached data, and using `O_DIRECT` then skips the kernel's page cache when reading and writing data from or to disk.  `O_SYNC/O_DSYNC` allow a single `write()` with `O_DIRECT` to be equivalent to a `write()` followed by a `fsync()`/`fdatasync()`.  In the Linux world, the existence of `O_DIRECT` is surprisingly controversial, and Linus has some {uri-linus-on-odirect}[famous rants on the subject] illustrating the OS/DB world view mismatch.

There are some notable examples of databases that rely on buffered IO and the kernel page cache (e.g. RocksDB, LMDB).  Relying on the kernel's page cache can be polite in the context of an embedded database meant to be used within another application and co-exist with many other applications on a user's computer.  Leaving the caching decisions to the kernel means that more memory for the page cache can be easily granted when the system has the memory to spare, and can be reclaimed when more available memory is needed.
If using buffered IO, {uri-preadv2-pwritev2}[preadv2/prwritev2's flags] can be helpful.  `pwritev2()` has also gained support for {uri-lkml-block-atomic-writes}[multi-block atomic writes], which is conditional on filesystem and drive support{nospace}sidenote:ref[].
[.aside]#sidenote:def[] Drive support means a drive for which Atomic Write Unit Power Fail (awupf) in {uri-gist-nvme-ctrl}[`nvme-cli id-ctrl`] returns something greater than zero.  I've never actually seen a drive support this though.#

Regardless of using buffered or unbuffered IO, it's wise to be mindful of the extra cost of appending to a file versus overwriting a pre-allocated block within a file.  Appending to a file changes the file's size, which thus incurs additional filesystem metadata operations.  Instead, consider using man:fallocate[2] to extend the file in larger chunks{nospace}sidenote:ref[].  Note that depending on the filesystem, `fallocate(FALLOC_FL_ZERO_RANGE)` "preferably" converts the range into unwritten extents. The range will not be physically zeroed out on the device and writes will still need to update metadata to mark the extent as written.  In Linux 6.17, `FALLOC_FL_WRITE_ZEROES` was added which forces extents to be allocated and the space filled with zeroes.  Otherwise, use of the default `0` mode is recommended.  When using `O_DIRECT`, especially heed this guidance, as {uri-clarifying-direct-io}[Clarifying Direct I/O Semantics] largely focuses on that `O_DIRECT` writes which require metadata modifications are a complicated and not well specified topic.
[.aside]#sidenote:def[] I once benchmarked the difference between appending one block every write versus writing over pre-allocated blocks as about a 40% throughput difference.#

Directly invoking `write()` performs synchronous IO.  Event-driven non-blocking IO is increasingly popular for concurrency and due to thread-per-core architectures, and there's a number of different ways to do asynchronous IO in linux.  Prefer them in the following order: man:io_uring[7] > man:io_submit[2] > man:aio[7] > man:epoll[7] > man:select[2].  Support for the number of operations one can issue asynchronously decreases rapidly the further one gets from io_uring.  For example, io_uring supports an async man:fallocate[2], but aio doesn't; aio supports async `fsync()`, and epoll doesn't.  A library which issues synchronous filesystem calls on background threads, like {uri-libeio}[libeio], will be needed to fill in support where it's missing.  For the utmost performance, one can use SPDK, but its usage is particularly not friendly.  biblink:[ModernStorageAPIs]{nospace}sidenote:ref[] has a nice comparison of SPDK vs io_uring vs aio.
[.aside]#sidenote:def[] bibitem:[ModernStorageAPIs]#

For commentary on each of these asynchronous IO frameworks, the libev source code is a treasure which catalogs all the caveats in a leading rant comment in each source file:

* http://cvs.schmorp.de/libev/ev_epoll.c?view=markup#l41[libev/ev_epoll.c]
* http://cvs.schmorp.de/libev/ev_linuxaio.c?view=markup#l41[libev/ev_linuxaio.c] -- highly recommended reading
* http://cvs.schmorp.de/libev/ev_iouring.c?view=markup#l41[libev/ev_iouring.c] -- "overall, the _API_ itself is, I dare to say, not a total trainwreck."

On macOS, the options for asynchronous IO are incredibly limited.  The `aio_*` calls from man:aio[7] will work, but the `io_*` ones are linux-specific.  Otherwise, use of {uri-libeio}[libeio] or a similar threadpool-based asynchronous IO framework is recommended.

On Windows, {uri-iocp}[I/O Completion Ports] is the canonical way to perform asynchronous IO.

== Durability
:uri-luu-file-consistency: https://danluu.com/file-consistency/
:uri-fsyncgate: https://danluu.com/fsyncgate/
:uri-bonsaidb-sync-file-range: https://bonsaidb.io/blog/durable-writes/
:uri-sled-sync-file-range: https://github.com/spacejam/sled/issues/1351
:uri-rocksdb-sync-file-range: https://github.com/facebook/rocksdb/blob/bed40e7266b55349ce9d2dce27aeb2055813a5fe/env/io_posix.cc#L160-L166
:uri-wsl-rename: https://toot.cat/@zkat/109973167110793372
:uri-flushfilebuffers-reliability: https://devblogs.microsoft.com/oldnewthing/20170510-00/?p=95505
:uri-bsd-ufs-fsync: https://lists.dragonflybsd.org/pipermail/kernel/2010-January/317935.html
:uri-bsd-ufs-osync: https://www.postgresql.org/message-id/CA%2BhUKG%2B0DWFSZTGmezxZttXTy0YYrX%3Doemxiw8Gzz3hSTU64Jw%40mail.gmail.com
:uri-bsd-man-fdatasync: https://man.freebsd.org/cgi/man.cgi?query=fdatasync&sektion=2&n=1

man:fsync[2] is the core primitive for making data durable, by which we mean "writes completed before fsync() begun will continue to exist on disk even if you rip out the power cable after fsync() completes".  `fsync()` is the beloved function because the alternatives have caveats.  man:sync[2] applies _all_ buffered writes to _all_ disks, not just the ones performed as part of the database's operations.  man:sync_file_range[2] allows only ranges of a file to have their buffered changes forced to disk, but is non-standard, and only provides durability on ext4 and xfs{nospace}sidenote:ref[].
[.aside]#sidenote:def[] The `sync_file_range()` manpage states "This system call does not flush disk write caches and thus does not provide any data integrity on systems with volatile disk write caches."  However, {uri-bonsaidb-sync-file-range}[testing done by the BonsaiDB author] confirmed that FUA bits are set only on ext4 and xfs, but not btrfs or zfs.  See discussion within {uri-sled-sync-file-range}[Sled] and {uri-rocksdb-sync-file-range}[RocksDB] for further reasons to be cautious.#

For each method of ensuring data durably reaches disk, there's a split between the methods that ensure _File Integrity_ (`fsync()` and `O_SYNC`) and those that ensure _Data Integrity_ (`fdatasync()` and `O_DSYNC`).  For their definitions, we look to the man pages:

[quote,'man:open[2]']
____
O_SYNC provides synchronized I/O file integrity completion,
meaning write operations will flush data and all associated
metadata to the underlying hardware.  O_DSYNC provides
synchronized I/O data integrity completion, meaning write
operations will flush data to the underlying hardware, but will
only flush metadata updates that are required to allow a
subsequent read operation to complete successfully.  Data
integrity completion can reduce the number of disk operations
that are required for applications that don't need the guarantees
of file integrity completion.

To understand the difference between the two types of completion,
consider two pieces of file metadata: the file last modification
timestamp (st_mtime) and the file length.  All write operations
will update the last file modification timestamp, but only writes
that add data to the end of the file will change the file length.
The last modification timestamp is not needed to ensure that a
read completes successfully, but the file length is.  Thus,
O_DSYNC would only guarantee to flush updates to the file length
metadata (whereas O_SYNC would also always flush the last
modification timestamp metadata).
____

To reiterate, if you issue a write to a file using `O_DIRECT` which appends to the file, and then call `fdatasync()`, the appended data _will_ be durable and readable once `fdatasync()` returns, as the filesystem metadata change to increase the length of the file is required to have also been made durable.

However, using `fsync()` correctly and minimally is still not easy.
{uri-luu-file-consistency}[Dan Luu's File Consistency page] and links therein provide a nice overview of some of the challenges. _On the Complexity of Crafting Crash-Consistent Applications_{nospace}sidenote:ref[] takes an even deeper look. The general rules to be aware of are:
[.aside]#sidenote:def[] bibitem:[CraftingCrashConsistency]#

* To write into a new file, first `fsync()` the file, then `fsync()` the containing directory.
* If using the `rename()`-is-atomic trick, again first `fsync()` the file, then `rename()`,then `fsync()` the directory.{nospace}sidenote:ref[]
[.aside]#sidenote:def[] Except there's this {uri-wsl-rename}[one report] of rename() not being atomic on the Windows Subsystem for Linux, so who knows.#
* On the first open of a mutable file, call `fsync()`, as a previous incarnation of the process might have crashed and left non-durable changes in the file.

`fsync()` can fail, and improperly handling that error started {uri-fsyncgate}[fsyncgate] as folk noticed PostgreSQL among other databases had incorrect handling of it.  This received further examination in _Can Applications Recover from fsync Failures?_{nospace}sidenote:ref[].
[.aside]#sidenote:def[] bibitem:[FsyncFailures]#

On macOS, note that the functions and promises to enforce durability are different than those on Unix platforms, because macOS intentionally violated the standards.  See link:/blog/2022-darwins-deceptive-durability.html[Darwin's Deceptive Durability] for the overview.  And continuing its quest to be as difficult as possible to write a database on, macOS uniquely does not support `O_DIRECT`, and thus one must invoke `fcntl(F_NOCACHE)` to get equivalent behavior.

On BSD flavors, UFS specifically does not issue a volatile drive cache flush as part of `fsync()`, as UFS relies on softupdates for consistency.  Matthew Dillon from DragonflyBSD land has a {uri-bsd-ufs-fsync}[well written complaint about this], and my attempt at reviewing the current UFS code seems to agree there's no `BIO_FLUSH` issued.  This is also confirmed on the postgres mailing list where Thomas Munro {uri-bsd-ufs-osync}[additionally points out] `O_SYNC`/`O_DSYNC` are not forced through the volatile disk cache on UFS as well.  There is no workaround to get powersafe durability, other than using ZFS or the native ext3/4 support.  The {uri-bsd-man-fdatasync}[FreeBSD manpage for `fdatasync()`] states: "Unlike fsync(), the system call does not guarantee that file attributes or metadata necessary to access the file are committed to the permanent storage."  However, this durability concern is largely moot because even the stronger `fsync()` still isn't durable.

On Windows, `FlushFileBuffers()`{nospace}sidenote:ref[] is equivalent to `fsync()`, and `NtFlushBuffersFileEx(FLUSH_FLAGS_FILE_DATA_SYNC_ONLY)` is equivalent to `fdatasync()`.  For files opened with `_open()`, call `_commit()` instead.
[.aside]#sidenote:def[] Except this {uri-flushfilebuffers-reliability}[terrifying note on the reliability of FlushBuffersFile] saying "Fortunately, nearly all drivers in the Windows 7 era respect [the command to force changes to disk]. (There are a few stragglers that still ignore it.)"#

Lastly, _Force Unit Access_ (commonly abbreviated as FUA) is the term that the SCSI/SATA/NVMe specifications use for "please force this data to be on non-volatile storage", so if you're ever trying to google durability related things, adding "FUA" will get you better answers.

== Filesystems

:uri-phoronix-fs-bench: https://www.phoronix.com/review/linux-58-filesystems/
:uri-scylladb-qual-fs: https://www.scylladb.com/2016/02/09/qualifying-filesystems/
:uri-xnvme: https://xnvme.io/

Prefer XFS if you can.  It {uri-phoronix-fs-bench}[benchmarks overall well].
It handles a bunch of special cases well that are {uri-scylladb-qual-fs}[important for databases].

Filesystems maintain metadata about how blocks are associated with files, and
optimizing around this will lead to lower latency.  Ext4 and XFS both can
aggregate contiguous blocks in a file into a single _extent_, reducing the
metadata overhead.  This encourages appending to files in large chunks at a time
(or using fallocate to extend the file before performing a series of small
appends).  Maintaining large extents also potentially discourages excessive use
of some filesystem metadata calls, as e.g. fine-grained use of
`FALLOC_FL_PUNCH_HOLE` would be an easy way to continuously fragment extents.
Large files incur large metadata, and so it's often a good idea to incrementally
truncate down a large file before unlinking it, otherwise the entire metadata
traversal and deleting will be performed synchronously with the unlink.

How the storage device is attached to the system changes the number of parallel
operations it can possibly support.  (And the range is wide: SATA NCQ supports
32 concurrent requests, NVMe supports 65k.)  If you submit more than this,
there's implicit queuing that happens in the kernel and userspace only sees
increased latencies.  Theoretically man:ionice[1] and man:ioprio_set[2] offer
some control over how requests are prioritized in that queue, but I've never
really noticed ionice make a difference.

It's possible to open a raw block device and entirely bypass the filesystem.
Doing so requires that all reads and writes be 4k aligned and a multiple of 4k
in size. It also requires reimplementing everything that comes for free with a
filesystem: free block tracking, disk space usage reporting, snapshot-based
backup/restore, application logging, drive health testing.  Anecdotally, I've
heard that the advantage of all of this is an ~10% speedup, so not a tradeoff
that's often worth the cost.  But for easy experimentation and testing of direct
block storage access, a loopback device (man:losetup[8]) allows mounting a file
as a block device.  I'd highly recommend using {uri-xnvme}[xNVMe] if you're looking
to directly interact with NVMe block storage.

== Kernel Things

Be aware of https://wiki.ubuntu.com/Kernel/Reference/IOSchedulers[IO
schedulers].  The general advice is to prefer `mq-deadline` or `none` for SSDs
(SATA or NVME), as the drives are fast enough that excessive scheduling overhead
generally isn't worthwhile.

If using buffered io, https://docs.kernel.org/admin-guide/sysctl/vm.html#dirty-ratio[vm.dirty_ratio] controls when Linux will start writing modified pages to disk.

You can periodically scrape `/proc/diskstats` to self-report on disk metrics.

== See Also

* https://www.evanjones.ca/durability-filesystem.html[Durability: Linux File APIs]
* https://www.scylladb.com/2017/10/05/io-access-methods-scylla/[Different I/O Access Methods for Linux, What We Chose for ScyllaDB, and Why]
* https://www.scylladb.com/2024/11/25/database-internals-working-with-io/[Database Internals: Working with IO]
* https://arxiv.org/abs/2411.16254[Asynchronous I/O -- With Great Power Comes Great Responsibility]